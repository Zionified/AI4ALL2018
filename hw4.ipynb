{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mzaPc6pHwpZ"
      },
      "source": [
        "# 10-714 Homework 4\n",
        "\n",
        "In this homework, you will leverage all of the components built in the last three homeworks to solve some modern problems with high performing network structures. We will start by adding a few new ops leveraging our new CPU/CUDA backends. Then, you will implement convolution, and a convolutional neural network to train a classifier on the CIFAR-10 image classification dataset. Then, you will implement recurrent and long-short term memory (LSTM) neural networks, and do word-level prediction language modeling on the Penn Treebank dataset.\n",
        "\n",
        "As always, we will start by copying this notebook and getting the starting code.\n",
        "Reminder: __you must save a copy in drive__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRaSrr4bHwpa",
        "outputId": "a1340123-60d5-4db5-b10f-ae61d7cb44be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/10714\n",
            "Cloning into 'hw4'...\n",
            "remote: Enumerating objects: 219, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 219 (delta 32), reused 35 (delta 27), pack-reused 153 (from 1)\u001b[K\n",
            "Receiving objects: 100% (219/219), 225.82 KiB | 1.41 MiB/s, done.\n",
            "Resolving deltas: 100% (99/99), done.\n",
            "/content/drive/MyDrive/10714/hw4\n",
            "Collecting git+https://github.com/dlsys10714/mugrade.git\n",
            "  Cloning https://github.com/dlsys10714/mugrade.git to /tmp/pip-req-build-9whojnlv\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/dlsys10714/mugrade.git /tmp/pip-req-build-9whojnlv\n",
            "  Resolved https://github.com/dlsys10714/mugrade.git to commit 656cdc2b7ad5a37e7a5347a7b0405df0acd72380\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mugrade\n",
            "  Building wheel for mugrade (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mugrade: filename=mugrade-1.2-py3-none-any.whl size=3935 sha256=a606dcf1e4d49ddadac5bb56efe18757ce103b2821724ddadf9e5ba0c8fdc9a3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b1obbxao/wheels/8b/ba/3a/621da1207eab160c01968c5e0bd1266f505b9e3f8010376d61\n",
            "Successfully built mugrade\n",
            "Installing collected packages: mugrade\n",
            "Successfully installed mugrade-1.2\n",
            "Collecting pybind11\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybind11\n",
            "Successfully installed pybind11-2.13.6\n"
          ]
        }
      ],
      "source": [
        "# Code to set up the assignment\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/\n",
        "!mkdir -p 10714\n",
        "%cd /content/drive/MyDrive/10714\n",
        "!git clone https://github.com/dlsys10714/hw4.git\n",
        "%cd /content/drive/MyDrive/10714/hw4\n",
        "\n",
        "!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
        "!pip3 install pybind11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsgUgqkWHwpb",
        "outputId": "3167990b-8987-4591-b504-d39bcdb42468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Found pybind11: /usr/local/lib/python3.10/dist-packages/pybind11/include (found version \"2.13.6\")\n",
            "-- Found cuda, building cuda backend\n",
            "-- Configuring done (0.5s)\n",
            "-- Generating done (0.6s)\n",
            "-- Build files have been written to: /content/drive/MyDrive/10714/hw4/build\n",
            "make[1]: Entering directory '/content/drive/MyDrive/10714/hw4/build'\n",
            "make[2]: Entering directory '/content/drive/MyDrive/10714/hw4/build'\n",
            "make[3]: Entering directory '/content/drive/MyDrive/10714/hw4/build'\n",
            "make[3]: Leaving directory '/content/drive/MyDrive/10714/hw4/build'\n",
            "make[3]: Entering directory '/content/drive/MyDrive/10714/hw4/build'\n",
            "[-25%] \u001b[32mBuilding CXX object CMakeFiles/ndarray_backend_cpu.dir/src/ndarray_backend_cpu.cc.o\u001b[0m\n",
            "[  0%] \u001b[32m\u001b[1mLinking CXX shared module /content/drive/MyDrive/10714/hw4/python/needle/backend_ndarray/ndarray_backend_cpu.cpython-310-x86_64-linux-gnu.so\u001b[0m\n",
            "make[3]: Leaving directory '/content/drive/MyDrive/10714/hw4/build'\n",
            "[  0%] Built target ndarray_backend_cpu\n",
            "make[3]: Entering directory '/content/drive/MyDrive/10714/hw4/build'\n",
            "[ 25%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/ndarray_backend_cuda.dir/src/ndarray_backend_cuda_generated_ndarray_backend_cuda.cu.o\u001b[0m\n",
            "nvcc fatal   : Unsupported gpu architecture 'compute_37'\n",
            "\u001b[31mCMake Error at ndarray_backend_cuda_generated_ndarray_backend_cuda.cu.o.cmake:220 (message):\n",
            "  Error generating\n",
            "  /content/drive/MyDrive/10714/hw4/build/CMakeFiles/ndarray_backend_cuda.dir/src/./ndarray_backend_cuda_generated_ndarray_backend_cuda.cu.o\n",
            "\n",
            "\u001b[0m\n",
            "make[3]: *** [CMakeFiles/ndarray_backend_cuda.dir/build.make:77: CMakeFiles/ndarray_backend_cuda.dir/src/ndarray_backend_cuda_generated_ndarray_backend_cuda.cu.o] Error 1\n",
            "make[3]: Leaving directory '/content/drive/MyDrive/10714/hw4/build'\n",
            "make[2]: *** [CMakeFiles/Makefile2:110: CMakeFiles/ndarray_backend_cuda.dir/all] Error 2\n",
            "make[2]: Leaving directory '/content/drive/MyDrive/10714/hw4/build'\n",
            "make[1]: *** [Makefile:91: all] Error 2\n",
            "make[1]: Leaving directory '/content/drive/MyDrive/10714/hw4/build'\n",
            "make: *** [Makefile:9: lib] Error 2\n"
          ]
        }
      ],
      "source": [
        "!make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMi0Uc-nHwpb",
        "outputId": "b2cb1697-d5e9-4e58-b527-15e70cdb6965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=./python\n",
            "env: NEEDLE_BACKEND=nd\n"
          ]
        }
      ],
      "source": [
        "%set_env PYTHONPATH ./python\n",
        "%set_env NEEDLE_BACKEND nd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7E3Eiiv1Hwpb"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('./python')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBRTXbLJHwpb",
        "outputId": "d2a35dff-8550-4e2f-8141-832d744f9d46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n"
          ]
        }
      ],
      "source": [
        "# Download the datasets you will be using for this assignment\n",
        "\n",
        "import urllib.request\n",
        "import os\n",
        "\n",
        "!mkdir -p './data/ptb'\n",
        "# Download Penn Treebank dataset\n",
        "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
        "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
        "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
        "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))\n",
        "\n",
        "# Download CIFAR-10 dataset\n",
        "if not os.path.isdir(\"./data/cifar-10-batches-py\"):\n",
        "    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"./data/cifar-10-python.tar.gz\")\n",
        "    !tar -xvzf './data/cifar-10-python.tar.gz' -C './data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZx0wMkYHwpb"
      },
      "source": [
        "To finish setting up the assignment, go ahead and fill in all the code in `python/needle/autograd.py` using your solution code from the previous homework. Also copy the solutions in `src/ndarray_backend_cpu.cc` and `src/ndarray_backend_cuda.cu` from homework 3.\n",
        "\n",
        "**Note**: Be careful not to accidentally delete or modify any new imports and function declarations when copying over code from previous assignments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf4ea9IvHwpb"
      },
      "source": [
        "## Part 1: ND Backend [10 pts]\n",
        "\n",
        "Recall that in homework 2, the `array_api` was imported as `numpy`. In this part, the goal is to write the necessary operations with `array_api` imported from the needle backend `NDArray` in `python/needle/backend_ndarray/ndarray.py`. Make sure to copy the solutions for `reshape`, `permute`, `broadcast_to` and `__getitem__` from homework 3.\n",
        "\n",
        "Fill in the following classes in `python/needle/ops_logarithmic.py` and `python/needle/ops_mathematic.py`:\n",
        "\n",
        "- `PowerScalar`\n",
        "- `EWiseDiv`\n",
        "- `DivScalar`\n",
        "- `Transpose`\n",
        "- `Reshape`\n",
        "- `BroadcastTo`\n",
        "- `Summation`\n",
        "- `MatMul`\n",
        "- `Negate`\n",
        "- `Log`\n",
        "- `Exp`\n",
        "- `ReLU`\n",
        "- `LogSumExp`\n",
        "- `Tanh` (new)\n",
        "- `Stack` (new)\n",
        "- `Split` (new)\n",
        "\n",
        "Note that for most of these, you already wrote the solutions in the previous homework and you should not change most part of your previous solution, if issues arise, please check if the `array_api` function used is supported in the needle backend.\n",
        "\n",
        "The `Tanh`, `Stack`, and `Split` operators are newly added. `Stack` concatenates same-sized tensors along a new axis, and `Split` undoes this operation. The gradients of the two operations can be written in terms of each other. We do not directly test `Split`, and only test the backward pass of `Stack` (for which we assume you used `Split`).\n",
        "\n",
        "**Note:** You may want to make your Summation op support sums over multiple axes; you will likely need it for the backward pass of the BroadcastTo op if yours supports broadcasting over multiple axes at a time. However, this is more about ease of use than necessity, and we leave this decision up to you (there are no corresponding tests).\n",
        "\n",
        "**Note:** Depending on your implementations, you may want to ensure that you call `.compact()` before reshaping arrays. (If this is necessary, you will run into corresponding error messages later in the assignment.)\n",
        "\n",
        "**Note**: Be careful not to accidentally delete or modify any new imports and function declarations when copying over code from previous assignments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHn7YeDQHwpb",
        "outputId": "5eda5c71-d34d-40e3-a17f-47b618baab34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/drive/MyDrive/10714/hw4\n",
            "plugins: anyio-3.7.1, typeguard-4.4.1\n",
            "collected 1803 items / 1685 deselected / 118 selected                                              \u001b[0m\n",
            "\n",
            "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape0-divide] \u001b[31mFAILED\u001b[0m\u001b[31m                        [  0%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape0-subtract] \u001b[31mFAILED\u001b[0m\u001b[31m                      [  1%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape1-divide] \u001b[31mFAILED\u001b[0m\u001b[31m                        [  2%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape1-subtract] \u001b[31mFAILED\u001b[0m\u001b[31m                      [  3%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape0-divide] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m             [  4%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape0-subtract] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m           [  5%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape1-divide] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m             [  5%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape1-subtract] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m           [  6%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape0-divide] \u001b[31mFAILED\u001b[0m\u001b[31m                       [  7%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[31m                     [  8%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape1-divide] \u001b[31mFAILED\u001b[0m\u001b[31m                       [  9%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[31m                     [ 10%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape0-divide] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m            [ 11%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape0-subtract] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m          [ 11%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape1-divide] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m            [ 12%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape1-subtract] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m          [ 13%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cpu-16-16-16] \u001b[31mFAILED\u001b[0m\u001b[31m                               [ 14%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cpu-8-8-8] \u001b[31mFAILED\u001b[0m\u001b[31m                                  [ 15%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cpu-1-2-3] \u001b[31mFAILED\u001b[0m\u001b[31m                                  [ 16%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cpu-3-4-5] \u001b[31mFAILED\u001b[0m\u001b[31m                                  [ 16%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cpu-5-4-3] \u001b[31mFAILED\u001b[0m\u001b[31m                                  [ 17%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cpu-16-16-32] \u001b[31mFAILED\u001b[0m\u001b[31m                               [ 18%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cpu-64-64-64] \u001b[31mFAILED\u001b[0m\u001b[31m                               [ 19%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cpu-72-72-72] \u001b[31mFAILED\u001b[0m\u001b[31m                               [ 20%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cpu-72-73-74] \u001b[31mFAILED\u001b[0m\u001b[31m                               [ 21%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cpu-74-73-72] \u001b[31mFAILED\u001b[0m\u001b[31m                               [ 22%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cpu-128-128-128] \u001b[31mFAILED\u001b[0m\u001b[31m                            [ 22%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cuda-16-16-16] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                    [ 23%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cuda-8-8-8] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                       [ 24%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cuda-1-2-3] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                       [ 25%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cuda-3-4-5] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                       [ 26%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cuda-5-4-3] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                       [ 27%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cuda-16-16-32] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                    [ 27%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cuda-64-64-64] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                    [ 28%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cuda-72-72-72] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                    [ 29%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cuda-72-73-74] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                    [ 30%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cuda-74-73-72] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                    [ 31%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_matmul[cuda-128-128-128] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                 [ 32%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_power[cpu-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m                                  [ 33%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_power[cpu-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m                                  [ 33%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_power[cuda-shape0] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                       [ 34%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_power[cuda-shape1] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                       [ 35%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_log[cpu-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m                                    [ 36%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_log[cpu-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m                                    [ 37%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_log[cuda-shape0] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                         [ 38%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_log[cuda-shape1] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                         [ 38%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_exp[cpu-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m                                    [ 39%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_exp[cpu-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m                                    [ 40%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_exp[cuda-shape0] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                         [ 41%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_exp[cuda-shape1] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                         [ 42%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_relu[cpu-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m                                   [ 43%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_relu[cpu-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m                                   [ 44%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_relu[cuda-shape0] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                        [ 44%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_relu[cuda-shape1] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                        [ 45%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_tanh[cpu-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m                                   [ 46%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_tanh[cpu-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m                                   [ 47%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_tanh[cuda-shape0] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                        [ 48%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_tanh[cuda-shape1] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                        [ 49%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_tanh_backward[cpu-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m                          [ 50%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_tanh_backward[cpu-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m                          [ 50%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_tanh_backward[cuda-shape0] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m               [ 51%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_tanh_backward[cuda-shape1] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m               [ 52%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_stack[cpu-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m                              [ 53%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_stack[cpu-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m                              [ 54%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_stack[cpu-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m                              [ 55%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_stack[cuda-shape0-0-1] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                   [ 55%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_stack[cuda-shape1-0-2] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                   [ 56%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_stack[cuda-shape2-2-5] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                   [ 57%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_stack_backward[cpu-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m                     [ 58%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_stack_backward[cpu-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m                     [ 59%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_stack_backward[cpu-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m                     [ 60%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_stack_backward[cuda-shape0-0-1] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m          [ 61%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_stack_backward[cuda-shape1-0-2] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m          [ 61%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_stack_backward[cuda-shape2-2-5] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m          [ 62%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_summation[cpu-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m                         [ 63%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_summation[cpu-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m                            [ 64%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_summation[cpu-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m                            [ 65%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_summation[cpu-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m                            [ 66%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_summation[cuda-shape0-None] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m              [ 66%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_summation[cuda-shape1-0] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                 [ 67%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_summation[cuda-shape2-1] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                 [ 68%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_summation[cuda-shape3-2] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                 [ 69%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m                [ 70%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m                   [ 71%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m                   [ 72%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m                   [ 72%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape0-None] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m     [ 73%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape1-0] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m        [ 74%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape2-1] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m        [ 75%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape3-2] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m        [ 76%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_broadcast_to[cpu-shape0-shape_to0] \u001b[31mFAILED\u001b[0m\u001b[31m                 [ 77%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_broadcast_to[cpu-shape1-shape_to1] \u001b[31mFAILED\u001b[0m\u001b[31m                 [ 77%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_broadcast_to[cuda-shape0-shape_to0] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m      [ 78%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_broadcast_to[cuda-shape1-shape_to1] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m      [ 79%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_reshape[cpu-shape0-shape_to0] \u001b[31mFAILED\u001b[0m\u001b[31m                      [ 80%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_reshape[cpu-shape1-shape_to1] \u001b[31mFAILED\u001b[0m\u001b[31m                      [ 81%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_reshape[cuda-shape0-shape_to0] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m           [ 82%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_reshape[cuda-shape1-shape_to1] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m           [ 83%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes0-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m                        [ 83%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes0-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m                        [ 84%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes1-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m                        [ 85%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes1-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m                        [ 86%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_transpose[cpu-None-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m                         [ 87%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_transpose[cpu-None-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m                         [ 88%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes0-shape0] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m             [ 88%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes0-shape1] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m             [ 89%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes1-shape0] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m             [ 90%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes1-shape1] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m             [ 91%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_transpose[cuda-None-shape0] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m              [ 92%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_transpose[cuda-None-shape1] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m              [ 93%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m                         [ 94%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m                            [ 94%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m                            [ 95%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m                            [ 96%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape0-None] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m              [ 97%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape1-0] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                 [ 98%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape2-1] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                 [ 99%]\u001b[0m\n",
            "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape3-2] \u001b[33mSKIPPED\u001b[0m (No GPU)\u001b[31m                 [100%]\u001b[0m\n",
            "\n",
            "============================================= FAILURES =============================================\n",
            "\u001b[31m\u001b[1m_________________________________ test_ewise_fn[cpu-shape0-divide] _________________________________\u001b[0m\n",
            "\n",
            "fn = <function <lambda> at 0x7f5d31839360>, shape = (1, 1, 1), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EWISE_OP_FNS, ids=EWISE_OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[1.7886285]]])\n",
            "B          = needle.Tensor([[[0.43650985]]])\n",
            "_A         = array([[[1.7886285]]], dtype=float32)\n",
            "_B         = array([[[0.43650985]]], dtype=float32)\n",
            "device     = cpu()\n",
            "fn         = <function <lambda> at 0x7f5d31839360>\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:56: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:42: in <lambda>\n",
            "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a / b,\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[1.7886285]]])\n",
            "        b          = needle.Tensor([[[0.43650985]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:337: in __truediv__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseDiv()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[0.43650985]]])\n",
            "        self       = needle.Tensor([[[1.7886285]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[1.7886285]]]), needle.Tensor([[[0.43650985]]]))\n",
            "        self       = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f5d388e2200>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[1.7886285]]]), needle.Tensor([[[0.43650985]]]))\n",
            "        op         = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f5d388e2200>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5d388e1cf0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5d388e1cf0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f5d388e2200>\n",
            "a = NDArray([[[1.7886285]]], device=cpu()), b = NDArray([[[0.43650985]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[1.7886285]]], device=cpu())\n",
            "b          = NDArray([[[0.43650985]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f5d388e2200>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:117: NotImplementedError\n",
            "\u001b[31m\u001b[1m________________________________ test_ewise_fn[cpu-shape0-subtract] ________________________________\u001b[0m\n",
            "\n",
            "fn = <function <lambda> at 0x7f5c9db25bd0>, shape = (1, 1, 1), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EWISE_OP_FNS, ids=EWISE_OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[0.09649747]]])\n",
            "B          = needle.Tensor([[[-1.8634927]]])\n",
            "_A         = array([[[0.09649747]]], dtype=float32)\n",
            "_B         = array([[[-1.8634927]]], dtype=float32)\n",
            "device     = cpu()\n",
            "fn         = <function <lambda> at 0x7f5c9db25bd0>\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:56: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:43: in <lambda>\n",
            "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33msubtract\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a - b\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[0.09649747]]])\n",
            "        b          = needle.Tensor([[[-1.8634927]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:331: in __sub__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseAdd()(\u001b[96mself\u001b[39;49;00m, needle.ops.Negate()(other))\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[-1.8634927]]])\n",
            "        self       = needle.Tensor([[[0.09649747]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-1.8634927]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Negate object at 0x7f5c9d52f850>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-1.8634927]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Negate object at 0x7f5c9d52f850>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d52e350>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d52e350>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Negate object at 0x7f5c9d52f850>\n",
            "a = NDArray([[[-1.8634927]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-1.8634927]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Negate object at 0x7f5c9d52f850>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:244: NotImplementedError\n",
            "\u001b[31m\u001b[1m_________________________________ test_ewise_fn[cpu-shape1-divide] _________________________________\u001b[0m\n",
            "\n",
            "fn = <function <lambda> at 0x7f5d31839360>, shape = (4, 5, 6), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EWISE_OP_FNS, ids=EWISE_OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-0.27738822 -0.35475898 -0.08274148 -0.6270007  -0.04381817\n",
            "   -0.47721803]\n",
            "  [-1.3138647   0.8846224... 0.48641446  0.8515189\n",
            "    0.48624933]\n",
            "  [-0.83423984  1.3449924  -0.6782127   0.42643508 -0.7533348\n",
            "   -1.7441102 ]]])\n",
            "B          = needle.Tensor([[[ 2.2575027e-01  2.8703517e-01 -7.7440962e-02  2.7606851e-01\n",
            "   -6.4841092e-01 -7.3746485e-01]\n",
            "  [-1.6...-01 -4.3999133e-01]\n",
            "  [ 1.3349704e-01 -1.2892612e+00 -1.9829026e-01  2.4575877e+00\n",
            "    1.0672156e+00  6.4142066e-01]]])\n",
            "_A         = array([[[-0.27738822, -0.35475898, -0.08274148, -0.6270007 ,\n",
            "         -0.04381817, -0.47721803],\n",
            "        [-1.3138647 ,...933],\n",
            "        [-0.83423984,  1.3449924 , -0.6782127 ,  0.42643508,\n",
            "         -0.7533348 , -1.7441102 ]]], dtype=float32)\n",
            "_B         = array([[[ 2.2575027e-01,  2.8703517e-01, -7.7440962e-02,  2.7606851e-01,\n",
            "         -6.4841092e-01, -7.3746485e-01],\n",
            "   ...3349704e-01, -1.2892612e+00, -1.9829026e-01,  2.4575877e+00,\n",
            "          1.0672156e+00,  6.4142066e-01]]], dtype=float32)\n",
            "device     = cpu()\n",
            "fn         = <function <lambda> at 0x7f5d31839360>\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:56: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:42: in <lambda>\n",
            "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a / b,\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[-0.27738822 -0.35475898 -0.08274148 -0.6270007  -0.04381817\n",
            "   -0.47721803]\n",
            "  [-1.3138647   0.8846224... 0.48641446  0.8515189\n",
            "    0.48624933]\n",
            "  [-0.83423984  1.3449924  -0.6782127   0.42643508 -0.7533348\n",
            "   -1.7441102 ]]])\n",
            "        b          = needle.Tensor([[[ 2.2575027e-01  2.8703517e-01 -7.7440962e-02  2.7606851e-01\n",
            "   -6.4841092e-01 -7.3746485e-01]\n",
            "  [-1.6...-01 -4.3999133e-01]\n",
            "  [ 1.3349704e-01 -1.2892612e+00 -1.9829026e-01  2.4575877e+00\n",
            "    1.0672156e+00  6.4142066e-01]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:337: in __truediv__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseDiv()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[ 2.2575027e-01  2.8703517e-01 -7.7440962e-02  2.7606851e-01\n",
            "   -6.4841092e-01 -7.3746485e-01]\n",
            "  [-1.6...-01 -4.3999133e-01]\n",
            "  [ 1.3349704e-01 -1.2892612e+00 -1.9829026e-01  2.4575877e+00\n",
            "    1.0672156e+00  6.4142066e-01]]])\n",
            "        self       = needle.Tensor([[[-0.27738822 -0.35475898 -0.08274148 -0.6270007  -0.04381817\n",
            "   -0.47721803]\n",
            "  [-1.3138647   0.8846224... 0.48641446  0.8515189\n",
            "    0.48624933]\n",
            "  [-0.83423984  1.3449924  -0.6782127   0.42643508 -0.7533348\n",
            "   -1.7441102 ]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.27738822 -0.35475898 -0.08274148 -0.6270007  -0.04381817\n",
            "   -0.47721803]\n",
            "  [-1.3138647   0.884622...01 -4.3999133e-01]\n",
            "  [ 1.3349704e-01 -1.2892612e+00 -1.9829026e-01  2.4575877e+00\n",
            "    1.0672156e+00  6.4142066e-01]]]))\n",
            "        self       = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f5c9d2e7d00>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-0.27738822 -0.35475898 -0.08274148 -0.6270007  -0.04381817\n",
            "   -0.47721803]\n",
            "  [-1.3138647   0.884622...01 -4.3999133e-01]\n",
            "  [ 1.3349704e-01 -1.2892612e+00 -1.9829026e-01  2.4575877e+00\n",
            "    1.0672156e+00  6.4142066e-01]]]))\n",
            "        op         = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f5c9d2e7d00>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d2e7e20>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d2e7e20>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f5c9d2e7d00>\n",
            "a = NDArray([[[-0.27738822 -0.35475898 -0.08274148 -0.6270007  -0.04381817\n",
            "   -0.47721803]\n",
            "  [-1.3138647   0.8846224   0.8....8515189\n",
            "    0.48624933]\n",
            "  [-0.83423984  1.3449924  -0.6782127   0.42643508 -0.7533348\n",
            "   -1.7441102 ]]], device=cpu())\n",
            "b = NDArray([[[ 2.2575027e-01  2.8703517e-01 -7.7440962e-02  2.7606851e-01\n",
            "   -6.4841092e-01 -7.3746485e-01]\n",
            "  [-1.6809011...e-01]\n",
            "  [ 1.3349704e-01 -1.2892612e+00 -1.9829026e-01  2.4575877e+00\n",
            "    1.0672156e+00  6.4142066e-01]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-0.27738822 -0.35475898 -0.08274148 -0.6270007  -0.04381817\n",
            "   -0.47721803]\n",
            "  [-1.3138647   0.8846224   0.8....8515189\n",
            "    0.48624933]\n",
            "  [-0.83423984  1.3449924  -0.6782127   0.42643508 -0.7533348\n",
            "   -1.7441102 ]]], device=cpu())\n",
            "b          = NDArray([[[ 2.2575027e-01  2.8703517e-01 -7.7440962e-02  2.7606851e-01\n",
            "   -6.4841092e-01 -7.3746485e-01]\n",
            "  [-1.6809011...e-01]\n",
            "  [ 1.3349704e-01 -1.2892612e+00 -1.9829026e-01  2.4575877e+00\n",
            "    1.0672156e+00  6.4142066e-01]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f5c9d2e7d00>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:117: NotImplementedError\n",
            "\u001b[31m\u001b[1m________________________________ test_ewise_fn[cpu-shape1-subtract] ________________________________\u001b[0m\n",
            "\n",
            "fn = <function <lambda> at 0x7f5c9db25bd0>, shape = (4, 5, 6), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EWISE_OP_FNS, ids=EWISE_OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[ 1.1039217   1.881755    0.5935881   2.0708785   1.0697984\n",
            "    0.16651951]\n",
            "  [ 1.7194766  -2.3592148 ... 0.23502702 -1.9450723\n",
            "   -1.1597229 ]\n",
            "  [-0.4758994   0.29684454 -0.00629878  1.5008904  -0.8701592\n",
            "   -0.23963207]]])\n",
            "B          = needle.Tensor([[[ 0.2550807  -0.2314974   0.4955804  -0.57056284  1.4203423\n",
            "   -0.31959015]\n",
            "  [ 1.1159452  -0.03030337... 0.5967932   0.4882425\n",
            "   -0.16943686]\n",
            "  [-1.3580452  -0.06711047 -0.92429376  0.8813011   0.5564429\n",
            "    0.74689156]]])\n",
            "_A         = array([[[ 1.1039217 ,  1.881755  ,  0.5935881 ,  2.0708785 ,\n",
            "          1.0697984 ,  0.16651951],\n",
            "        [ 1.7194766 ,...29 ],\n",
            "        [-0.4758994 ,  0.29684454, -0.00629878,  1.5008904 ,\n",
            "         -0.8701592 , -0.23963207]]], dtype=float32)\n",
            "_B         = array([[[ 0.2550807 , -0.2314974 ,  0.4955804 , -0.57056284,\n",
            "          1.4203423 , -0.31959015],\n",
            "        [ 1.1159452 ,...686],\n",
            "        [-1.3580452 , -0.06711047, -0.92429376,  0.8813011 ,\n",
            "          0.5564429 ,  0.74689156]]], dtype=float32)\n",
            "device     = cpu()\n",
            "fn         = <function <lambda> at 0x7f5c9db25bd0>\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:56: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:43: in <lambda>\n",
            "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33msubtract\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a - b\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[ 1.1039217   1.881755    0.5935881   2.0708785   1.0697984\n",
            "    0.16651951]\n",
            "  [ 1.7194766  -2.3592148 ... 0.23502702 -1.9450723\n",
            "   -1.1597229 ]\n",
            "  [-0.4758994   0.29684454 -0.00629878  1.5008904  -0.8701592\n",
            "   -0.23963207]]])\n",
            "        b          = needle.Tensor([[[ 0.2550807  -0.2314974   0.4955804  -0.57056284  1.4203423\n",
            "   -0.31959015]\n",
            "  [ 1.1159452  -0.03030337... 0.5967932   0.4882425\n",
            "   -0.16943686]\n",
            "  [-1.3580452  -0.06711047 -0.92429376  0.8813011   0.5564429\n",
            "    0.74689156]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:331: in __sub__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseAdd()(\u001b[96mself\u001b[39;49;00m, needle.ops.Negate()(other))\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[[ 0.2550807  -0.2314974   0.4955804  -0.57056284  1.4203423\n",
            "   -0.31959015]\n",
            "  [ 1.1159452  -0.03030337... 0.5967932   0.4882425\n",
            "   -0.16943686]\n",
            "  [-1.3580452  -0.06711047 -0.92429376  0.8813011   0.5564429\n",
            "    0.74689156]]])\n",
            "        self       = needle.Tensor([[[ 1.1039217   1.881755    0.5935881   2.0708785   1.0697984\n",
            "    0.16651951]\n",
            "  [ 1.7194766  -2.3592148 ... 0.23502702 -1.9450723\n",
            "   -1.1597229 ]\n",
            "  [-0.4758994   0.29684454 -0.00629878  1.5008904  -0.8701592\n",
            "   -0.23963207]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[ 0.2550807  -0.2314974   0.4955804  -0.57056284  1.4203423\n",
            "   -0.31959015]\n",
            "  [ 1.1159452  -0.0303033....5967932   0.4882425\n",
            "   -0.16943686]\n",
            "  [-1.3580452  -0.06711047 -0.92429376  0.8813011   0.5564429\n",
            "    0.74689156]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Negate object at 0x7f5c9d52df60>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[ 0.2550807  -0.2314974   0.4955804  -0.57056284  1.4203423\n",
            "   -0.31959015]\n",
            "  [ 1.1159452  -0.0303033....5967932   0.4882425\n",
            "   -0.16943686]\n",
            "  [-1.3580452  -0.06711047 -0.92429376  0.8813011   0.5564429\n",
            "    0.74689156]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Negate object at 0x7f5c9d52df60>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d52de40>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d52de40>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Negate object at 0x7f5c9d52df60>\n",
            "a = NDArray([[[ 0.2550807  -0.2314974   0.4955804  -0.57056284  1.4203423\n",
            "   -0.31959015]\n",
            "  [ 1.1159452  -0.03030337  1.49....4882425\n",
            "   -0.16943686]\n",
            "  [-1.3580452  -0.06711047 -0.92429376  0.8813011   0.5564429\n",
            "    0.74689156]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[ 0.2550807  -0.2314974   0.4955804  -0.57056284  1.4203423\n",
            "   -0.31959015]\n",
            "  [ 1.1159452  -0.03030337  1.49....4882425\n",
            "   -0.16943686]\n",
            "  [-1.3580452  -0.06711047 -0.92429376  0.8813011   0.5564429\n",
            "    0.74689156]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Negate object at 0x7f5c9d52df60>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:244: NotImplementedError\n",
            "\u001b[31m\u001b[1m________________________________ test_scalar_fn[cpu-shape0-divide] _________________________________\u001b[0m\n",
            "\n",
            "fn = <function <lambda> at 0x7f5c9db25cf0>, shape = (1, 1, 1), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randn(\u001b[94m1\u001b[39;49;00m).astype(np.float32).item()\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(fn(_A, _B), fn(A, _B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-0.34836832]]])\n",
            "_A         = array([[[-0.34836832]]], dtype=float32)\n",
            "_B         = -1.8110191822052002\n",
            "device     = cpu()\n",
            "fn         = <function <lambda> at 0x7f5c9db25cf0>\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:72: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:60: in <lambda>\n",
            "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a / b,\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[-0.34836832]]])\n",
            "        b          = -1.8110191822052002\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:339: in __truediv__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.DivScalar(other)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        other      = -1.8110191822052002\n",
            "        self       = needle.Tensor([[[-0.34836832]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.34836832]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.DivScalar object at 0x7f5c9d57a2f0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-0.34836832]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.DivScalar object at 0x7f5c9d57a2f0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d578280>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d578280>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.DivScalar object at 0x7f5c9d57a2f0>\n",
            "a = NDArray([[[-0.34836832]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-0.34836832]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.DivScalar object at 0x7f5c9d57a2f0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:136: NotImplementedError\n",
            "\u001b[31m\u001b[1m________________________________ test_scalar_fn[cpu-shape1-divide] _________________________________\u001b[0m\n",
            "\n",
            "fn = <function <lambda> at 0x7f5c9db25cf0>, shape = (4, 5, 6), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randn(\u001b[94m1\u001b[39;49;00m).astype(np.float32).item()\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(fn(_A, _B), fn(A, _B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-1.4833167   0.13791604  1.1932526  -1.0757235   1.7676828\n",
            "   -0.34903246]\n",
            "  [-1.0753901   1.3716581 ...-0.8066985   1.6680253\n",
            "    0.25968838]\n",
            "  [ 0.11024354  0.57954335 -1.7123466   0.42849943  0.9961082\n",
            "    0.33900097]]])\n",
            "_A         = array([[[-1.4833167 ,  0.13791604,  1.1932526 , -1.0757235 ,\n",
            "          1.7676828 , -0.34903246],\n",
            "        [-1.0753901 ,...838],\n",
            "        [ 0.11024354,  0.57954335, -1.7123466 ,  0.42849943,\n",
            "          0.9961082 ,  0.33900097]]], dtype=float32)\n",
            "_B         = 0.14525873959064484\n",
            "device     = cpu()\n",
            "fn         = <function <lambda> at 0x7f5c9db25cf0>\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:72: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:60: in <lambda>\n",
            "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a / b,\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[-1.4833167   0.13791604  1.1932526  -1.0757235   1.7676828\n",
            "   -0.34903246]\n",
            "  [-1.0753901   1.3716581 ...-0.8066985   1.6680253\n",
            "    0.25968838]\n",
            "  [ 0.11024354  0.57954335 -1.7123466   0.42849943  0.9961082\n",
            "    0.33900097]]])\n",
            "        b          = 0.14525873959064484\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:339: in __truediv__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.DivScalar(other)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        other      = 0.14525873959064484\n",
            "        self       = needle.Tensor([[[-1.4833167   0.13791604  1.1932526  -1.0757235   1.7676828\n",
            "   -0.34903246]\n",
            "  [-1.0753901   1.3716581 ...-0.8066985   1.6680253\n",
            "    0.25968838]\n",
            "  [ 0.11024354  0.57954335 -1.7123466   0.42849943  0.9961082\n",
            "    0.33900097]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-1.4833167   0.13791604  1.1932526  -1.0757235   1.7676828\n",
            "   -0.34903246]\n",
            "  [-1.0753901   1.3716581....8066985   1.6680253\n",
            "    0.25968838]\n",
            "  [ 0.11024354  0.57954335 -1.7123466   0.42849943  0.9961082\n",
            "    0.33900097]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.DivScalar object at 0x7f5c9d39dbd0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-1.4833167   0.13791604  1.1932526  -1.0757235   1.7676828\n",
            "   -0.34903246]\n",
            "  [-1.0753901   1.3716581....8066985   1.6680253\n",
            "    0.25968838]\n",
            "  [ 0.11024354  0.57954335 -1.7123466   0.42849943  0.9961082\n",
            "    0.33900097]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.DivScalar object at 0x7f5c9d39dbd0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d39ef20>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d39ef20>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.DivScalar object at 0x7f5c9d39dbd0>\n",
            "a = NDArray([[[-1.4833167   0.13791604  1.1932526  -1.0757235   1.7676828\n",
            "   -0.34903246]\n",
            "  [-1.0753901   1.3716581   0.29....6680253\n",
            "    0.25968838]\n",
            "  [ 0.11024354  0.57954335 -1.7123466   0.42849943  0.9961082\n",
            "    0.33900097]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-1.4833167   0.13791604  1.1932526  -1.0757235   1.7676828\n",
            "   -0.34903246]\n",
            "  [-1.0753901   1.3716581   0.29....6680253\n",
            "    0.25968838]\n",
            "  [ 0.11024354  0.57954335 -1.7123466   0.42849943  0.9961082\n",
            "    0.33900097]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.DivScalar object at 0x7f5c9d39dbd0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:136: NotImplementedError\n",
            "\u001b[31m\u001b[1m____________________________________ test_matmul[cpu-16-16-16] _____________________________________\u001b[0m\n",
            "\n",
            "m = 16, n = 16, p = 16, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[ 0.5459043  -1.0255798  -0.9868334  -1.1512803  -0.11136018  0.58853644\n",
            "   0.40300608  0.4074028  -0.3...591106 -0.28878465  1.204689   -0.5803022  -0.39473587 -0.02506427\n",
            "  -1.0649109   0.90236807 -2.1894739   1.4513721 ]])\n",
            "B          = needle.Tensor([[-4.35158372e-01 -1.25658166e+00  2.82263488e-01 -1.71122360e+00\n",
            "   5.37350357e-01  7.73624003e-01 -5.7...01  6.72981918e-01  3.01030993e-01 -1.78013062e+00\n",
            "  -1.01348393e-01 -1.65038943e+00 -1.83468729e-01  5.98948419e-01]])\n",
            "_A         = array([[ 0.5459043 , -1.0255798 , -0.9868334 , -1.1512803 , -0.11136018,\n",
            "         0.58853644,  0.40300608,  0.4074028 ...803022 ,\n",
            "        -0.39473587, -0.02506427, -1.0649109 ,  0.90236807, -2.1894739 ,\n",
            "         1.4513721 ]], dtype=float32)\n",
            "_B         = array([[-4.35158372e-01, -1.25658166e+00,  2.82263488e-01,\n",
            "        -1.71122360e+00,  5.37350357e-01,  7.73624003e-01,\n",
            "..., -1.78013062e+00,\n",
            "        -1.01348393e-01, -1.65038943e+00, -1.83468729e-01,\n",
            "         5.98948419e-01]], dtype=float32)\n",
            "device     = cpu()\n",
            "m          = 16\n",
            "n          = 16\n",
            "p          = 16\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[-4.35158372e-01 -1.25658166e+00  2.82263488e-01 -1.71122360e+00\n",
            "   5.37350357e-01  7.73624003e-01 -5.7...01  6.72981918e-01  3.01030993e-01 -1.78013062e+00\n",
            "  -1.01348393e-01 -1.65038943e+00 -1.83468729e-01  5.98948419e-01]])\n",
            "        self       = needle.Tensor([[ 0.5459043  -1.0255798  -0.9868334  -1.1512803  -0.11136018  0.58853644\n",
            "   0.40300608  0.4074028  -0.3...591106 -0.28878465  1.204689   -0.5803022  -0.39473587 -0.02506427\n",
            "  -1.0649109   0.90236807 -2.1894739   1.4513721 ]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[ 0.5459043  -1.0255798  -0.9868334  -1.1512803  -0.11136018  0.58853644\n",
            "   0.40300608  0.4074028  -0....1  6.72981918e-01  3.01030993e-01 -1.78013062e+00\n",
            "  -1.01348393e-01 -1.65038943e+00 -1.83468729e-01  5.98948419e-01]]))\n",
            "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d3b21d0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[ 0.5459043  -1.0255798  -0.9868334  -1.1512803  -0.11136018  0.58853644\n",
            "   0.40300608  0.4074028  -0....1  6.72981918e-01  3.01030993e-01 -1.78013062e+00\n",
            "  -1.01348393e-01 -1.65038943e+00 -1.83468729e-01  5.98948419e-01]]))\n",
            "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d3b21d0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3b3790>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3b3790>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d3b21d0>\n",
            "a = NDArray([[ 0.5459043  -1.0255798  -0.9868334  -1.1512803  -0.11136018  0.58853644\n",
            "   0.40300608  0.4074028  -0.3441802...8465  1.204689   -0.5803022  -0.39473587 -0.02506427\n",
            "  -1.0649109   0.90236807 -2.1894739   1.4513721 ]], device=cpu())\n",
            "b = NDArray([[-4.35158372e-01 -1.25658166e+00  2.82263488e-01 -1.71122360e+00\n",
            "   5.37350357e-01  7.73624003e-01 -5.7591033...e-01  3.01030993e-01 -1.78013062e+00\n",
            "  -1.01348393e-01 -1.65038943e+00 -1.83468729e-01  5.98948419e-01]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[ 0.5459043  -1.0255798  -0.9868334  -1.1512803  -0.11136018  0.58853644\n",
            "   0.40300608  0.4074028  -0.3441802...8465  1.204689   -0.5803022  -0.39473587 -0.02506427\n",
            "  -1.0649109   0.90236807 -2.1894739   1.4513721 ]], device=cpu())\n",
            "b          = NDArray([[-4.35158372e-01 -1.25658166e+00  2.82263488e-01 -1.71122360e+00\n",
            "   5.37350357e-01  7.73624003e-01 -5.7591033...e-01  3.01030993e-01 -1.78013062e+00\n",
            "  -1.01348393e-01 -1.65038943e+00 -1.83468729e-01  5.98948419e-01]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d3b21d0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:228: NotImplementedError\n",
            "\u001b[31m\u001b[1m______________________________________ test_matmul[cpu-8-8-8] ______________________________________\u001b[0m\n",
            "\n",
            "m = 8, n = 8, p = 8, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[ 1.7708541   0.7078324  -1.604868   -0.42942074  1.4245789   0.10273963\n",
            "   1.6898173  -0.8045091 ]\n",
            " [-...562  -2.2599277 ]\n",
            " [-1.8563243  -1.3699316   0.25954303  0.6353133  -0.26719326  2.0503454\n",
            "  -0.34281254 -0.13744056]])\n",
            "B          = needle.Tensor([[-0.44083533  1.1039202  -0.11487813 -0.5188742   1.5744855   1.1826057\n",
            "   0.9007966  -1.3814906 ]\n",
            " [-1...7794  0.1676047 ]\n",
            " [ 0.4303807  -0.3919837  -0.23323113 -0.5806714   0.09621757  1.1046574\n",
            "   0.18735284  0.65478384]])\n",
            "_A         = array([[ 1.7708541 ,  0.7078324 , -1.604868  , -0.42942074,  1.4245789 ,\n",
            "         0.10273963,  1.6898173 , -0.8045091 ...3 , -1.3699316 ,  0.25954303,  0.6353133 , -0.26719326,\n",
            "         2.0503454 , -0.34281254, -0.13744056]], dtype=float32)\n",
            "_B         = array([[-0.44083533,  1.1039202 , -0.11487813, -0.5188742 ,  1.5744855 ,\n",
            "         1.1826057 ,  0.9007966 , -1.3814906 ...7 , -0.3919837 , -0.23323113, -0.5806714 ,  0.09621757,\n",
            "         1.1046574 ,  0.18735284,  0.65478384]], dtype=float32)\n",
            "device     = cpu()\n",
            "m          = 8\n",
            "n          = 8\n",
            "p          = 8\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[-0.44083533  1.1039202  -0.11487813 -0.5188742   1.5744855   1.1826057\n",
            "   0.9007966  -1.3814906 ]\n",
            " [-1...7794  0.1676047 ]\n",
            " [ 0.4303807  -0.3919837  -0.23323113 -0.5806714   0.09621757  1.1046574\n",
            "   0.18735284  0.65478384]])\n",
            "        self       = needle.Tensor([[ 1.7708541   0.7078324  -1.604868   -0.42942074  1.4245789   0.10273963\n",
            "   1.6898173  -0.8045091 ]\n",
            " [-...562  -2.2599277 ]\n",
            " [-1.8563243  -1.3699316   0.25954303  0.6353133  -0.26719326  2.0503454\n",
            "  -0.34281254 -0.13744056]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[ 1.7708541   0.7078324  -1.604868   -0.42942074  1.4245789   0.10273963\n",
            "   1.6898173  -0.8045091 ]\n",
            " [...794  0.1676047 ]\n",
            " [ 0.4303807  -0.3919837  -0.23323113 -0.5806714   0.09621757  1.1046574\n",
            "   0.18735284  0.65478384]]))\n",
            "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d471b10>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[ 1.7708541   0.7078324  -1.604868   -0.42942074  1.4245789   0.10273963\n",
            "   1.6898173  -0.8045091 ]\n",
            " [...794  0.1676047 ]\n",
            " [ 0.4303807  -0.3919837  -0.23323113 -0.5806714   0.09621757  1.1046574\n",
            "   0.18735284  0.65478384]]))\n",
            "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d471b10>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d4702e0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d4702e0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d471b10>\n",
            "a = NDArray([[ 1.7708541   0.7078324  -1.604868   -0.42942074  1.4245789   0.10273963\n",
            "   1.6898173  -0.8045091 ]\n",
            " [-0.6371...7 ]\n",
            " [-1.8563243  -1.3699316   0.25954303  0.6353133  -0.26719326  2.0503454\n",
            "  -0.34281254 -0.13744056]], device=cpu())\n",
            "b = NDArray([[-0.44083533  1.1039202  -0.11487813 -0.5188742   1.5744855   1.1826057\n",
            "   0.9007966  -1.3814906 ]\n",
            " [-1.18248...7 ]\n",
            " [ 0.4303807  -0.3919837  -0.23323113 -0.5806714   0.09621757  1.1046574\n",
            "   0.18735284  0.65478384]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[ 1.7708541   0.7078324  -1.604868   -0.42942074  1.4245789   0.10273963\n",
            "   1.6898173  -0.8045091 ]\n",
            " [-0.6371...7 ]\n",
            " [-1.8563243  -1.3699316   0.25954303  0.6353133  -0.26719326  2.0503454\n",
            "  -0.34281254 -0.13744056]], device=cpu())\n",
            "b          = NDArray([[-0.44083533  1.1039202  -0.11487813 -0.5188742   1.5744855   1.1826057\n",
            "   0.9007966  -1.3814906 ]\n",
            " [-1.18248...7 ]\n",
            " [ 0.4303807  -0.3919837  -0.23323113 -0.5806714   0.09621757  1.1046574\n",
            "   0.18735284  0.65478384]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d471b10>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:228: NotImplementedError\n",
            "\u001b[31m\u001b[1m______________________________________ test_matmul[cpu-1-2-3] ______________________________________\u001b[0m\n",
            "\n",
            "m = 1, n = 2, p = 3, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[-0.64681673  0.71601075]])\n",
            "B          = needle.Tensor([[-3.1739824  -1.4351766  -0.34335443]\n",
            " [-1.8365756   0.56197065  0.2013918 ]])\n",
            "_A         = array([[-0.64681673,  0.71601075]], dtype=float32)\n",
            "_B         = array([[-3.1739824 , -1.4351766 , -0.34335443],\n",
            "       [-1.8365756 ,  0.56197065,  0.2013918 ]], dtype=float32)\n",
            "device     = cpu()\n",
            "m          = 1\n",
            "n          = 2\n",
            "p          = 3\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[-3.1739824  -1.4351766  -0.34335443]\n",
            " [-1.8365756   0.56197065  0.2013918 ]])\n",
            "        self       = needle.Tensor([[-0.64681673  0.71601075]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-0.64681673  0.71601075]]), needle.Tensor([[-3.1739824  -1.4351766  -0.34335443]\n",
            " [-1.8365756   0.56197065  0.2013918 ]]))\n",
            "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d2ec8e0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-0.64681673  0.71601075]]), needle.Tensor([[-3.1739824  -1.4351766  -0.34335443]\n",
            " [-1.8365756   0.56197065  0.2013918 ]]))\n",
            "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d2ec8e0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d2efee0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d2efee0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d2ec8e0>\n",
            "a = NDArray([[-0.64681673  0.71601075]], device=cpu())\n",
            "b = NDArray([[-3.1739824  -1.4351766  -0.34335443]\n",
            " [-1.8365756   0.56197065  0.2013918 ]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[-0.64681673  0.71601075]], device=cpu())\n",
            "b          = NDArray([[-3.1739824  -1.4351766  -0.34335443]\n",
            " [-1.8365756   0.56197065  0.2013918 ]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d2ec8e0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:228: NotImplementedError\n",
            "\u001b[31m\u001b[1m______________________________________ test_matmul[cpu-3-4-5] ______________________________________\u001b[0m\n",
            "\n",
            "m = 3, n = 4, p = 5, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[-0.9273708   0.08778039 -1.5268633  -0.9182085 ]\n",
            " [-0.39510968 -1.6226748   1.0431075  -0.6696007 ]\n",
            " [ 1.0201294  -0.15438324  0.3058363  -0.92267954]])\n",
            "B          = needle.Tensor([[-0.31467086 -0.42952046  0.23011792  1.3751211  -0.5697806 ]\n",
            " [-0.16880828  0.35674536  0.8592977  -0....0913   0.00476667  0.24025472 -0.6887998  -0.3099795 ]\n",
            " [ 0.7594371  -0.02695737 -0.5762634   0.7085369   0.7130303 ]])\n",
            "_A         = array([[-0.9273708 ,  0.08778039, -1.5268633 , -0.9182085 ],\n",
            "       [-0.39510968, -1.6226748 ,  1.0431075 , -0.6696007 ],\n",
            "       [ 1.0201294 , -0.15438324,  0.3058363 , -0.92267954]],\n",
            "      dtype=float32)\n",
            "_B         = array([[-0.31467086, -0.42952046,  0.23011792,  1.3751211 , -0.5697806 ],\n",
            "       [-0.16880828,  0.35674536,  0.8592977...6887998 , -0.3099795 ],\n",
            "       [ 0.7594371 , -0.02695737, -0.5762634 ,  0.7085369 ,  0.7130303 ]],\n",
            "      dtype=float32)\n",
            "device     = cpu()\n",
            "m          = 3\n",
            "n          = 4\n",
            "p          = 5\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[-0.31467086 -0.42952046  0.23011792  1.3751211  -0.5697806 ]\n",
            " [-0.16880828  0.35674536  0.8592977  -0....0913   0.00476667  0.24025472 -0.6887998  -0.3099795 ]\n",
            " [ 0.7594371  -0.02695737 -0.5762634   0.7085369   0.7130303 ]])\n",
            "        self       = needle.Tensor([[-0.9273708   0.08778039 -1.5268633  -0.9182085 ]\n",
            " [-0.39510968 -1.6226748   1.0431075  -0.6696007 ]\n",
            " [ 1.0201294  -0.15438324  0.3058363  -0.92267954]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-0.9273708   0.08778039 -1.5268633  -0.9182085 ]\n",
            " [-0.39510968 -1.6226748   1.0431075  -0.6696007 ]\n",
            " ...913   0.00476667  0.24025472 -0.6887998  -0.3099795 ]\n",
            " [ 0.7594371  -0.02695737 -0.5762634   0.7085369   0.7130303 ]]))\n",
            "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d52ddb0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-0.9273708   0.08778039 -1.5268633  -0.9182085 ]\n",
            " [-0.39510968 -1.6226748   1.0431075  -0.6696007 ]\n",
            " ...913   0.00476667  0.24025472 -0.6887998  -0.3099795 ]\n",
            " [ 0.7594371  -0.02695737 -0.5762634   0.7085369   0.7130303 ]]))\n",
            "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d52ddb0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d52f7c0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d52f7c0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d52ddb0>\n",
            "a = NDArray([[-0.9273708   0.08778039 -1.5268633  -0.9182085 ]\n",
            " [-0.39510968 -1.6226748   1.0431075  -0.6696007 ]\n",
            " [ 1.0201294  -0.15438324  0.3058363  -0.92267954]], device=cpu())\n",
            "b = NDArray([[-0.31467086 -0.42952046  0.23011792  1.3751211  -0.5697806 ]\n",
            " [-0.16880828  0.35674536  0.8592977  -0.115372...667  0.24025472 -0.6887998  -0.3099795 ]\n",
            " [ 0.7594371  -0.02695737 -0.5762634   0.7085369   0.7130303 ]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[-0.9273708   0.08778039 -1.5268633  -0.9182085 ]\n",
            " [-0.39510968 -1.6226748   1.0431075  -0.6696007 ]\n",
            " [ 1.0201294  -0.15438324  0.3058363  -0.92267954]], device=cpu())\n",
            "b          = NDArray([[-0.31467086 -0.42952046  0.23011792  1.3751211  -0.5697806 ]\n",
            " [-0.16880828  0.35674536  0.8592977  -0.115372...667  0.24025472 -0.6887998  -0.3099795 ]\n",
            " [ 0.7594371  -0.02695737 -0.5762634   0.7085369   0.7130303 ]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d52ddb0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:228: NotImplementedError\n",
            "\u001b[31m\u001b[1m______________________________________ test_matmul[cpu-5-4-3] ______________________________________\u001b[0m\n",
            "\n",
            "m = 5, n = 4, p = 3, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[ 0.6125179   0.6676573  -0.3971575   0.204205  ]\n",
            " [ 0.31498376  0.12197978 -1.7589004  -0.0187833 ]\n",
            " [...    0.03260794]\n",
            " [-1.808827   -1.4966553  -2.9709995  -0.22909594]\n",
            " [ 1.6263021   1.7526253  -1.1286101   0.65199554]])\n",
            "B          = needle.Tensor([[ 1.2609208  -0.30388004 -1.9670581 ]\n",
            " [ 1.4251591  -0.3573957  -0.39176092]\n",
            " [ 1.7144959   0.47264996  0.84813285]\n",
            " [ 1.608428   -0.14277244 -0.96933216]])\n",
            "_A         = array([[ 0.6125179 ,  0.6676573 , -0.3971575 ,  0.204205  ],\n",
            "       [ 0.31498376,  0.12197978, -1.7589004 , -0.0187833...4966553 , -2.9709995 , -0.22909594],\n",
            "       [ 1.6263021 ,  1.7526253 , -1.1286101 ,  0.65199554]],\n",
            "      dtype=float32)\n",
            "_B         = array([[ 1.2609208 , -0.30388004, -1.9670581 ],\n",
            "       [ 1.4251591 , -0.3573957 , -0.39176092],\n",
            "       [ 1.7144959 ,  0.47264996,  0.84813285],\n",
            "       [ 1.608428  , -0.14277244, -0.96933216]], dtype=float32)\n",
            "device     = cpu()\n",
            "m          = 5\n",
            "n          = 4\n",
            "p          = 3\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[ 1.2609208  -0.30388004 -1.9670581 ]\n",
            " [ 1.4251591  -0.3573957  -0.39176092]\n",
            " [ 1.7144959   0.47264996  0.84813285]\n",
            " [ 1.608428   -0.14277244 -0.96933216]])\n",
            "        self       = needle.Tensor([[ 0.6125179   0.6676573  -0.3971575   0.204205  ]\n",
            " [ 0.31498376  0.12197978 -1.7589004  -0.0187833 ]\n",
            " [...    0.03260794]\n",
            " [-1.808827   -1.4966553  -2.9709995  -0.22909594]\n",
            " [ 1.6263021   1.7526253  -1.1286101   0.65199554]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[ 0.6125179   0.6676573  -0.3971575   0.204205  ]\n",
            " [ 0.31498376  0.12197978 -1.7589004  -0.0187833 ]\n",
            " ... [ 1.4251591  -0.3573957  -0.39176092]\n",
            " [ 1.7144959   0.47264996  0.84813285]\n",
            " [ 1.608428   -0.14277244 -0.96933216]]))\n",
            "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5d388f8100>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[ 0.6125179   0.6676573  -0.3971575   0.204205  ]\n",
            " [ 0.31498376  0.12197978 -1.7589004  -0.0187833 ]\n",
            " ... [ 1.4251591  -0.3573957  -0.39176092]\n",
            " [ 1.7144959   0.47264996  0.84813285]\n",
            " [ 1.608428   -0.14277244 -0.96933216]]))\n",
            "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7f5d388f8100>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5d388f8340>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5d388f8340>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.MatMul object at 0x7f5d388f8100>\n",
            "a = NDArray([[ 0.6125179   0.6676573  -0.3971575   0.204205  ]\n",
            " [ 0.31498376  0.12197978 -1.7589004  -0.0187833 ]\n",
            " [ 0.351...]\n",
            " [-1.808827   -1.4966553  -2.9709995  -0.22909594]\n",
            " [ 1.6263021   1.7526253  -1.1286101   0.65199554]], device=cpu())\n",
            "b = NDArray([[ 1.2609208  -0.30388004 -1.9670581 ]\n",
            " [ 1.4251591  -0.3573957  -0.39176092]\n",
            " [ 1.7144959   0.47264996  0.84813285]\n",
            " [ 1.608428   -0.14277244 -0.96933216]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[ 0.6125179   0.6676573  -0.3971575   0.204205  ]\n",
            " [ 0.31498376  0.12197978 -1.7589004  -0.0187833 ]\n",
            " [ 0.351...]\n",
            " [-1.808827   -1.4966553  -2.9709995  -0.22909594]\n",
            " [ 1.6263021   1.7526253  -1.1286101   0.65199554]], device=cpu())\n",
            "b          = NDArray([[ 1.2609208  -0.30388004 -1.9670581 ]\n",
            " [ 1.4251591  -0.3573957  -0.39176092]\n",
            " [ 1.7144959   0.47264996  0.84813285]\n",
            " [ 1.608428   -0.14277244 -0.96933216]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5d388f8100>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:228: NotImplementedError\n",
            "\u001b[31m\u001b[1m____________________________________ test_matmul[cpu-16-16-32] _____________________________________\u001b[0m\n",
            "\n",
            "m = 16, n = 16, p = 32, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[ 0.7993588   0.01489265  0.7618823   0.42314237 -0.7378046   0.05537529\n",
            "  -0.30522415 -0.20304653 -0.0...514616 -1.2945547  -2.0282612   2.2147806  -0.60998374 -0.19773254\n",
            "  -1.1516168   1.5212711  -0.36488008 -0.17315696]])\n",
            "B          = needle.Tensor([[ 0.03036788 -0.02900854  1.3998724  -1.8398094   0.60762084 -0.6432508\n",
            "  -0.4321556  -0.6426842   0.39...78651  0.10203802\n",
            "   1.2234045   2.5558736   0.20855476  1.7366973   0.2988114  -0.6695927\n",
            "   0.62757087  1.3566512 ]])\n",
            "_A         = array([[ 0.7993588 ,  0.01489265,  0.7618823 ,  0.42314237, -0.7378046 ,\n",
            "         0.05537529, -0.30522415, -0.20304653...147806 ,\n",
            "        -0.60998374, -0.19773254, -1.1516168 ,  1.5212711 , -0.36488008,\n",
            "        -0.17315696]], dtype=float32)\n",
            "_B         = array([[ 0.03036788, -0.02900854,  1.3998724 , -1.8398094 ,  0.60762084,\n",
            "        -0.6432508 , -0.4321556 , -0.6426842 ...     2.5558736 ,  0.20855476,  1.7366973 ,  0.2988114 , -0.6695927 ,\n",
            "         0.62757087,  1.3566512 ]], dtype=float32)\n",
            "device     = cpu()\n",
            "m          = 16\n",
            "n          = 16\n",
            "p          = 32\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[ 0.03036788 -0.02900854  1.3998724  -1.8398094   0.60762084 -0.6432508\n",
            "  -0.4321556  -0.6426842   0.39...78651  0.10203802\n",
            "   1.2234045   2.5558736   0.20855476  1.7366973   0.2988114  -0.6695927\n",
            "   0.62757087  1.3566512 ]])\n",
            "        self       = needle.Tensor([[ 0.7993588   0.01489265  0.7618823   0.42314237 -0.7378046   0.05537529\n",
            "  -0.30522415 -0.20304653 -0.0...514616 -1.2945547  -2.0282612   2.2147806  -0.60998374 -0.19773254\n",
            "  -1.1516168   1.5212711  -0.36488008 -0.17315696]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[ 0.7993588   0.01489265  0.7618823   0.42314237 -0.7378046   0.05537529\n",
            "  -0.30522415 -0.20304653 -0....8651  0.10203802\n",
            "   1.2234045   2.5558736   0.20855476  1.7366973   0.2988114  -0.6695927\n",
            "   0.62757087  1.3566512 ]]))\n",
            "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d39cee0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[ 0.7993588   0.01489265  0.7618823   0.42314237 -0.7378046   0.05537529\n",
            "  -0.30522415 -0.20304653 -0....8651  0.10203802\n",
            "   1.2234045   2.5558736   0.20855476  1.7366973   0.2988114  -0.6695927\n",
            "   0.62757087  1.3566512 ]]))\n",
            "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d39cee0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d39d3c0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d39d3c0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d39cee0>\n",
            "a = NDArray([[ 0.7993588   0.01489265  0.7618823   0.42314237 -0.7378046   0.05537529\n",
            "  -0.30522415 -0.20304653 -0.0070443...547  -2.0282612   2.2147806  -0.60998374 -0.19773254\n",
            "  -1.1516168   1.5212711  -0.36488008 -0.17315696]], device=cpu())\n",
            "b = NDArray([[ 0.03036788 -0.02900854  1.3998724  -1.8398094   0.60762084 -0.6432508\n",
            "  -0.4321556  -0.6426842   0.39959696...802\n",
            "   1.2234045   2.5558736   0.20855476  1.7366973   0.2988114  -0.6695927\n",
            "   0.62757087  1.3566512 ]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[ 0.7993588   0.01489265  0.7618823   0.42314237 -0.7378046   0.05537529\n",
            "  -0.30522415 -0.20304653 -0.0070443...547  -2.0282612   2.2147806  -0.60998374 -0.19773254\n",
            "  -1.1516168   1.5212711  -0.36488008 -0.17315696]], device=cpu())\n",
            "b          = NDArray([[ 0.03036788 -0.02900854  1.3998724  -1.8398094   0.60762084 -0.6432508\n",
            "  -0.4321556  -0.6426842   0.39959696...802\n",
            "   1.2234045   2.5558736   0.20855476  1.7366973   0.2988114  -0.6695927\n",
            "   0.62757087  1.3566512 ]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d39cee0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:228: NotImplementedError\n",
            "\u001b[31m\u001b[1m____________________________________ test_matmul[cpu-64-64-64] _____________________________________\u001b[0m\n",
            "\n",
            "m = 64, n = 64, p = 64, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[ 0.32083803  0.5895268  -0.9540305  ... -0.37398282 -0.16691524\n",
            "  -0.23749262]\n",
            " [-0.6142388  -0.656909....32286686 -0.15132289\n",
            "  -0.35855898]\n",
            " [-0.41467297  0.05066269 -0.42384505 ...  1.1080023   0.35687977\n",
            "   0.245734  ]])\n",
            "B          = needle.Tensor([[ 0.5813029   0.8870155  -0.588916   ...  2.599509    0.3715275\n",
            "   1.2655233 ]\n",
            " [-0.6687268  -0.0805072...0.29515836 -0.42044154\n",
            "   0.6692859 ]\n",
            " [ 0.9936586  -0.9664672  -0.564469   ... -2.5510733   1.7028129\n",
            "  -1.5053847 ]])\n",
            "_A         = array([[ 0.32083803,  0.5895268 , -0.9540305 , ..., -0.37398282,\n",
            "        -0.16691524, -0.23749262],\n",
            "       [-0.6142388...8],\n",
            "       [-0.41467297,  0.05066269, -0.42384505, ...,  1.1080023 ,\n",
            "         0.35687977,  0.245734  ]], dtype=float32)\n",
            "_B         = array([[ 0.5813029 ,  0.8870155 , -0.588916  , ...,  2.599509  ,\n",
            "         0.3715275 ,  1.2655233 ],\n",
            "       [-0.6687268... ],\n",
            "       [ 0.9936586 , -0.9664672 , -0.564469  , ..., -2.5510733 ,\n",
            "         1.7028129 , -1.5053847 ]], dtype=float32)\n",
            "device     = cpu()\n",
            "m          = 64\n",
            "n          = 64\n",
            "p          = 64\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[ 0.5813029   0.8870155  -0.588916   ...  2.599509    0.3715275\n",
            "   1.2655233 ]\n",
            " [-0.6687268  -0.0805072...0.29515836 -0.42044154\n",
            "   0.6692859 ]\n",
            " [ 0.9936586  -0.9664672  -0.564469   ... -2.5510733   1.7028129\n",
            "  -1.5053847 ]])\n",
            "        self       = needle.Tensor([[ 0.32083803  0.5895268  -0.9540305  ... -0.37398282 -0.16691524\n",
            "  -0.23749262]\n",
            " [-0.6142388  -0.656909....32286686 -0.15132289\n",
            "  -0.35855898]\n",
            " [-0.41467297  0.05066269 -0.42384505 ...  1.1080023   0.35687977\n",
            "   0.245734  ]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[ 0.32083803  0.5895268  -0.9540305  ... -0.37398282 -0.16691524\n",
            "  -0.23749262]\n",
            " [-0.6142388  -0.65690....29515836 -0.42044154\n",
            "   0.6692859 ]\n",
            " [ 0.9936586  -0.9664672  -0.564469   ... -2.5510733   1.7028129\n",
            "  -1.5053847 ]]))\n",
            "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d52dc30>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[ 0.32083803  0.5895268  -0.9540305  ... -0.37398282 -0.16691524\n",
            "  -0.23749262]\n",
            " [-0.6142388  -0.65690....29515836 -0.42044154\n",
            "   0.6692859 ]\n",
            " [ 0.9936586  -0.9664672  -0.564469   ... -2.5510733   1.7028129\n",
            "  -1.5053847 ]]))\n",
            "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d52dc30>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d52ee00>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d52ee00>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d52dc30>\n",
            "a = NDArray([[ 0.32083803  0.5895268  -0.9540305  ... -0.37398282 -0.16691524\n",
            "  -0.23749262]\n",
            " [-0.6142388  -0.6569092  -0....5132289\n",
            "  -0.35855898]\n",
            " [-0.41467297  0.05066269 -0.42384505 ...  1.1080023   0.35687977\n",
            "   0.245734  ]], device=cpu())\n",
            "b = NDArray([[ 0.5813029   0.8870155  -0.588916   ...  2.599509    0.3715275\n",
            "   1.2655233 ]\n",
            " [-0.6687268  -0.08050723  1.7...42044154\n",
            "   0.6692859 ]\n",
            " [ 0.9936586  -0.9664672  -0.564469   ... -2.5510733   1.7028129\n",
            "  -1.5053847 ]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[ 0.32083803  0.5895268  -0.9540305  ... -0.37398282 -0.16691524\n",
            "  -0.23749262]\n",
            " [-0.6142388  -0.6569092  -0....5132289\n",
            "  -0.35855898]\n",
            " [-0.41467297  0.05066269 -0.42384505 ...  1.1080023   0.35687977\n",
            "   0.245734  ]], device=cpu())\n",
            "b          = NDArray([[ 0.5813029   0.8870155  -0.588916   ...  2.599509    0.3715275\n",
            "   1.2655233 ]\n",
            " [-0.6687268  -0.08050723  1.7...42044154\n",
            "   0.6692859 ]\n",
            " [ 0.9936586  -0.9664672  -0.564469   ... -2.5510733   1.7028129\n",
            "  -1.5053847 ]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d52dc30>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:228: NotImplementedError\n",
            "\u001b[31m\u001b[1m____________________________________ test_matmul[cpu-72-72-72] _____________________________________\u001b[0m\n",
            "\n",
            "m = 72, n = 72, p = 72, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[ 0.9313206  -2.2504177  -1.0806347  ... -0.10637094  0.48675302\n",
            "   2.1051116 ]\n",
            " [ 1.7502499  -2.261720....1096429  -0.62971306\n",
            "  -0.4454478 ]\n",
            " [ 0.38306278 -1.3725233   0.94347966 ... -0.5464146  -0.24303754\n",
            "   0.52405655]])\n",
            "B          = needle.Tensor([[-0.6836024  -0.30054682 -0.9792686  ...  0.50831896  0.201864\n",
            "  -0.025059  ]\n",
            " [ 0.26857692  1.6424308 ...1.6865423  -0.24971327\n",
            "   0.983084  ]\n",
            " [-0.09598085 -0.4518927   0.6751473  ...  0.37912768 -1.6585017\n",
            "  -0.19639991]])\n",
            "_A         = array([[ 0.9313206 , -2.2504177 , -1.0806347 , ..., -0.10637094,\n",
            "         0.48675302,  2.1051116 ],\n",
            "       [ 1.7502499... ],\n",
            "       [ 0.38306278, -1.3725233 ,  0.94347966, ..., -0.5464146 ,\n",
            "        -0.24303754,  0.52405655]], dtype=float32)\n",
            "_B         = array([[-0.6836024 , -0.30054682, -0.9792686 , ...,  0.50831896,\n",
            "         0.201864  , -0.025059  ],\n",
            "       [ 0.2685769... ],\n",
            "       [-0.09598085, -0.4518927 ,  0.6751473 , ...,  0.37912768,\n",
            "        -1.6585017 , -0.19639991]], dtype=float32)\n",
            "device     = cpu()\n",
            "m          = 72\n",
            "n          = 72\n",
            "p          = 72\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[-0.6836024  -0.30054682 -0.9792686  ...  0.50831896  0.201864\n",
            "  -0.025059  ]\n",
            " [ 0.26857692  1.6424308 ...1.6865423  -0.24971327\n",
            "   0.983084  ]\n",
            " [-0.09598085 -0.4518927   0.6751473  ...  0.37912768 -1.6585017\n",
            "  -0.19639991]])\n",
            "        self       = needle.Tensor([[ 0.9313206  -2.2504177  -1.0806347  ... -0.10637094  0.48675302\n",
            "   2.1051116 ]\n",
            " [ 1.7502499  -2.261720....1096429  -0.62971306\n",
            "  -0.4454478 ]\n",
            " [ 0.38306278 -1.3725233   0.94347966 ... -0.5464146  -0.24303754\n",
            "   0.52405655]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[ 0.9313206  -2.2504177  -1.0806347  ... -0.10637094  0.48675302\n",
            "   2.1051116 ]\n",
            " [ 1.7502499  -2.26172....6865423  -0.24971327\n",
            "   0.983084  ]\n",
            " [-0.09598085 -0.4518927   0.6751473  ...  0.37912768 -1.6585017\n",
            "  -0.19639991]]))\n",
            "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d3ee320>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[ 0.9313206  -2.2504177  -1.0806347  ... -0.10637094  0.48675302\n",
            "   2.1051116 ]\n",
            " [ 1.7502499  -2.26172....6865423  -0.24971327\n",
            "   0.983084  ]\n",
            " [-0.09598085 -0.4518927   0.6751473  ...  0.37912768 -1.6585017\n",
            "  -0.19639991]]))\n",
            "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d3ee320>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3eeec0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3eeec0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d3ee320>\n",
            "a = NDArray([[ 0.9313206  -2.2504177  -1.0806347  ... -0.10637094  0.48675302\n",
            "   2.1051116 ]\n",
            " [ 1.7502499  -2.2617202   0....2971306\n",
            "  -0.4454478 ]\n",
            " [ 0.38306278 -1.3725233   0.94347966 ... -0.5464146  -0.24303754\n",
            "   0.52405655]], device=cpu())\n",
            "b = NDArray([[-0.6836024  -0.30054682 -0.9792686  ...  0.50831896  0.201864\n",
            "  -0.025059  ]\n",
            " [ 0.26857692  1.6424308  -0.44...24971327\n",
            "   0.983084  ]\n",
            " [-0.09598085 -0.4518927   0.6751473  ...  0.37912768 -1.6585017\n",
            "  -0.19639991]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[ 0.9313206  -2.2504177  -1.0806347  ... -0.10637094  0.48675302\n",
            "   2.1051116 ]\n",
            " [ 1.7502499  -2.2617202   0....2971306\n",
            "  -0.4454478 ]\n",
            " [ 0.38306278 -1.3725233   0.94347966 ... -0.5464146  -0.24303754\n",
            "   0.52405655]], device=cpu())\n",
            "b          = NDArray([[-0.6836024  -0.30054682 -0.9792686  ...  0.50831896  0.201864\n",
            "  -0.025059  ]\n",
            " [ 0.26857692  1.6424308  -0.44...24971327\n",
            "   0.983084  ]\n",
            " [-0.09598085 -0.4518927   0.6751473  ...  0.37912768 -1.6585017\n",
            "  -0.19639991]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d3ee320>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:228: NotImplementedError\n",
            "\u001b[31m\u001b[1m____________________________________ test_matmul[cpu-72-73-74] _____________________________________\u001b[0m\n",
            "\n",
            "m = 72, n = 73, p = 74, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[ 1.2301393  -1.7680473   0.86831224 ... -1.2118692  -0.7857895\n",
            "  -0.9395097 ]\n",
            " [-0.24594781 -0.0432007...0.18608749 -0.24767143\n",
            "   1.4238417 ]\n",
            " [ 2.4232516  -1.6076419  -1.1701185  ... -0.11754402  0.4648232\n",
            "  -0.15135539]])\n",
            "B          = needle.Tensor([[-0.2883005   0.18553501  0.59806883 ...  1.346098   -1.604723\n",
            "   0.5156507 ]\n",
            " [ 0.19386627  0.06776064...-0.19546118  1.6449244\n",
            "   0.5804095 ]\n",
            " [-0.12253371 -0.4244525   0.42172652 ...  1.8422971  -1.8658501\n",
            "  -0.12115688]])\n",
            "_A         = array([[ 1.2301393 , -1.7680473 ,  0.86831224, ..., -1.2118692 ,\n",
            "        -0.7857895 , -0.9395097 ],\n",
            "       [-0.2459478... ],\n",
            "       [ 2.4232516 , -1.6076419 , -1.1701185 , ..., -0.11754402,\n",
            "         0.4648232 , -0.15135539]], dtype=float32)\n",
            "_B         = array([[-0.2883005 ,  0.18553501,  0.59806883, ...,  1.346098  ,\n",
            "        -1.604723  ,  0.5156507 ],\n",
            "       [ 0.1938662... ],\n",
            "       [-0.12253371, -0.4244525 ,  0.42172652, ...,  1.8422971 ,\n",
            "        -1.8658501 , -0.12115688]], dtype=float32)\n",
            "device     = cpu()\n",
            "m          = 72\n",
            "n          = 73\n",
            "p          = 74\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[-0.2883005   0.18553501  0.59806883 ...  1.346098   -1.604723\n",
            "   0.5156507 ]\n",
            " [ 0.19386627  0.06776064...-0.19546118  1.6449244\n",
            "   0.5804095 ]\n",
            " [-0.12253371 -0.4244525   0.42172652 ...  1.8422971  -1.8658501\n",
            "  -0.12115688]])\n",
            "        self       = needle.Tensor([[ 1.2301393  -1.7680473   0.86831224 ... -1.2118692  -0.7857895\n",
            "  -0.9395097 ]\n",
            " [-0.24594781 -0.0432007...0.18608749 -0.24767143\n",
            "   1.4238417 ]\n",
            " [ 2.4232516  -1.6076419  -1.1701185  ... -0.11754402  0.4648232\n",
            "  -0.15135539]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[ 1.2301393  -1.7680473   0.86831224 ... -1.2118692  -0.7857895\n",
            "  -0.9395097 ]\n",
            " [-0.24594781 -0.043200...0.19546118  1.6449244\n",
            "   0.5804095 ]\n",
            " [-0.12253371 -0.4244525   0.42172652 ...  1.8422971  -1.8658501\n",
            "  -0.12115688]]))\n",
            "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d29e950>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[ 1.2301393  -1.7680473   0.86831224 ... -1.2118692  -0.7857895\n",
            "  -0.9395097 ]\n",
            " [-0.24594781 -0.043200...0.19546118  1.6449244\n",
            "   0.5804095 ]\n",
            " [-0.12253371 -0.4244525   0.42172652 ...  1.8422971  -1.8658501\n",
            "  -0.12115688]]))\n",
            "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d29e950>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d29dde0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d29dde0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d29e950>\n",
            "a = NDArray([[ 1.2301393  -1.7680473   0.86831224 ... -1.2118692  -0.7857895\n",
            "  -0.9395097 ]\n",
            " [-0.24594781 -0.04320071 -0.3...24767143\n",
            "   1.4238417 ]\n",
            " [ 2.4232516  -1.6076419  -1.1701185  ... -0.11754402  0.4648232\n",
            "  -0.15135539]], device=cpu())\n",
            "b = NDArray([[-0.2883005   0.18553501  0.59806883 ...  1.346098   -1.604723\n",
            "   0.5156507 ]\n",
            " [ 0.19386627  0.06776064  0.83....6449244\n",
            "   0.5804095 ]\n",
            " [-0.12253371 -0.4244525   0.42172652 ...  1.8422971  -1.8658501\n",
            "  -0.12115688]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[ 1.2301393  -1.7680473   0.86831224 ... -1.2118692  -0.7857895\n",
            "  -0.9395097 ]\n",
            " [-0.24594781 -0.04320071 -0.3...24767143\n",
            "   1.4238417 ]\n",
            " [ 2.4232516  -1.6076419  -1.1701185  ... -0.11754402  0.4648232\n",
            "  -0.15135539]], device=cpu())\n",
            "b          = NDArray([[-0.2883005   0.18553501  0.59806883 ...  1.346098   -1.604723\n",
            "   0.5156507 ]\n",
            " [ 0.19386627  0.06776064  0.83....6449244\n",
            "   0.5804095 ]\n",
            " [-0.12253371 -0.4244525   0.42172652 ...  1.8422971  -1.8658501\n",
            "  -0.12115688]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d29e950>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:228: NotImplementedError\n",
            "\u001b[31m\u001b[1m____________________________________ test_matmul[cpu-74-73-72] _____________________________________\u001b[0m\n",
            "\n",
            "m = 74, n = 73, p = 72, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[-0.01246515  0.8519659   0.734272   ...  1.1319168   0.78492314\n",
            "  -0.53943443]\n",
            " [ 0.16450913 -0.655989...-1.0107665   1.0125555\n",
            "  -1.5025924 ]\n",
            " [-0.06283722 -1.0530041  -0.40745488 ... -1.2454727   0.6731567\n",
            "  -1.7810721 ]])\n",
            "B          = needle.Tensor([[-1.377632   -0.12727466 -1.7180446  ...  0.8463711   1.2489613\n",
            "   0.32375348]\n",
            " [ 0.158703   -0.7104815... 0.7065411   0.3959941\n",
            "  -0.28685495]\n",
            " [ 0.4722353   1.4204288  -0.9388694  ... -1.1688057   1.0393434\n",
            "  -1.3324741 ]])\n",
            "_A         = array([[-0.01246515,  0.8519659 ,  0.734272  , ...,  1.1319168 ,\n",
            "         0.78492314, -0.53943443],\n",
            "       [ 0.1645091... ],\n",
            "       [-0.06283722, -1.0530041 , -0.40745488, ..., -1.2454727 ,\n",
            "         0.6731567 , -1.7810721 ]], dtype=float32)\n",
            "_B         = array([[-1.377632  , -0.12727466, -1.7180446 , ...,  0.8463711 ,\n",
            "         1.2489613 ,  0.32375348],\n",
            "       [ 0.158703 ...5],\n",
            "       [ 0.4722353 ,  1.4204288 , -0.9388694 , ..., -1.1688057 ,\n",
            "         1.0393434 , -1.3324741 ]], dtype=float32)\n",
            "device     = cpu()\n",
            "m          = 74\n",
            "n          = 73\n",
            "p          = 72\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[-1.377632   -0.12727466 -1.7180446  ...  0.8463711   1.2489613\n",
            "   0.32375348]\n",
            " [ 0.158703   -0.7104815... 0.7065411   0.3959941\n",
            "  -0.28685495]\n",
            " [ 0.4722353   1.4204288  -0.9388694  ... -1.1688057   1.0393434\n",
            "  -1.3324741 ]])\n",
            "        self       = needle.Tensor([[-0.01246515  0.8519659   0.734272   ...  1.1319168   0.78492314\n",
            "  -0.53943443]\n",
            " [ 0.16450913 -0.655989...-1.0107665   1.0125555\n",
            "  -1.5025924 ]\n",
            " [-0.06283722 -1.0530041  -0.40745488 ... -1.2454727   0.6731567\n",
            "  -1.7810721 ]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-0.01246515  0.8519659   0.734272   ...  1.1319168   0.78492314\n",
            "  -0.53943443]\n",
            " [ 0.16450913 -0.65598...0.7065411   0.3959941\n",
            "  -0.28685495]\n",
            " [ 0.4722353   1.4204288  -0.9388694  ... -1.1688057   1.0393434\n",
            "  -1.3324741 ]]))\n",
            "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5d388f83a0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-0.01246515  0.8519659   0.734272   ...  1.1319168   0.78492314\n",
            "  -0.53943443]\n",
            " [ 0.16450913 -0.65598...0.7065411   0.3959941\n",
            "  -0.28685495]\n",
            " [ 0.4722353   1.4204288  -0.9388694  ... -1.1688057   1.0393434\n",
            "  -1.3324741 ]]))\n",
            "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7f5d388f83a0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5d388f82b0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5d388f82b0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.MatMul object at 0x7f5d388f83a0>\n",
            "a = NDArray([[-0.01246515  0.8519659   0.734272   ...  1.1319168   0.78492314\n",
            "  -0.53943443]\n",
            " [ 0.16450913 -0.65598917 -0.....0125555\n",
            "  -1.5025924 ]\n",
            " [-0.06283722 -1.0530041  -0.40745488 ... -1.2454727   0.6731567\n",
            "  -1.7810721 ]], device=cpu())\n",
            "b = NDArray([[-1.377632   -0.12727466 -1.7180446  ...  0.8463711   1.2489613\n",
            "   0.32375348]\n",
            " [ 0.158703   -0.7104815  -0.0....3959941\n",
            "  -0.28685495]\n",
            " [ 0.4722353   1.4204288  -0.9388694  ... -1.1688057   1.0393434\n",
            "  -1.3324741 ]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[-0.01246515  0.8519659   0.734272   ...  1.1319168   0.78492314\n",
            "  -0.53943443]\n",
            " [ 0.16450913 -0.65598917 -0.....0125555\n",
            "  -1.5025924 ]\n",
            " [-0.06283722 -1.0530041  -0.40745488 ... -1.2454727   0.6731567\n",
            "  -1.7810721 ]], device=cpu())\n",
            "b          = NDArray([[-1.377632   -0.12727466 -1.7180446  ...  0.8463711   1.2489613\n",
            "   0.32375348]\n",
            " [ 0.158703   -0.7104815  -0.0....3959941\n",
            "  -0.28685495]\n",
            " [ 0.4722353   1.4204288  -0.9388694  ... -1.1688057   1.0393434\n",
            "  -1.3324741 ]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5d388f83a0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:228: NotImplementedError\n",
            "\u001b[31m\u001b[1m___________________________________ test_matmul[cpu-128-128-128] ___________________________________\u001b[0m\n",
            "\n",
            "m = 128, n = 128, p = 128, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[ 0.54665184 -2.2036994  -0.37683746 ... -0.6405449   0.3687821\n",
            "  -1.3951297 ]\n",
            " [-0.47832993  1.6353279...-0.95137507  0.5702582\n",
            "  -0.93965185]\n",
            " [ 1.7675116   0.55993146 -0.29148516 ...  0.79557186  0.5079256\n",
            "  -1.2890497 ]])\n",
            "B          = needle.Tensor([[-0.84114873 -0.8431679  -0.8893435  ... -0.8391408   1.8222126\n",
            "  -0.9242148 ]\n",
            " [-0.08694614 -0.4952154...0.81450176  0.04873849\n",
            "   1.1043309 ]\n",
            " [-2.5026178  -0.6078532   0.5073778  ... -0.07948758  0.9346621\n",
            "  -0.32723945]])\n",
            "_A         = array([[ 0.54665184, -2.2036994 , -0.37683746, ..., -0.6405449 ,\n",
            "         0.3687821 , -1.3951297 ],\n",
            "       [-0.4783299...5],\n",
            "       [ 1.7675116 ,  0.55993146, -0.29148516, ...,  0.79557186,\n",
            "         0.5079256 , -1.2890497 ]], dtype=float32)\n",
            "_B         = array([[-0.84114873, -0.8431679 , -0.8893435 , ..., -0.8391408 ,\n",
            "         1.8222126 , -0.9242148 ],\n",
            "       [-0.0869461... ],\n",
            "       [-2.5026178 , -0.6078532 ,  0.5073778 , ..., -0.07948758,\n",
            "         0.9346621 , -0.32723945]], dtype=float32)\n",
            "device     = cpu()\n",
            "m          = 128\n",
            "n          = 128\n",
            "p          = 128\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:342: in __matmul__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.MatMul()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
            "        other      = needle.Tensor([[-0.84114873 -0.8431679  -0.8893435  ... -0.8391408   1.8222126\n",
            "  -0.9242148 ]\n",
            " [-0.08694614 -0.4952154...0.81450176  0.04873849\n",
            "   1.1043309 ]\n",
            " [-2.5026178  -0.6078532   0.5073778  ... -0.07948758  0.9346621\n",
            "  -0.32723945]])\n",
            "        self       = needle.Tensor([[ 0.54665184 -2.2036994  -0.37683746 ... -0.6405449   0.3687821\n",
            "  -1.3951297 ]\n",
            " [-0.47832993  1.6353279...-0.95137507  0.5702582\n",
            "  -0.93965185]\n",
            " [ 1.7675116   0.55993146 -0.29148516 ...  0.79557186  0.5079256\n",
            "  -1.2890497 ]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[ 0.54665184 -2.2036994  -0.37683746 ... -0.6405449   0.3687821\n",
            "  -1.3951297 ]\n",
            " [-0.47832993  1.635327....81450176  0.04873849\n",
            "   1.1043309 ]\n",
            " [-2.5026178  -0.6078532   0.5073778  ... -0.07948758  0.9346621\n",
            "  -0.32723945]]))\n",
            "        self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d38d3f0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[ 0.54665184 -2.2036994  -0.37683746 ... -0.6405449   0.3687821\n",
            "  -1.3951297 ]\n",
            " [-0.47832993  1.635327....81450176  0.04873849\n",
            "   1.1043309 ]\n",
            " [-2.5026178  -0.6078532   0.5073778  ... -0.07948758  0.9346621\n",
            "  -0.32723945]]))\n",
            "        op         = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d38d3f0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d38dba0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d38dba0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d38d3f0>\n",
            "a = NDArray([[ 0.54665184 -2.2036994  -0.37683746 ... -0.6405449   0.3687821\n",
            "  -1.3951297 ]\n",
            " [-0.47832993  1.6353279  -0.5....5702582\n",
            "  -0.93965185]\n",
            " [ 1.7675116   0.55993146 -0.29148516 ...  0.79557186  0.5079256\n",
            "  -1.2890497 ]], device=cpu())\n",
            "b = NDArray([[-0.84114873 -0.8431679  -0.8893435  ... -0.8391408   1.8222126\n",
            "  -0.9242148 ]\n",
            " [-0.08694614 -0.49521542  3.3...04873849\n",
            "   1.1043309 ]\n",
            " [-2.5026178  -0.6078532   0.5073778  ... -0.07948758  0.9346621\n",
            "  -0.32723945]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[ 0.54665184 -2.2036994  -0.37683746 ... -0.6405449   0.3687821\n",
            "  -1.3951297 ]\n",
            " [-0.47832993  1.6353279  -0.5....5702582\n",
            "  -0.93965185]\n",
            " [ 1.7675116   0.55993146 -0.29148516 ...  0.79557186  0.5079256\n",
            "  -1.2890497 ]], device=cpu())\n",
            "b          = NDArray([[-0.84114873 -0.8431679  -0.8893435  ... -0.8391408   1.8222126\n",
            "  -0.9242148 ]\n",
            " [-0.08694614 -0.49521542  3.3...04873849\n",
            "   1.1043309 ]\n",
            " [-2.5026178  -0.6078532   0.5073778  ... -0.07948758  0.9346621\n",
            "  -0.32723945]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.MatMul object at 0x7f5c9d38d3f0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:228: NotImplementedError\n",
            "\u001b[31m\u001b[1m______________________________________ test_power[cpu-shape0] ______________________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_power\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randint(\u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(_A**_B, (A**_B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-0.83786756]]])\n",
            "_A         = array([[[-0.83786756]]], dtype=float32)\n",
            "_B         = 0\n",
            "device     = cpu()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:102: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:327: in __pow__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.PowerScalar(other)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        other      = 0\n",
            "        self       = needle.Tensor([[[-0.83786756]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.83786756]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.PowerScalar object at 0x7f5c9d52fbb0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-0.83786756]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.PowerScalar object at 0x7f5c9d52fbb0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d52fcd0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d52fcd0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.PowerScalar object at 0x7f5c9d52fbb0>\n",
            "a = NDArray([[[-0.83786756]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a: NDArray) -> NDArray:\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-0.83786756]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.PowerScalar object at 0x7f5c9d52fbb0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:99: NotImplementedError\n",
            "\u001b[31m\u001b[1m______________________________________ test_power[cpu-shape1] ______________________________________\u001b[0m\n",
            "\n",
            "shape = (4, 5, 6), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_power\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        _B = np.random.randint(\u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(_A**_B, (A**_B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-1.76380365e-03  7.76708245e-01 -7.18881249e-01 -4.09029454e-01\n",
            "    1.15395207e-02 -1.38974893e+00]\n",
            " ...45681846e+00]\n",
            "  [-2.82689124e-01  2.85839272e+00 -9.53417838e-01 -8.34345341e-01\n",
            "    3.91825885e-01  6.88781261e-01]]])\n",
            "_A         = array([[[-1.76380365e-03,  7.76708245e-01, -7.18881249e-01,\n",
            "         -4.09029454e-01,  1.15395207e-02, -1.38974893e+00...,  2.85839272e+00, -9.53417838e-01,\n",
            "         -8.34345341e-01,  3.91825885e-01,  6.88781261e-01]]],\n",
            "      dtype=float32)\n",
            "_B         = 0\n",
            "device     = cpu()\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:102: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:327: in __pow__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.PowerScalar(other)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        other      = 0\n",
            "        self       = needle.Tensor([[[-1.76380365e-03  7.76708245e-01 -7.18881249e-01 -4.09029454e-01\n",
            "    1.15395207e-02 -1.38974893e+00]\n",
            " ...45681846e+00]\n",
            "  [-2.82689124e-01  2.85839272e+00 -9.53417838e-01 -8.34345341e-01\n",
            "    3.91825885e-01  6.88781261e-01]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-1.76380365e-03  7.76708245e-01 -7.18881249e-01 -4.09029454e-01\n",
            "    1.15395207e-02 -1.38974893e+00]\n",
            "...681846e+00]\n",
            "  [-2.82689124e-01  2.85839272e+00 -9.53417838e-01 -8.34345341e-01\n",
            "    3.91825885e-01  6.88781261e-01]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.PowerScalar object at 0x7f5c9d2e76d0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-1.76380365e-03  7.76708245e-01 -7.18881249e-01 -4.09029454e-01\n",
            "    1.15395207e-02 -1.38974893e+00]\n",
            "...681846e+00]\n",
            "  [-2.82689124e-01  2.85839272e+00 -9.53417838e-01 -8.34345341e-01\n",
            "    3.91825885e-01  6.88781261e-01]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.PowerScalar object at 0x7f5c9d2e76d0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d2e4a30>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d2e4a30>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.PowerScalar object at 0x7f5c9d2e76d0>\n",
            "a = NDArray([[[-1.76380365e-03  7.76708245e-01 -7.18881249e-01 -4.09029454e-01\n",
            "    1.15395207e-02 -1.38974893e+00]\n",
            "  [-8.4...  [-2.82689124e-01  2.85839272e+00 -9.53417838e-01 -8.34345341e-01\n",
            "    3.91825885e-01  6.88781261e-01]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a: NDArray) -> NDArray:\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-1.76380365e-03  7.76708245e-01 -7.18881249e-01 -4.09029454e-01\n",
            "    1.15395207e-02 -1.38974893e+00]\n",
            "  [-8.4...  [-2.82689124e-01  2.85839272e+00 -9.53417838e-01 -8.34345341e-01\n",
            "    3.91825885e-01  6.88781261e-01]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.PowerScalar object at 0x7f5c9d2e76d0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:99: NotImplementedError\n",
            "\u001b[31m\u001b[1m_______________________________________ test_log[cpu-shape0] _______________________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_log\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32) + \u001b[94m5.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.log(_A), ndl.log(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[6.304988]]])\n",
            "_A         = array([[[6.304988]]], dtype=float32)\n",
            "device     = cpu()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:110: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:270: in log\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Log()(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[6.304988]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[6.304988]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Log object at 0x7f5c9d34d210>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[6.304988]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Log object at 0x7f5c9d34d210>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d34c760>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d34c760>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Log object at 0x7f5c9d34d210>\n",
            "a = NDArray([[[6.304988]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[6.304988]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Log object at 0x7f5c9d34d210>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:260: NotImplementedError\n",
            "\u001b[31m\u001b[1m_______________________________________ test_log[cpu-shape1] _______________________________________\u001b[0m\n",
            "\n",
            "shape = (4, 5, 6), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_log\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32) + \u001b[94m5.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.log(_A), ndl.log(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[4.5901713 3.5619059 6.313068  5.0170035 5.4994755 5.2709227]\n",
            "  [4.2620707 4.73901   4.182539  4.33677...6 3.8747883 6.0094857 3.7410336 4.665304  4.102933 ]\n",
            "  [6.3116837 5.487118  4.074286  3.973268  3.9337008 5.5365143]]])\n",
            "_A         = array([[[4.5901713, 3.5619059, 6.313068 , 5.0170035, 5.4994755,\n",
            "         5.2709227],\n",
            "        [4.2620707, 4.73901  , 4....      4.102933 ],\n",
            "        [6.3116837, 5.487118 , 4.074286 , 3.973268 , 3.9337008,\n",
            "         5.5365143]]], dtype=float32)\n",
            "device     = cpu()\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:110: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:270: in log\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Log()(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[4.5901713 3.5619059 6.313068  5.0170035 5.4994755 5.2709227]\n",
            "  [4.2620707 4.73901   4.182539  4.33677...6 3.8747883 6.0094857 3.7410336 4.665304  4.102933 ]\n",
            "  [6.3116837 5.487118  4.074286  3.973268  3.9337008 5.5365143]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[4.5901713 3.5619059 6.313068  5.0170035 5.4994755 5.2709227]\n",
            "  [4.2620707 4.73901   4.182539  4.3367...3.8747883 6.0094857 3.7410336 4.665304  4.102933 ]\n",
            "  [6.3116837 5.487118  4.074286  3.973268  3.9337008 5.5365143]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Log object at 0x7f5c9d57bc10>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[4.5901713 3.5619059 6.313068  5.0170035 5.4994755 5.2709227]\n",
            "  [4.2620707 4.73901   4.182539  4.3367...3.8747883 6.0094857 3.7410336 4.665304  4.102933 ]\n",
            "  [6.3116837 5.487118  4.074286  3.973268  3.9337008 5.5365143]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Log object at 0x7f5c9d57bc10>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d578be0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d578be0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Log object at 0x7f5c9d57bc10>\n",
            "a = NDArray([[[4.5901713 3.5619059 6.313068  5.0170035 5.4994755 5.2709227]\n",
            "  [4.2620707 4.73901   4.182539  4.336778  4.8...0094857 3.7410336 4.665304  4.102933 ]\n",
            "  [6.3116837 5.487118  4.074286  3.973268  3.9337008 5.5365143]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[4.5901713 3.5619059 6.313068  5.0170035 5.4994755 5.2709227]\n",
            "  [4.2620707 4.73901   4.182539  4.336778  4.8...0094857 3.7410336 4.665304  4.102933 ]\n",
            "  [6.3116837 5.487118  4.074286  3.973268  3.9337008 5.5365143]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Log object at 0x7f5c9d57bc10>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:260: NotImplementedError\n",
            "\u001b[31m\u001b[1m_______________________________________ test_exp[cpu-shape0] _______________________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_exp\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.exp(_A), ndl.exp(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-0.39791787]]])\n",
            "_A         = array([[[-0.39791787]]], dtype=float32)\n",
            "device     = cpu()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:118: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:286: in exp\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Exp()(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[-0.39791787]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.39791787]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Exp object at 0x7f5c9d3f77f0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-0.39791787]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Exp object at 0x7f5c9d3f77f0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3f5de0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3f5de0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Exp object at 0x7f5c9d3f77f0>\n",
            "a = NDArray([[[-0.39791787]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-0.39791787]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Exp object at 0x7f5c9d3f77f0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:276: NotImplementedError\n",
            "\u001b[31m\u001b[1m_______________________________________ test_exp[cpu-shape1] _______________________________________\u001b[0m\n",
            "\n",
            "shape = (4, 5, 6), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_exp\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.exp(_A), ndl.exp(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-1.75505683e-01 -1.27445638e+00  1.06259668e+00  1.36399508e+00\n",
            "   -8.21637392e-01 -4.29588407e-01]\n",
            " ...64467514e-01]\n",
            "  [ 7.74387002e-01 -1.49051147e-03 -7.60637105e-01  1.92789078e+00\n",
            "   -2.05988884e+00 -1.33716178e+00]]])\n",
            "_A         = array([[[-1.75505683e-01, -1.27445638e+00,  1.06259668e+00,\n",
            "          1.36399508e+00, -8.21637392e-01, -4.29588407e-01..., -1.49051147e-03, -7.60637105e-01,\n",
            "          1.92789078e+00, -2.05988884e+00, -1.33716178e+00]]],\n",
            "      dtype=float32)\n",
            "device     = cpu()\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:118: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:286: in exp\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Exp()(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[-1.75505683e-01 -1.27445638e+00  1.06259668e+00  1.36399508e+00\n",
            "   -8.21637392e-01 -4.29588407e-01]\n",
            " ...64467514e-01]\n",
            "  [ 7.74387002e-01 -1.49051147e-03 -7.60637105e-01  1.92789078e+00\n",
            "   -2.05988884e+00 -1.33716178e+00]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-1.75505683e-01 -1.27445638e+00  1.06259668e+00  1.36399508e+00\n",
            "   -8.21637392e-01 -4.29588407e-01]\n",
            "...467514e-01]\n",
            "  [ 7.74387002e-01 -1.49051147e-03 -7.60637105e-01  1.92789078e+00\n",
            "   -2.05988884e+00 -1.33716178e+00]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Exp object at 0x7f5c9d2ec7c0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-1.75505683e-01 -1.27445638e+00  1.06259668e+00  1.36399508e+00\n",
            "   -8.21637392e-01 -4.29588407e-01]\n",
            "...467514e-01]\n",
            "  [ 7.74387002e-01 -1.49051147e-03 -7.60637105e-01  1.92789078e+00\n",
            "   -2.05988884e+00 -1.33716178e+00]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Exp object at 0x7f5c9d2ec7c0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d2edf60>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d2edf60>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Exp object at 0x7f5c9d2ec7c0>\n",
            "a = NDArray([[[-1.75505683e-01 -1.27445638e+00  1.06259668e+00  1.36399508e+00\n",
            "   -8.21637392e-01 -4.29588407e-01]\n",
            "  [-9.4...  [ 7.74387002e-01 -1.49051147e-03 -7.60637105e-01  1.92789078e+00\n",
            "   -2.05988884e+00 -1.33716178e+00]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-1.75505683e-01 -1.27445638e+00  1.06259668e+00  1.36399508e+00\n",
            "   -8.21637392e-01 -4.29588407e-01]\n",
            "  [-9.4...  [ 7.74387002e-01 -1.49051147e-03 -7.60637105e-01  1.92789078e+00\n",
            "   -2.05988884e+00 -1.33716178e+00]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Exp object at 0x7f5c9d2ec7c0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:276: NotImplementedError\n",
            "\u001b[31m\u001b[1m______________________________________ test_relu[cpu-shape0] _______________________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_relu\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.maximum(_A, \u001b[94m0\u001b[39;49;00m), ndl.relu(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[2.4102263]]])\n",
            "_A         = array([[[2.4102263]]], dtype=float32)\n",
            "device     = cpu()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:126: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:302: in relu\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ReLU()(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[2.4102263]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[2.4102263]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.ReLU object at 0x7f5c9d34bbb0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[2.4102263]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.ReLU object at 0x7f5c9d34bbb0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d34bf70>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d34bf70>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.ReLU object at 0x7f5c9d34bbb0>\n",
            "a = NDArray([[[2.4102263]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[2.4102263]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.ReLU object at 0x7f5c9d34bbb0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:292: NotImplementedError\n",
            "\u001b[31m\u001b[1m______________________________________ test_relu[cpu-shape1] _______________________________________\u001b[0m\n",
            "\n",
            "shape = (4, 5, 6), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_relu\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.maximum(_A, \u001b[94m0\u001b[39;49;00m), ndl.relu(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-1.3714054e+00  1.0139933e+00 -1.8735963e+00 -6.7863345e-01\n",
            "    7.8846800e-01 -9.9124104e-01]\n",
            "  [ 1.1...-01 -8.9130884e-01]\n",
            "  [ 9.2772730e-02  1.8728062e+00 -1.0467057e+00 -8.7913162e-01\n",
            "    9.1103613e-01 -6.9426799e-01]]])\n",
            "_A         = array([[[-1.3714054e+00,  1.0139933e+00, -1.8735963e+00, -6.7863345e-01,\n",
            "          7.8846800e-01, -9.9124104e-01],\n",
            "   ...2772730e-02,  1.8728062e+00, -1.0467057e+00, -8.7913162e-01,\n",
            "          9.1103613e-01, -6.9426799e-01]]], dtype=float32)\n",
            "device     = cpu()\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:126: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:302: in relu\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m ReLU()(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[-1.3714054e+00  1.0139933e+00 -1.8735963e+00 -6.7863345e-01\n",
            "    7.8846800e-01 -9.9124104e-01]\n",
            "  [ 1.1...-01 -8.9130884e-01]\n",
            "  [ 9.2772730e-02  1.8728062e+00 -1.0467057e+00 -8.7913162e-01\n",
            "    9.1103613e-01 -6.9426799e-01]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-1.3714054e+00  1.0139933e+00 -1.8735963e+00 -6.7863345e-01\n",
            "    7.8846800e-01 -9.9124104e-01]\n",
            "  [ 1....1 -8.9130884e-01]\n",
            "  [ 9.2772730e-02  1.8728062e+00 -1.0467057e+00 -8.7913162e-01\n",
            "    9.1103613e-01 -6.9426799e-01]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.ReLU object at 0x7f5c9d38dcf0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-1.3714054e+00  1.0139933e+00 -1.8735963e+00 -6.7863345e-01\n",
            "    7.8846800e-01 -9.9124104e-01]\n",
            "  [ 1....1 -8.9130884e-01]\n",
            "  [ 9.2772730e-02  1.8728062e+00 -1.0467057e+00 -8.7913162e-01\n",
            "    9.1103613e-01 -6.9426799e-01]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.ReLU object at 0x7f5c9d38dcf0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d38ffd0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d38ffd0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.ReLU object at 0x7f5c9d38dcf0>\n",
            "a = NDArray([[[-1.3714054e+00  1.0139933e+00 -1.8735963e+00 -6.7863345e-01\n",
            "    7.8846800e-01 -9.9124104e-01]\n",
            "  [ 1.1681533...e-01]\n",
            "  [ 9.2772730e-02  1.8728062e+00 -1.0467057e+00 -8.7913162e-01\n",
            "    9.1103613e-01 -6.9426799e-01]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-1.3714054e+00  1.0139933e+00 -1.8735963e+00 -6.7863345e-01\n",
            "    7.8846800e-01 -9.9124104e-01]\n",
            "  [ 1.1681533...e-01]\n",
            "  [ 9.2772730e-02  1.8728062e+00 -1.0467057e+00 -8.7913162e-01\n",
            "    9.1103613e-01 -6.9426799e-01]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.ReLU object at 0x7f5c9d38dcf0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:292: NotImplementedError\n",
            "\u001b[31m\u001b[1m______________________________________ test_tanh[cpu-shape0] _______________________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[0.9900114]]])\n",
            "_A         = array([[[0.9900114]]], dtype=float32)\n",
            "device     = cpu()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:134: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:318: in tanh\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tanh()(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[0.9900114]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[0.9900114]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Tanh object at 0x7f5c9d3d0a30>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[0.9900114]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Tanh object at 0x7f5c9d3d0a30>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3d0550>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3d0550>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Tanh object at 0x7f5c9d3d0a30>\n",
            "a = NDArray([[[0.9900114]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[0.9900114]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Tanh object at 0x7f5c9d3d0a30>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:308: NotImplementedError\n",
            "\u001b[31m\u001b[1m______________________________________ test_tanh[cpu-shape1] _______________________________________\u001b[0m\n",
            "\n",
            "shape = (4, 5, 6), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[ 0.5179334  -0.37743577 -0.8345885  -0.55899256 -0.16776597\n",
            "    1.2094738 ]\n",
            "  [ 0.12040378  0.5919100....06299214  0.24829769\n",
            "   -0.7538751 ]\n",
            "  [ 1.8209276  -0.09116117  0.49744523  0.45524272 -0.43281022\n",
            "    1.1288149 ]]])\n",
            "_A         = array([[[ 0.5179334 , -0.37743577, -0.8345885 , -0.55899256,\n",
            "         -0.16776597,  1.2094738 ],\n",
            "        [ 0.12040378,...51 ],\n",
            "        [ 1.8209276 , -0.09116117,  0.49744523,  0.45524272,\n",
            "         -0.43281022,  1.1288149 ]]], dtype=float32)\n",
            "device     = cpu()\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:134: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:318: in tanh\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tanh()(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[ 0.5179334  -0.37743577 -0.8345885  -0.55899256 -0.16776597\n",
            "    1.2094738 ]\n",
            "  [ 0.12040378  0.5919100....06299214  0.24829769\n",
            "   -0.7538751 ]\n",
            "  [ 1.8209276  -0.09116117  0.49744523  0.45524272 -0.43281022\n",
            "    1.1288149 ]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[ 0.5179334  -0.37743577 -0.8345885  -0.55899256 -0.16776597\n",
            "    1.2094738 ]\n",
            "  [ 0.12040378  0.591910...6299214  0.24829769\n",
            "   -0.7538751 ]\n",
            "  [ 1.8209276  -0.09116117  0.49744523  0.45524272 -0.43281022\n",
            "    1.1288149 ]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Tanh object at 0x7f5c9d9dd150>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[ 0.5179334  -0.37743577 -0.8345885  -0.55899256 -0.16776597\n",
            "    1.2094738 ]\n",
            "  [ 0.12040378  0.591910...6299214  0.24829769\n",
            "   -0.7538751 ]\n",
            "  [ 1.8209276  -0.09116117  0.49744523  0.45524272 -0.43281022\n",
            "    1.1288149 ]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Tanh object at 0x7f5c9d9dd150>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d9dca90>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d9dca90>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Tanh object at 0x7f5c9d9dd150>\n",
            "a = NDArray([[[ 0.5179334  -0.37743577 -0.8345885  -0.55899256 -0.16776597\n",
            "    1.2094738 ]\n",
            "  [ 0.12040378  0.59191006 -1.7...4829769\n",
            "   -0.7538751 ]\n",
            "  [ 1.8209276  -0.09116117  0.49744523  0.45524272 -0.43281022\n",
            "    1.1288149 ]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[ 0.5179334  -0.37743577 -0.8345885  -0.55899256 -0.16776597\n",
            "    1.2094738 ]\n",
            "  [ 0.12040378  0.59191006 -1.7...4829769\n",
            "   -0.7538751 ]\n",
            "  [ 1.8209276  -0.09116117  0.49744523  0.45524272 -0.43281022\n",
            "    1.1288149 ]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Tanh object at 0x7f5c9d9dd150>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:308: NotImplementedError\n",
            "\u001b[31m\u001b[1m__________________________________ test_tanh_backward[cpu-shape0] __________________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       backward_check(ndl.tanh, A)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-0.6441798]]])\n",
            "_A         = array([[[-0.6441798]]], dtype=float32)\n",
            "device     = cpu()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:142: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:16: in backward_check\n",
            "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.6441798]]]),)\n",
            "        eps        = 1e-05\n",
            "        f          = <function tanh at 0x7f5d31e28d30>\n",
            "        kwargs     = {}\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:318: in tanh\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tanh()(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[-0.6441798]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.6441798]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Tanh object at 0x7f5c9d34e080>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-0.6441798]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Tanh object at 0x7f5c9d34e080>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d34f250>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d34f250>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Tanh object at 0x7f5c9d34e080>\n",
            "a = NDArray([[[-0.6441798]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-0.6441798]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Tanh object at 0x7f5c9d34e080>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:308: NotImplementedError\n",
            "\u001b[31m\u001b[1m__________________________________ test_tanh_backward[cpu-shape1] __________________________________\u001b[0m\n",
            "\n",
            "shape = (4, 5, 6), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       backward_check(ndl.tanh, A)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[ 0.07584199 -0.47216693  0.36699656  0.22710867  1.1375109\n",
            "   -2.06138   ]\n",
            "  [ 0.8301564  -0.30287954...2.192182    2.1712961\n",
            "    1.1894598 ]\n",
            "  [ 0.25481978 -1.6722503  -0.3395371  -0.6779228   0.01023871\n",
            "    0.7156316 ]]])\n",
            "_A         = array([[[ 0.07584199, -0.47216693,  0.36699656,  0.22710867,\n",
            "          1.1375109 , -2.06138   ],\n",
            "        [ 0.8301564 ,...98 ],\n",
            "        [ 0.25481978, -1.6722503 , -0.3395371 , -0.6779228 ,\n",
            "          0.01023871,  0.7156316 ]]], dtype=float32)\n",
            "device     = cpu()\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:142: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:16: in backward_check\n",
            "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[ 0.07584199 -0.47216693  0.36699656  0.22710867  1.1375109\n",
            "   -2.06138   ]\n",
            "  [ 0.8301564  -0.3028795...192182    2.1712961\n",
            "    1.1894598 ]\n",
            "  [ 0.25481978 -1.6722503  -0.3395371  -0.6779228   0.01023871\n",
            "    0.7156316 ]]]),)\n",
            "        eps        = 1e-05\n",
            "        f          = <function tanh at 0x7f5d31e28d30>\n",
            "        kwargs     = {}\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:318: in tanh\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tanh()(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[ 0.07584199 -0.47216693  0.36699656  0.22710867  1.1375109\n",
            "   -2.06138   ]\n",
            "  [ 0.8301564  -0.30287954...2.192182    2.1712961\n",
            "    1.1894598 ]\n",
            "  [ 0.25481978 -1.6722503  -0.3395371  -0.6779228   0.01023871\n",
            "    0.7156316 ]]])\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[ 0.07584199 -0.47216693  0.36699656  0.22710867  1.1375109\n",
            "   -2.06138   ]\n",
            "  [ 0.8301564  -0.3028795...192182    2.1712961\n",
            "    1.1894598 ]\n",
            "  [ 0.25481978 -1.6722503  -0.3395371  -0.6779228   0.01023871\n",
            "    0.7156316 ]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Tanh object at 0x7f5c9d473ac0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[ 0.07584199 -0.47216693  0.36699656  0.22710867  1.1375109\n",
            "   -2.06138   ]\n",
            "  [ 0.8301564  -0.3028795...192182    2.1712961\n",
            "    1.1894598 ]\n",
            "  [ 0.25481978 -1.6722503  -0.3395371  -0.6779228   0.01023871\n",
            "    0.7156316 ]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Tanh object at 0x7f5c9d473ac0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d470370>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d470370>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Tanh object at 0x7f5c9d473ac0>\n",
            "a = NDArray([[[ 0.07584199 -0.47216693  0.36699656  0.22710867  1.1375109\n",
            "   -2.06138   ]\n",
            "  [ 0.8301564  -0.30287954  1.83...1712961\n",
            "    1.1894598 ]\n",
            "  [ 0.25481978 -1.6722503  -0.3395371  -0.6779228   0.01023871\n",
            "    0.7156316 ]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[ 0.07584199 -0.47216693  0.36699656  0.22710867  1.1375109\n",
            "   -2.06138   ]\n",
            "  [ 0.8301564  -0.30287954  1.83...1712961\n",
            "    1.1894598 ]\n",
            "  [ 0.25481978 -1.6722503  -0.3395371  -0.6779228   0.01023871\n",
            "    0.7156316 ]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Tanh object at 0x7f5c9d473ac0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:308: NotImplementedError\n",
            "\u001b[31m\u001b[1m____________________________________ test_stack[cpu-shape0-0-1] ____________________________________\u001b[0m\n",
            "\n",
            "shape = (5, 5), axis = 0, l = 1, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
            "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
            "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
            ">       out = ndl.stack(A, axis=axis)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = [needle.Tensor([[ 1.8746282  -0.5228368  -0.2814408  -0.6886872  -0.48341087]\n",
            " [ 1.8093601   1.6722593   0.80164     1...7503 -1.7781712   2.7362163  -0.89411116  0.1457576 ]\n",
            " [-0.39882693  0.40402585  2.1675591  -1.014474    0.8700887 ]])]\n",
            "A_t        = [tensor([[ 1.8746, -0.5228, -0.2814, -0.6887, -0.4834],\n",
            "        [ 1.8094,  1.6723,  0.8016,  1.5130,  0.5717],\n",
            "       ....1509],\n",
            "        [ 0.3486, -1.7782,  2.7362, -0.8941,  0.1458],\n",
            "        [-0.3988,  0.4040,  2.1676, -1.0145,  0.8701]])]\n",
            "_A         = [array([[ 1.8746282 , -0.5228368 , -0.2814408 , -0.6886872 , -0.48341087],\n",
            "       [ 1.8093601 ,  1.6722593 ,  0.80164 ...9411116,  0.1457576 ],\n",
            "       [-0.39882693,  0.40402585,  2.1675591 , -1.014474  ,  0.8700887 ]],\n",
            "      dtype=float32)]\n",
            "axis       = 0\n",
            "device     = cpu()\n",
            "l          = 1\n",
            "shape      = (5, 5)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:154: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:343: in stack\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\u001b[90m\u001b[39;49;00m\n",
            "        args       = [needle.Tensor([[ 1.8746282  -0.5228368  -0.2814408  -0.6886872  -0.48341087]\n",
            " [ 1.8093601   1.6722593   0.80164     1...7503 -1.7781712   2.7362163  -0.89411116  0.1457576 ]\n",
            " [-0.39882693  0.40402585  2.1675591  -1.014474    0.8700887 ]])]\n",
            "        axis       = 0\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.TensorTuple(needle.Tensor([[ 1.8746282  -0.5228368  -0.2814408  -0.6886872  -0.48341087]\n",
            " [ 1.8093601   1.6722...3 -1.7781712   2.7362163  -0.89411116  0.1457576 ]\n",
            " [-0.39882693  0.40402585  2.1675591  -1.014474    0.8700887 ]]),),)\n",
            "        self       = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d349ba0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.TensorTuple(needle.Tensor([[ 1.8746282  -0.5228368  -0.2814408  -0.6886872  -0.48341087]\n",
            " [ 1.8093601   1.6722...3 -1.7781712   2.7362163  -0.89411116  0.1457576 ]\n",
            " [-0.39882693  0.40402585  2.1675591  -1.014474    0.8700887 ]]),),)\n",
            "        op         = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d349ba0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d34b820>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d34b820>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d349ba0>\n",
            "args = (NDArray([[ 1.8746282  -0.5228368  -0.2814408  -0.6886872  -0.48341087]\n",
            " [ 1.8093601   1.6722593   0.80164     1.51303...   2.7362163  -0.89411116  0.1457576 ]\n",
            " [-0.39882693  0.40402585  2.1675591  -1.014474    0.8700887 ]], device=cpu()),)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "args       = (NDArray([[ 1.8746282  -0.5228368  -0.2814408  -0.6886872  -0.48341087]\n",
            " [ 1.8093601   1.6722593   0.80164     1.51303...   2.7362163  -0.89411116  0.1457576 ]\n",
            " [-0.39882693  0.40402585  2.1675591  -1.014474    0.8700887 ]], device=cpu()),)\n",
            "self       = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d349ba0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:333: NotImplementedError\n",
            "\u001b[31m\u001b[1m____________________________________ test_stack[cpu-shape1-0-2] ____________________________________\u001b[0m\n",
            "\n",
            "shape = (5, 5), axis = 0, l = 2, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
            "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
            "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
            ">       out = ndl.stack(A, axis=axis)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = [needle.Tensor([[-0.8162118  -0.56361467 -0.40814486 -0.17366247  0.91073054]\n",
            " [ 0.73939127 -0.15176739  0.45011562 -0...05   -0.48544037  0.73506886 -1.014833   -0.488132  ]\n",
            " [ 1.6830415   2.4426148  -0.6554058  -1.5415661  -1.0009488 ]])]\n",
            "A_t        = [tensor([[-0.8162, -0.5636, -0.4081, -0.1737,  0.9107],\n",
            "        [ 0.7394, -0.1518,  0.4501, -0.9363,  1.2074],\n",
            "       ....1310],\n",
            "        [ 1.1315, -0.4854,  0.7351, -1.0148, -0.4881],\n",
            "        [ 1.6830,  2.4426, -0.6554, -1.5416, -1.0009]])]\n",
            "_A         = [array([[-0.8162118 , -0.56361467, -0.40814486, -0.17366247,  0.91073054],\n",
            "       [ 0.73939127, -0.15176739,  0.450115...14833  , -0.488132  ],\n",
            "       [ 1.6830415 ,  2.4426148 , -0.6554058 , -1.5415661 , -1.0009488 ]],\n",
            "      dtype=float32)]\n",
            "axis       = 0\n",
            "device     = cpu()\n",
            "l          = 2\n",
            "shape      = (5, 5)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:154: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:343: in stack\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\u001b[90m\u001b[39;49;00m\n",
            "        args       = [needle.Tensor([[-0.8162118  -0.56361467 -0.40814486 -0.17366247  0.91073054]\n",
            " [ 0.73939127 -0.15176739  0.45011562 -0...05   -0.48544037  0.73506886 -1.014833   -0.488132  ]\n",
            " [ 1.6830415   2.4426148  -0.6554058  -1.5415661  -1.0009488 ]])]\n",
            "        axis       = 0\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.TensorTuple(needle.Tensor([[-0.8162118  -0.56361467 -0.40814486 -0.17366247  0.91073054]\n",
            " [ 0.73939127 -0.1517...   -0.48544037  0.73506886 -1.014833   -0.488132  ]\n",
            " [ 1.6830415   2.4426148  -0.6554058  -1.5415661  -1.0009488 ]])),)\n",
            "        self       = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d406500>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.TensorTuple(needle.Tensor([[-0.8162118  -0.56361467 -0.40814486 -0.17366247  0.91073054]\n",
            " [ 0.73939127 -0.1517...   -0.48544037  0.73506886 -1.014833   -0.488132  ]\n",
            " [ 1.6830415   2.4426148  -0.6554058  -1.5415661  -1.0009488 ]])),)\n",
            "        op         = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d406500>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d4041c0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d4041c0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d406500>\n",
            "args = (NDArray([[-0.8162118  -0.56361467 -0.40814486 -0.17366247  0.91073054]\n",
            " [ 0.73939127 -0.15176739  0.45011562 -0.93628...37  0.73506886 -1.014833   -0.488132  ]\n",
            " [ 1.6830415   2.4426148  -0.6554058  -1.5415661  -1.0009488 ]], device=cpu()))\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "args       = (NDArray([[-0.8162118  -0.56361467 -0.40814486 -0.17366247  0.91073054]\n",
            " [ 0.73939127 -0.15176739  0.45011562 -0.93628...37  0.73506886 -1.014833   -0.488132  ]\n",
            " [ 1.6830415   2.4426148  -0.6554058  -1.5415661  -1.0009488 ]], device=cpu()))\n",
            "self       = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d406500>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:333: NotImplementedError\n",
            "\u001b[31m\u001b[1m____________________________________ test_stack[cpu-shape2-2-5] ____________________________________\u001b[0m\n",
            "\n",
            "shape = (1, 5, 7), axis = 2, l = 5, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
            "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
            "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
            ">       out = ndl.stack(A, axis=axis)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = [needle.Tensor([[[-1.3666527  -0.3545086  -0.08867705 -1.0482492   1.8042701\n",
            "    1.1116209   1.5219059 ]\n",
            "  [-0.8927898...-2.307063   -0.19255137]\n",
            "  [-0.71574205  0.5441485   1.5386025  -0.40712336  0.93843824\n",
            "   -0.05133121  0.47227034]]])]\n",
            "A_t        = [tensor([[[-1.3667, -0.3545, -0.0887, -1.0482,  1.8043,  1.1116,  1.5219],\n",
            "         [-0.8928, -0.6316, -0.9389, -0.258...2256,  0.8967, -0.9943, -2.3071, -0.1926],\n",
            "         [-0.7157,  0.5441,  1.5386, -0.4071,  0.9384, -0.0513,  0.4723]]])]\n",
            "_A         = [array([[[-1.3666527 , -0.3545086 , -0.08867705, -1.0482492 ,\n",
            "          1.8042701 ,  1.1116209 ,  1.5219059 ],\n",
            "       ...[-0.71574205,  0.5441485 ,  1.5386025 , -0.40712336,\n",
            "          0.93843824, -0.05133121,  0.47227034]]], dtype=float32)]\n",
            "axis       = 2\n",
            "device     = cpu()\n",
            "l          = 5\n",
            "shape      = (1, 5, 7)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:154: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:343: in stack\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\u001b[90m\u001b[39;49;00m\n",
            "        args       = [needle.Tensor([[[-1.3666527  -0.3545086  -0.08867705 -1.0482492   1.8042701\n",
            "    1.1116209   1.5219059 ]\n",
            "  [-0.8927898...-2.307063   -0.19255137]\n",
            "  [-0.71574205  0.5441485   1.5386025  -0.40712336  0.93843824\n",
            "   -0.05133121  0.47227034]]])]\n",
            "        axis       = 2\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.TensorTuple(needle.Tensor([[[-1.3666527  -0.3545086  -0.08867705 -1.0482492   1.8042701\n",
            "    1.1116209   1.5219....307063   -0.19255137]\n",
            "  [-0.71574205  0.5441485   1.5386025  -0.40712336  0.93843824\n",
            "   -0.05133121  0.47227034]]])),)\n",
            "        self       = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d2e5c00>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.TensorTuple(needle.Tensor([[[-1.3666527  -0.3545086  -0.08867705 -1.0482492   1.8042701\n",
            "    1.1116209   1.5219....307063   -0.19255137]\n",
            "  [-0.71574205  0.5441485   1.5386025  -0.40712336  0.93843824\n",
            "   -0.05133121  0.47227034]]])),)\n",
            "        op         = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d2e5c00>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d2e6620>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d2e6620>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d2e5c00>\n",
            "args = (NDArray([[[-1.3666527  -0.3545086  -0.08867705 -1.0482492   1.8042701\n",
            "    1.1116209   1.5219059 ]\n",
            "  [-0.8927898  -0.6....19255137]\n",
            "  [-0.71574205  0.5441485   1.5386025  -0.40712336  0.93843824\n",
            "   -0.05133121  0.47227034]]], device=cpu()))\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "args       = (NDArray([[[-1.3666527  -0.3545086  -0.08867705 -1.0482492   1.8042701\n",
            "    1.1116209   1.5219059 ]\n",
            "  [-0.8927898  -0.6....19255137]\n",
            "  [-0.71574205  0.5441485   1.5386025  -0.40712336  0.93843824\n",
            "   -0.05133121  0.47227034]]], device=cpu()))\n",
            "self       = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d2e5c00>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:333: NotImplementedError\n",
            "\u001b[31m\u001b[1m_______________________________ test_stack_backward[cpu-shape0-0-1] ________________________________\u001b[0m\n",
            "\n",
            "shape = (5, 5), axis = 0, l = 1, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
            "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
            "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n",
            "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = [needle.Tensor([[ 9.7328430e-01  6.1700344e-01 -5.2133733e-01  1.0966518e-03\n",
            "  -1.0496793e+00]\n",
            " [-1.2557693e-01  1.103...-01  2.3885749e-01\n",
            "  -5.4447418e-01]\n",
            " [ 2.6402649e-01 -3.1051833e-02  1.8831353e-01  6.0250080e-01\n",
            "  -1.6289680e+00]])]\n",
            "A_t        = [tensor([[ 9.7328e-01,  6.1700e-01, -5.2134e-01,  1.0967e-03, -1.0497e+00],\n",
            "        [-1.2558e-01,  1.1034e+00,  9.9986..., -5.4447e-01],\n",
            "        [ 2.6403e-01, -3.1052e-02,  1.8831e-01,  6.0250e-01, -1.6290e+00]],\n",
            "       requires_grad=True)]\n",
            "_A         = [array([[ 9.7328430e-01,  6.1700344e-01, -5.2133733e-01,  1.0966518e-03,\n",
            "        -1.0496793e+00],\n",
            "       [-1.2557693e-...-01],\n",
            "       [ 2.6402649e-01, -3.1051833e-02,  1.8831353e-01,  6.0250080e-01,\n",
            "        -1.6289680e+00]], dtype=float32)]\n",
            "axis       = 0\n",
            "device     = cpu()\n",
            "i          = 0\n",
            "l          = 1\n",
            "shape      = (5, 5)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:343: in stack\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\u001b[90m\u001b[39;49;00m\n",
            "        args       = [needle.Tensor([[ 9.7328430e-01  6.1700344e-01 -5.2133733e-01  1.0966518e-03\n",
            "  -1.0496793e+00]\n",
            " [-1.2557693e-01  1.103...-01  2.3885749e-01\n",
            "  -5.4447418e-01]\n",
            " [ 2.6402649e-01 -3.1051833e-02  1.8831353e-01  6.0250080e-01\n",
            "  -1.6289680e+00]])]\n",
            "        axis       = 0\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.TensorTuple(needle.Tensor([[ 9.7328430e-01  6.1700344e-01 -5.2133733e-01  1.0966518e-03\n",
            "  -1.0496793e+00]\n",
            " [-1...  2.3885749e-01\n",
            "  -5.4447418e-01]\n",
            " [ 2.6402649e-01 -3.1051833e-02  1.8831353e-01  6.0250080e-01\n",
            "  -1.6289680e+00]]),),)\n",
            "        self       = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d3099f0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.TensorTuple(needle.Tensor([[ 9.7328430e-01  6.1700344e-01 -5.2133733e-01  1.0966518e-03\n",
            "  -1.0496793e+00]\n",
            " [-1...  2.3885749e-01\n",
            "  -5.4447418e-01]\n",
            " [ 2.6402649e-01 -3.1051833e-02  1.8831353e-01  6.0250080e-01\n",
            "  -1.6289680e+00]]),),)\n",
            "        op         = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d3099f0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3081f0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3081f0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d3099f0>\n",
            "args = (NDArray([[ 9.7328430e-01  6.1700344e-01 -5.2133733e-01  1.0966518e-03\n",
            "  -1.0496793e+00]\n",
            " [-1.2557693e-01  1.1033653e+...-01\n",
            "  -5.4447418e-01]\n",
            " [ 2.6402649e-01 -3.1051833e-02  1.8831353e-01  6.0250080e-01\n",
            "  -1.6289680e+00]], device=cpu()),)\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "args       = (NDArray([[ 9.7328430e-01  6.1700344e-01 -5.2133733e-01  1.0966518e-03\n",
            "  -1.0496793e+00]\n",
            " [-1.2557693e-01  1.1033653e+...-01\n",
            "  -5.4447418e-01]\n",
            " [ 2.6402649e-01 -3.1051833e-02  1.8831353e-01  6.0250080e-01\n",
            "  -1.6289680e+00]], device=cpu()),)\n",
            "self       = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d3099f0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:333: NotImplementedError\n",
            "\u001b[31m\u001b[1m_______________________________ test_stack_backward[cpu-shape1-0-2] ________________________________\u001b[0m\n",
            "\n",
            "shape = (5, 5), axis = 0, l = 2, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
            "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
            "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n",
            "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = [needle.Tensor([[-0.5199793  -0.4773358  -1.460082    1.0111856   1.0002393 ]\n",
            " [-0.52636147  0.32424414 -0.73866236 -1...943  -0.07404925 -0.72915363  0.3502874  -1.3699573 ]\n",
            " [ 0.04136498  1.7261171   0.3988833  -0.19048904 -1.2501737 ]])]\n",
            "A_t        = [tensor([[-0.5200, -0.4773, -1.4601,  1.0112,  1.0002],\n",
            "        [-0.5264,  0.3242, -0.7387, -1.1135,  1.1475],\n",
            "       ...5125, -0.0740, -0.7292,  0.3503, -1.3700],\n",
            "        [ 0.0414,  1.7261,  0.3989, -0.1905, -1.2502]], requires_grad=True)]\n",
            "_A         = [array([[-0.5199793 , -0.4773358 , -1.460082  ,  1.0111856 ,  1.0002393 ],\n",
            "       [-0.52636147,  0.32424414, -0.738662...502874 , -1.3699573 ],\n",
            "       [ 0.04136498,  1.7261171 ,  0.3988833 , -0.19048904, -1.2501737 ]],\n",
            "      dtype=float32)]\n",
            "axis       = 0\n",
            "device     = cpu()\n",
            "i          = 1\n",
            "l          = 2\n",
            "shape      = (5, 5)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:343: in stack\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\u001b[90m\u001b[39;49;00m\n",
            "        args       = [needle.Tensor([[-0.5199793  -0.4773358  -1.460082    1.0111856   1.0002393 ]\n",
            " [-0.52636147  0.32424414 -0.73866236 -1...943  -0.07404925 -0.72915363  0.3502874  -1.3699573 ]\n",
            " [ 0.04136498  1.7261171   0.3988833  -0.19048904 -1.2501737 ]])]\n",
            "        axis       = 0\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.TensorTuple(needle.Tensor([[-0.5199793  -0.4773358  -1.460082    1.0111856   1.0002393 ]\n",
            " [-0.52636147  0.3242...3  -0.07404925 -0.72915363  0.3502874  -1.3699573 ]\n",
            " [ 0.04136498  1.7261171   0.3988833  -0.19048904 -1.2501737 ]])),)\n",
            "        self       = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d2ed0f0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.TensorTuple(needle.Tensor([[-0.5199793  -0.4773358  -1.460082    1.0111856   1.0002393 ]\n",
            " [-0.52636147  0.3242...3  -0.07404925 -0.72915363  0.3502874  -1.3699573 ]\n",
            " [ 0.04136498  1.7261171   0.3988833  -0.19048904 -1.2501737 ]])),)\n",
            "        op         = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d2ed0f0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d2ec940>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d2ec940>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d2ed0f0>\n",
            "args = (NDArray([[-0.5199793  -0.4773358  -1.460082    1.0111856   1.0002393 ]\n",
            " [-0.52636147  0.32424414 -0.73866236 -1.11350...25 -0.72915363  0.3502874  -1.3699573 ]\n",
            " [ 0.04136498  1.7261171   0.3988833  -0.19048904 -1.2501737 ]], device=cpu()))\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "args       = (NDArray([[-0.5199793  -0.4773358  -1.460082    1.0111856   1.0002393 ]\n",
            " [-0.52636147  0.32424414 -0.73866236 -1.11350...25 -0.72915363  0.3502874  -1.3699573 ]\n",
            " [ 0.04136498  1.7261171   0.3988833  -0.19048904 -1.2501737 ]], device=cpu()))\n",
            "self       = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d2ed0f0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:333: NotImplementedError\n",
            "\u001b[31m\u001b[1m_______________________________ test_stack_backward[cpu-shape2-2-5] ________________________________\u001b[0m\n",
            "\n",
            "shape = (1, 5, 7), axis = 2, l = 5, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
            "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
            "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n",
            "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = [needle.Tensor([[[-1.6303244   1.1500732   0.67453164 -0.6427385  -0.34132683\n",
            "    1.2238566  -1.2085282 ]\n",
            "  [ 0.530796...  2.3903434   0.76327163]\n",
            "  [ 0.7220973  -0.36859453 -0.99120116  0.8497344  -2.4889216\n",
            "   -1.3338861  -0.8416688 ]]])]\n",
            "A_t        = [tensor([[[-1.6303,  1.1501,  0.6745, -0.6427, -0.3413,  1.2239, -1.2085],\n",
            "         [ 0.5308, -1.6544, -1.1750,  0.917...3903,  0.7633],\n",
            "         [ 0.7221, -0.3686, -0.9912,  0.8497, -2.4889, -1.3339, -0.8417]]],\n",
            "       requires_grad=True)]\n",
            "_A         = [array([[[-1.6303244 ,  1.1500732 ,  0.67453164, -0.6427385 ,\n",
            "         -0.34132683,  1.2238566 , -1.2085282 ],\n",
            "       ...[ 0.7220973 , -0.36859453, -0.99120116,  0.8497344 ,\n",
            "         -2.4889216 , -1.3338861 , -0.8416688 ]]], dtype=float32)]\n",
            "axis       = 2\n",
            "device     = cpu()\n",
            "i          = 4\n",
            "l          = 5\n",
            "shape      = (1, 5, 7)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:343: in stack\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\u001b[90m\u001b[39;49;00m\n",
            "        args       = [needle.Tensor([[[-1.6303244   1.1500732   0.67453164 -0.6427385  -0.34132683\n",
            "    1.2238566  -1.2085282 ]\n",
            "  [ 0.530796...  2.3903434   0.76327163]\n",
            "  [ 0.7220973  -0.36859453 -0.99120116  0.8497344  -2.4889216\n",
            "   -1.3338861  -0.8416688 ]]])]\n",
            "        axis       = 2\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.TensorTuple(needle.Tensor([[[-1.6303244   1.1500732   0.67453164 -0.6427385  -0.34132683\n",
            "    1.2238566  -1.208...2.3903434   0.76327163]\n",
            "  [ 0.7220973  -0.36859453 -0.99120116  0.8497344  -2.4889216\n",
            "   -1.3338861  -0.8416688 ]]])),)\n",
            "        self       = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d39fd60>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.TensorTuple(needle.Tensor([[[-1.6303244   1.1500732   0.67453164 -0.6427385  -0.34132683\n",
            "    1.2238566  -1.208...2.3903434   0.76327163]\n",
            "  [ 0.7220973  -0.36859453 -0.99120116  0.8497344  -2.4889216\n",
            "   -1.3338861  -0.8416688 ]]])),)\n",
            "        op         = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d39fd60>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d39df00>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d39df00>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d39fd60>\n",
            "args = (NDArray([[[-1.6303244   1.1500732   0.67453164 -0.6427385  -0.34132683\n",
            "    1.2238566  -1.2085282 ]\n",
            "  [ 0.5307966  -1....0.76327163]\n",
            "  [ 0.7220973  -0.36859453 -0.99120116  0.8497344  -2.4889216\n",
            "   -1.3338861  -0.8416688 ]]], device=cpu()))\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "args       = (NDArray([[[-1.6303244   1.1500732   0.67453164 -0.6427385  -0.34132683\n",
            "    1.2238566  -1.2085282 ]\n",
            "  [ 0.5307966  -1....0.76327163]\n",
            "  [ 0.7220973  -0.36859453 -0.99120116  0.8497344  -2.4889216\n",
            "   -1.3338861  -0.8416688 ]]], device=cpu()))\n",
            "self       = <needle.ops.ops_mathematic.Stack object at 0x7f5c9d39fd60>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:333: NotImplementedError\n",
            "\u001b[31m\u001b[1m_________________________________ test_summation[cpu-shape0-None] __________________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), axes = None, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-0.9358733]]])\n",
            "_A         = array([[[-0.9358733]]], dtype=float32)\n",
            "axes       = None\n",
            "device     = cpu()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:183: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:222: in summation\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[-0.9358733]]])\n",
            "        axes       = None\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.9358733]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d3b0a90>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-0.9358733]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d3b0a90>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3b3820>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3b3820>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d3b0a90>\n",
            "a = NDArray([[[-0.9358733]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-0.9358733]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d3b0a90>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:212: NotImplementedError\n",
            "\u001b[31m\u001b[1m___________________________________ test_summation[cpu-shape1-0] ___________________________________\u001b[0m\n",
            "\n",
            "shape = (5, 3), axes = 0, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[ 0.7283092  -1.1427668   0.06255174]\n",
            " [ 2.2583823   0.61813253 -0.00449909]\n",
            " [-0.3056322  -0.611826    0.9690245 ]\n",
            " [ 1.0893643  -1.0063945  -0.397469  ]\n",
            " [-1.5228411   0.27464268  0.8954157 ]])\n",
            "_A         = array([[ 0.7283092 , -1.1427668 ,  0.06255174],\n",
            "       [ 2.2583823 ,  0.61813253, -0.00449909],\n",
            "       [-0.3056322 , -...0245 ],\n",
            "       [ 1.0893643 , -1.0063945 , -0.397469  ],\n",
            "       [-1.5228411 ,  0.27464268,  0.8954157 ]], dtype=float32)\n",
            "axes       = 0\n",
            "device     = cpu()\n",
            "shape      = (5, 3)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:183: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:222: in summation\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[ 0.7283092  -1.1427668   0.06255174]\n",
            " [ 2.2583823   0.61813253 -0.00449909]\n",
            " [-0.3056322  -0.611826    0.9690245 ]\n",
            " [ 1.0893643  -1.0063945  -0.397469  ]\n",
            " [-1.5228411   0.27464268  0.8954157 ]])\n",
            "        axes       = 0\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[ 0.7283092  -1.1427668   0.06255174]\n",
            " [ 2.2583823   0.61813253 -0.00449909]\n",
            " [-0.3056322  -0.611826    0.9690245 ]\n",
            " [ 1.0893643  -1.0063945  -0.397469  ]\n",
            " [-1.5228411   0.27464268  0.8954157 ]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d3d3370>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[ 0.7283092  -1.1427668   0.06255174]\n",
            " [ 2.2583823   0.61813253 -0.00449909]\n",
            " [-0.3056322  -0.611826    0.9690245 ]\n",
            " [ 1.0893643  -1.0063945  -0.397469  ]\n",
            " [-1.5228411   0.27464268  0.8954157 ]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d3d3370>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3d1510>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3d1510>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d3d3370>\n",
            "a = NDArray([[ 0.7283092  -1.1427668   0.06255174]\n",
            " [ 2.2583823   0.61813253 -0.00449909]\n",
            " [-0.3056322  -0.611826    0.9690245 ]\n",
            " [ 1.0893643  -1.0063945  -0.397469  ]\n",
            " [-1.5228411   0.27464268  0.8954157 ]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[ 0.7283092  -1.1427668   0.06255174]\n",
            " [ 2.2583823   0.61813253 -0.00449909]\n",
            " [-0.3056322  -0.611826    0.9690245 ]\n",
            " [ 1.0893643  -1.0063945  -0.397469  ]\n",
            " [-1.5228411   0.27464268  0.8954157 ]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d3d3370>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:212: NotImplementedError\n",
            "\u001b[31m\u001b[1m___________________________________ test_summation[cpu-shape2-1] ___________________________________\u001b[0m\n",
            "\n",
            "shape = (8, 3, 2), axes = 1, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-1.4774635  -1.6267474 ]\n",
            "  [ 0.49602756  0.31081805]\n",
            "  [ 1.2617579   0.89151007]]\n",
            "\n",
            " [[-0.14909866 -1.... ]\n",
            "  [-0.6780247   1.828674  ]]\n",
            "\n",
            " [[ 0.13491665 -1.538405  ]\n",
            "  [-0.4009321  -2.0557287 ]\n",
            "  [ 0.7399504   1.1037949 ]]])\n",
            "_A         = array([[[-1.4774635 , -1.6267474 ],\n",
            "        [ 0.49602756,  0.31081805],\n",
            "        [ 1.2617579 ,  0.89151007]],\n",
            "\n",
            "       [...  [[ 0.13491665, -1.538405  ],\n",
            "        [-0.4009321 , -2.0557287 ],\n",
            "        [ 0.7399504 ,  1.1037949 ]]], dtype=float32)\n",
            "axes       = 1\n",
            "device     = cpu()\n",
            "shape      = (8, 3, 2)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:183: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:222: in summation\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[-1.4774635  -1.6267474 ]\n",
            "  [ 0.49602756  0.31081805]\n",
            "  [ 1.2617579   0.89151007]]\n",
            "\n",
            " [[-0.14909866 -1.... ]\n",
            "  [-0.6780247   1.828674  ]]\n",
            "\n",
            " [[ 0.13491665 -1.538405  ]\n",
            "  [-0.4009321  -2.0557287 ]\n",
            "  [ 0.7399504   1.1037949 ]]])\n",
            "        axes       = 1\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-1.4774635  -1.6267474 ]\n",
            "  [ 0.49602756  0.31081805]\n",
            "  [ 1.2617579   0.89151007]]\n",
            "\n",
            " [[-0.14909866 -1...\n",
            "  [-0.6780247   1.828674  ]]\n",
            "\n",
            " [[ 0.13491665 -1.538405  ]\n",
            "  [-0.4009321  -2.0557287 ]\n",
            "  [ 0.7399504   1.1037949 ]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d431db0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-1.4774635  -1.6267474 ]\n",
            "  [ 0.49602756  0.31081805]\n",
            "  [ 1.2617579   0.89151007]]\n",
            "\n",
            " [[-0.14909866 -1...\n",
            "  [-0.6780247   1.828674  ]]\n",
            "\n",
            " [[ 0.13491665 -1.538405  ]\n",
            "  [-0.4009321  -2.0557287 ]\n",
            "  [ 0.7399504   1.1037949 ]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d431db0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d4333a0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d4333a0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d431db0>\n",
            "a = NDArray([[[-1.4774635  -1.6267474 ]\n",
            "  [ 0.49602756  0.31081805]\n",
            "  [ 1.2617579   0.89151007]]\n",
            "\n",
            " [[-0.14909866 -1.306816...47   1.828674  ]]\n",
            "\n",
            " [[ 0.13491665 -1.538405  ]\n",
            "  [-0.4009321  -2.0557287 ]\n",
            "  [ 0.7399504   1.1037949 ]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-1.4774635  -1.6267474 ]\n",
            "  [ 0.49602756  0.31081805]\n",
            "  [ 1.2617579   0.89151007]]\n",
            "\n",
            " [[-0.14909866 -1.306816...47   1.828674  ]]\n",
            "\n",
            " [[ 0.13491665 -1.538405  ]\n",
            "  [-0.4009321  -2.0557287 ]\n",
            "  [ 0.7399504   1.1037949 ]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d431db0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:212: NotImplementedError\n",
            "\u001b[31m\u001b[1m___________________________________ test_summation[cpu-shape3-2] ___________________________________\u001b[0m\n",
            "\n",
            "shape = (8, 3, 2), axes = 2, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[ 0.02339398 -0.7209777 ]\n",
            "  [ 0.48379913 -0.5355563 ]\n",
            "  [-1.7946156  -2.3418393 ]]\n",
            "\n",
            " [[-0.06267333 -1....4]\n",
            "  [-1.297391   -0.09585811]]\n",
            "\n",
            " [[ 0.47757328 -1.6015831 ]\n",
            "  [ 0.8859247   0.03872911]\n",
            "  [-2.4073732   0.9825859 ]]])\n",
            "_A         = array([[[ 0.02339398, -0.7209777 ],\n",
            "        [ 0.48379913, -0.5355563 ],\n",
            "        [-1.7946156 , -2.3418393 ]],\n",
            "\n",
            "       [...  [[ 0.47757328, -1.6015831 ],\n",
            "        [ 0.8859247 ,  0.03872911],\n",
            "        [-2.4073732 ,  0.9825859 ]]], dtype=float32)\n",
            "axes       = 2\n",
            "device     = cpu()\n",
            "shape      = (8, 3, 2)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:183: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:222: in summation\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[ 0.02339398 -0.7209777 ]\n",
            "  [ 0.48379913 -0.5355563 ]\n",
            "  [-1.7946156  -2.3418393 ]]\n",
            "\n",
            " [[-0.06267333 -1....4]\n",
            "  [-1.297391   -0.09585811]]\n",
            "\n",
            " [[ 0.47757328 -1.6015831 ]\n",
            "  [ 0.8859247   0.03872911]\n",
            "  [-2.4073732   0.9825859 ]]])\n",
            "        axes       = 2\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[ 0.02339398 -0.7209777 ]\n",
            "  [ 0.48379913 -0.5355563 ]\n",
            "  [-1.7946156  -2.3418393 ]]\n",
            "\n",
            " [[-0.06267333 -1...\n",
            "  [-1.297391   -0.09585811]]\n",
            "\n",
            " [[ 0.47757328 -1.6015831 ]\n",
            "  [ 0.8859247   0.03872911]\n",
            "  [-2.4073732   0.9825859 ]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d2ef7f0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[ 0.02339398 -0.7209777 ]\n",
            "  [ 0.48379913 -0.5355563 ]\n",
            "  [-1.7946156  -2.3418393 ]]\n",
            "\n",
            " [[-0.06267333 -1...\n",
            "  [-1.297391   -0.09585811]]\n",
            "\n",
            " [[ 0.47757328 -1.6015831 ]\n",
            "  [ 0.8859247   0.03872911]\n",
            "  [-2.4073732   0.9825859 ]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d2ef7f0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d2ef460>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d2ef460>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d2ef7f0>\n",
            "a = NDArray([[[ 0.02339398 -0.7209777 ]\n",
            "  [ 0.48379913 -0.5355563 ]\n",
            "  [-1.7946156  -2.3418393 ]]\n",
            "\n",
            " [[-0.06267333 -1.513345...1   -0.09585811]]\n",
            "\n",
            " [[ 0.47757328 -1.6015831 ]\n",
            "  [ 0.8859247   0.03872911]\n",
            "  [-2.4073732   0.9825859 ]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[ 0.02339398 -0.7209777 ]\n",
            "  [ 0.48379913 -0.5355563 ]\n",
            "  [-1.7946156  -2.3418393 ]]\n",
            "\n",
            " [[-0.06267333 -1.513345...1   -0.09585811]]\n",
            "\n",
            " [[ 0.47757328 -1.6015831 ]\n",
            "  [ 0.8859247   0.03872911]\n",
            "  [-2.4073732   0.9825859 ]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d2ef7f0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:212: NotImplementedError\n",
            "\u001b[31m\u001b[1m_____________________________ test_summation_backward[cpu-shape0-None] _____________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), axes = None, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[0.6039443]]])\n",
            "_A         = array([[[0.6039443]]], dtype=float32)\n",
            "axes       = None\n",
            "device     = cpu()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:16: in backward_check\n",
            "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[0.6039443]]]),)\n",
            "        eps        = 1e-05\n",
            "        f          = <function summation at 0x7f5d31e28280>\n",
            "        kwargs     = {'axes': None}\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:222: in summation\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[0.6039443]]])\n",
            "        axes       = None\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[0.6039443]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d34ca30>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[0.6039443]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d34ca30>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d34dab0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d34dab0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d34ca30>\n",
            "a = NDArray([[[0.6039443]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[0.6039443]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d34ca30>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:212: NotImplementedError\n",
            "\u001b[31m\u001b[1m______________________________ test_summation_backward[cpu-shape1-0] _______________________________\u001b[0m\n",
            "\n",
            "shape = (5, 3), axes = 0, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[-1.0119154   0.44852072 -0.7156536 ]\n",
            " [-0.28398412  1.3041083   1.387943  ]\n",
            " [ 1.4733936  -0.68743455 -1.4527953 ]\n",
            " [-0.6309952   0.83004636  0.37497717]\n",
            " [ 0.26435927 -0.4604916  -0.666616  ]])\n",
            "_A         = array([[-1.0119154 ,  0.44852072, -0.7156536 ],\n",
            "       [-0.28398412,  1.3041083 ,  1.387943  ],\n",
            "       [ 1.4733936 , -...7953 ],\n",
            "       [-0.6309952 ,  0.83004636,  0.37497717],\n",
            "       [ 0.26435927, -0.4604916 , -0.666616  ]], dtype=float32)\n",
            "axes       = 0\n",
            "device     = cpu()\n",
            "shape      = (5, 3)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:16: in backward_check\n",
            "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-1.0119154   0.44852072 -0.7156536 ]\n",
            " [-0.28398412  1.3041083   1.387943  ]\n",
            " [ 1.4733936  -0.68743455 -1.4527953 ]\n",
            " [-0.6309952   0.83004636  0.37497717]\n",
            " [ 0.26435927 -0.4604916  -0.666616  ]]),)\n",
            "        eps        = 1e-05\n",
            "        f          = <function summation at 0x7f5d31e28280>\n",
            "        kwargs     = {'axes': 0}\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:222: in summation\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[-1.0119154   0.44852072 -0.7156536 ]\n",
            " [-0.28398412  1.3041083   1.387943  ]\n",
            " [ 1.4733936  -0.68743455 -1.4527953 ]\n",
            " [-0.6309952   0.83004636  0.37497717]\n",
            " [ 0.26435927 -0.4604916  -0.666616  ]])\n",
            "        axes       = 0\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[-1.0119154   0.44852072 -0.7156536 ]\n",
            " [-0.28398412  1.3041083   1.387943  ]\n",
            " [ 1.4733936  -0.68743455 -1.4527953 ]\n",
            " [-0.6309952   0.83004636  0.37497717]\n",
            " [ 0.26435927 -0.4604916  -0.666616  ]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f5c9b185810>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[-1.0119154   0.44852072 -0.7156536 ]\n",
            " [-0.28398412  1.3041083   1.387943  ]\n",
            " [ 1.4733936  -0.68743455 -1.4527953 ]\n",
            " [-0.6309952   0.83004636  0.37497717]\n",
            " [ 0.26435927 -0.4604916  -0.666616  ]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Summation object at 0x7f5c9b185810>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9b187730>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9b187730>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Summation object at 0x7f5c9b185810>\n",
            "a = NDArray([[-1.0119154   0.44852072 -0.7156536 ]\n",
            " [-0.28398412  1.3041083   1.387943  ]\n",
            " [ 1.4733936  -0.68743455 -1.4527953 ]\n",
            " [-0.6309952   0.83004636  0.37497717]\n",
            " [ 0.26435927 -0.4604916  -0.666616  ]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[-1.0119154   0.44852072 -0.7156536 ]\n",
            " [-0.28398412  1.3041083   1.387943  ]\n",
            " [ 1.4733936  -0.68743455 -1.4527953 ]\n",
            " [-0.6309952   0.83004636  0.37497717]\n",
            " [ 0.26435927 -0.4604916  -0.666616  ]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Summation object at 0x7f5c9b185810>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:212: NotImplementedError\n",
            "\u001b[31m\u001b[1m______________________________ test_summation_backward[cpu-shape2-1] _______________________________\u001b[0m\n",
            "\n",
            "shape = (8, 3, 2), axes = 1, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-1.5339047  -0.37733373]\n",
            "  [-0.6903583   0.35359657]\n",
            "  [ 0.5598595   0.7105761 ]]\n",
            "\n",
            " [[-0.8595503   0.... ]\n",
            "  [ 0.24980997  0.6041445 ]]\n",
            "\n",
            " [[ 1.287218   -0.06960689]\n",
            "  [ 0.6557981   0.5887635 ]\n",
            "  [-0.21547143 -1.6977823 ]]])\n",
            "_A         = array([[[-1.5339047 , -0.37733373],\n",
            "        [-0.6903583 ,  0.35359657],\n",
            "        [ 0.5598595 ,  0.7105761 ]],\n",
            "\n",
            "       [...  [[ 1.287218  , -0.06960689],\n",
            "        [ 0.6557981 ,  0.5887635 ],\n",
            "        [-0.21547143, -1.6977823 ]]], dtype=float32)\n",
            "axes       = 1\n",
            "device     = cpu()\n",
            "shape      = (8, 3, 2)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:16: in backward_check\n",
            "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-1.5339047  -0.37733373]\n",
            "  [-0.6903583   0.35359657]\n",
            "  [ 0.5598595   0.7105761 ]]\n",
            "\n",
            " [[-0.8595503   0...\n",
            "  [ 0.24980997  0.6041445 ]]\n",
            "\n",
            " [[ 1.287218   -0.06960689]\n",
            "  [ 0.6557981   0.5887635 ]\n",
            "  [-0.21547143 -1.6977823 ]]]),)\n",
            "        eps        = 1e-05\n",
            "        f          = <function summation at 0x7f5d31e28280>\n",
            "        kwargs     = {'axes': 1}\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:222: in summation\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[-1.5339047  -0.37733373]\n",
            "  [-0.6903583   0.35359657]\n",
            "  [ 0.5598595   0.7105761 ]]\n",
            "\n",
            " [[-0.8595503   0.... ]\n",
            "  [ 0.24980997  0.6041445 ]]\n",
            "\n",
            " [[ 1.287218   -0.06960689]\n",
            "  [ 0.6557981   0.5887635 ]\n",
            "  [-0.21547143 -1.6977823 ]]])\n",
            "        axes       = 1\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-1.5339047  -0.37733373]\n",
            "  [-0.6903583   0.35359657]\n",
            "  [ 0.5598595   0.7105761 ]]\n",
            "\n",
            " [[-0.8595503   0...\n",
            "  [ 0.24980997  0.6041445 ]]\n",
            "\n",
            " [[ 1.287218   -0.06960689]\n",
            "  [ 0.6557981   0.5887635 ]\n",
            "  [-0.21547143 -1.6977823 ]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d39d060>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-1.5339047  -0.37733373]\n",
            "  [-0.6903583   0.35359657]\n",
            "  [ 0.5598595   0.7105761 ]]\n",
            "\n",
            " [[-0.8595503   0...\n",
            "  [ 0.24980997  0.6041445 ]]\n",
            "\n",
            " [[ 1.287218   -0.06960689]\n",
            "  [ 0.6557981   0.5887635 ]\n",
            "  [-0.21547143 -1.6977823 ]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d39d060>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d39fa90>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d39fa90>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d39d060>\n",
            "a = NDArray([[[-1.5339047  -0.37733373]\n",
            "  [-0.6903583   0.35359657]\n",
            "  [ 0.5598595   0.7105761 ]]\n",
            "\n",
            " [[-0.8595503   0.126829...997  0.6041445 ]]\n",
            "\n",
            " [[ 1.287218   -0.06960689]\n",
            "  [ 0.6557981   0.5887635 ]\n",
            "  [-0.21547143 -1.6977823 ]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-1.5339047  -0.37733373]\n",
            "  [-0.6903583   0.35359657]\n",
            "  [ 0.5598595   0.7105761 ]]\n",
            "\n",
            " [[-0.8595503   0.126829...997  0.6041445 ]]\n",
            "\n",
            " [[ 1.287218   -0.06960689]\n",
            "  [ 0.6557981   0.5887635 ]\n",
            "  [-0.21547143 -1.6977823 ]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d39d060>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:212: NotImplementedError\n",
            "\u001b[31m\u001b[1m______________________________ test_summation_backward[cpu-shape3-2] _______________________________\u001b[0m\n",
            "\n",
            "shape = (8, 3, 2), axes = 2, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[ 0.49925917 -1.1749268 ]\n",
            "  [-1.5551547   0.62649083]\n",
            "  [ 0.12255095 -1.0435718 ]]\n",
            "\n",
            " [[ 0.24988598  0....5]\n",
            "  [-0.59295154 -0.868786  ]]\n",
            "\n",
            " [[ 1.2810982  -0.21315454]\n",
            "  [ 1.3723994  -0.36860856]\n",
            "  [-0.06208068  0.1608922 ]]])\n",
            "_A         = array([[[ 0.49925917, -1.1749268 ],\n",
            "        [-1.5551547 ,  0.62649083],\n",
            "        [ 0.12255095, -1.0435718 ]],\n",
            "\n",
            "       [...  [[ 1.2810982 , -0.21315454],\n",
            "        [ 1.3723994 , -0.36860856],\n",
            "        [-0.06208068,  0.1608922 ]]], dtype=float32)\n",
            "axes       = 2\n",
            "device     = cpu()\n",
            "shape      = (8, 3, 2)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:16: in backward_check\n",
            "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[ 0.49925917 -1.1749268 ]\n",
            "  [-1.5551547   0.62649083]\n",
            "  [ 0.12255095 -1.0435718 ]]\n",
            "\n",
            " [[ 0.24988598  0...\n",
            "  [-0.59295154 -0.868786  ]]\n",
            "\n",
            " [[ 1.2810982  -0.21315454]\n",
            "  [ 1.3723994  -0.36860856]\n",
            "  [-0.06208068  0.1608922 ]]]),)\n",
            "        eps        = 1e-05\n",
            "        f          = <function summation at 0x7f5d31e28280>\n",
            "        kwargs     = {'axes': 2}\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:222: in summation\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[ 0.49925917 -1.1749268 ]\n",
            "  [-1.5551547   0.62649083]\n",
            "  [ 0.12255095 -1.0435718 ]]\n",
            "\n",
            " [[ 0.24988598  0....5]\n",
            "  [-0.59295154 -0.868786  ]]\n",
            "\n",
            " [[ 1.2810982  -0.21315454]\n",
            "  [ 1.3723994  -0.36860856]\n",
            "  [-0.06208068  0.1608922 ]]])\n",
            "        axes       = 2\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[ 0.49925917 -1.1749268 ]\n",
            "  [-1.5551547   0.62649083]\n",
            "  [ 0.12255095 -1.0435718 ]]\n",
            "\n",
            " [[ 0.24988598  0...\n",
            "  [-0.59295154 -0.868786  ]]\n",
            "\n",
            " [[ 1.2810982  -0.21315454]\n",
            "  [ 1.3723994  -0.36860856]\n",
            "  [-0.06208068  0.1608922 ]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d3159f0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[ 0.49925917 -1.1749268 ]\n",
            "  [-1.5551547   0.62649083]\n",
            "  [ 0.12255095 -1.0435718 ]]\n",
            "\n",
            " [[ 0.24988598  0...\n",
            "  [-0.59295154 -0.868786  ]]\n",
            "\n",
            " [[ 1.2810982  -0.21315454]\n",
            "  [ 1.3723994  -0.36860856]\n",
            "  [-0.06208068  0.1608922 ]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d3159f0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3145b0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3145b0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d3159f0>\n",
            "a = NDArray([[[ 0.49925917 -1.1749268 ]\n",
            "  [-1.5551547   0.62649083]\n",
            "  [ 0.12255095 -1.0435718 ]]\n",
            "\n",
            " [[ 0.24988598  0.323462...154 -0.868786  ]]\n",
            "\n",
            " [[ 1.2810982  -0.21315454]\n",
            "  [ 1.3723994  -0.36860856]\n",
            "  [-0.06208068  0.1608922 ]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[ 0.49925917 -1.1749268 ]\n",
            "  [-1.5551547   0.62649083]\n",
            "  [ 0.12255095 -1.0435718 ]]\n",
            "\n",
            " [[ 0.24988598  0.323462...154 -0.868786  ]]\n",
            "\n",
            " [[ 1.2810982  -0.21315454]\n",
            "  [ 1.3723994  -0.36860856]\n",
            "  [-0.06208068  0.1608922 ]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Summation object at 0x7f5c9d3159f0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:212: NotImplementedError\n",
            "\u001b[31m\u001b[1m_____________________________ test_broadcast_to[cpu-shape0-shape_to0] ______________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), shape_to = (3, 3, 3), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape,shape_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BROADCAST_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_broadcast_to\u001b[39;49;00m(shape, shape_to, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.broadcast_to(_A, shape_to), ndl.broadcast_to(A, shape_to).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[2.0724792]]])\n",
            "_A         = array([[[2.0724792]]], dtype=float32)\n",
            "device     = cpu()\n",
            "shape      = (1, 1, 1)\n",
            "shape_to   = (3, 3, 3)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:201: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:203: in broadcast_to\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m BroadcastTo(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[2.0724792]]])\n",
            "        shape      = (3, 3, 3)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[2.0724792]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.BroadcastTo object at 0x7f5c9d3075e0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[2.0724792]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.BroadcastTo object at 0x7f5c9d3075e0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d306ad0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d306ad0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.BroadcastTo object at 0x7f5c9d3075e0>\n",
            "a = NDArray([[[2.0724792]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[2.0724792]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.BroadcastTo object at 0x7f5c9d3075e0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:193: NotImplementedError\n",
            "\u001b[31m\u001b[1m_____________________________ test_broadcast_to[cpu-shape1-shape_to1] ______________________________\u001b[0m\n",
            "\n",
            "shape = (4, 1, 6), shape_to = (4, 3, 6), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape,shape_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BROADCAST_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_broadcast_to\u001b[39;49;00m(shape, shape_to, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.broadcast_to(_A, shape_to), ndl.broadcast_to(A, shape_to).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-0.04451764  1.1893455   3.016384    2.086504   -0.43867043\n",
            "   -0.2248867 ]]\n",
            "\n",
            " [[ 0.47733635 -0.24154....39262605 -1.3748187\n",
            "   -1.0438015 ]]\n",
            "\n",
            " [[-1.4479293   0.16030224  0.42136237 -0.27678046 -0.6100461\n",
            "   -0.8055568 ]]])\n",
            "_A         = array([[[-0.04451764,  1.1893455 ,  3.016384  ,  2.086504  ,\n",
            "         -0.43867043, -0.2248867 ]],\n",
            "\n",
            "       [[ 0.4773363... ]],\n",
            "\n",
            "       [[-1.4479293 ,  0.16030224,  0.42136237, -0.27678046,\n",
            "         -0.6100461 , -0.8055568 ]]], dtype=float32)\n",
            "device     = cpu()\n",
            "shape      = (4, 1, 6)\n",
            "shape_to   = (4, 3, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:201: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:203: in broadcast_to\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m BroadcastTo(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[-0.04451764  1.1893455   3.016384    2.086504   -0.43867043\n",
            "   -0.2248867 ]]\n",
            "\n",
            " [[ 0.47733635 -0.24154....39262605 -1.3748187\n",
            "   -1.0438015 ]]\n",
            "\n",
            " [[-1.4479293   0.16030224  0.42136237 -0.27678046 -0.6100461\n",
            "   -0.8055568 ]]])\n",
            "        shape      = (4, 3, 6)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.04451764  1.1893455   3.016384    2.086504   -0.43867043\n",
            "   -0.2248867 ]]\n",
            "\n",
            " [[ 0.47733635 -0.2415...9262605 -1.3748187\n",
            "   -1.0438015 ]]\n",
            "\n",
            " [[-1.4479293   0.16030224  0.42136237 -0.27678046 -0.6100461\n",
            "   -0.8055568 ]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.BroadcastTo object at 0x7f5c9d2e5cc0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-0.04451764  1.1893455   3.016384    2.086504   -0.43867043\n",
            "   -0.2248867 ]]\n",
            "\n",
            " [[ 0.47733635 -0.2415...9262605 -1.3748187\n",
            "   -1.0438015 ]]\n",
            "\n",
            " [[-1.4479293   0.16030224  0.42136237 -0.27678046 -0.6100461\n",
            "   -0.8055568 ]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.BroadcastTo object at 0x7f5c9d2e5cc0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d2e7a90>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d2e7a90>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.BroadcastTo object at 0x7f5c9d2e5cc0>\n",
            "a = NDArray([[[-0.04451764  1.1893455   3.016384    2.086504   -0.43867043\n",
            "   -0.2248867 ]]\n",
            "\n",
            " [[ 0.47733635 -0.24154349  0...748187\n",
            "   -1.0438015 ]]\n",
            "\n",
            " [[-1.4479293   0.16030224  0.42136237 -0.27678046 -0.6100461\n",
            "   -0.8055568 ]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-0.04451764  1.1893455   3.016384    2.086504   -0.43867043\n",
            "   -0.2248867 ]]\n",
            "\n",
            " [[ 0.47733635 -0.24154349  0...748187\n",
            "   -1.0438015 ]]\n",
            "\n",
            " [[-1.4479293   0.16030224  0.42136237 -0.27678046 -0.6100461\n",
            "   -0.8055568 ]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.BroadcastTo object at 0x7f5c9d2e5cc0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:193: NotImplementedError\n",
            "\u001b[31m\u001b[1m________________________________ test_reshape[cpu-shape0-shape_to0] ________________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), shape_to = (1,), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape,shape_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, RESHAPE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_reshape\u001b[39;49;00m(shape, shape_to, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.reshape(_A, shape_to), ndl.reshape(A, shape_to).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-0.78740907]]])\n",
            "_A         = array([[[-0.78740907]]], dtype=float32)\n",
            "device     = cpu()\n",
            "shape      = (1, 1, 1)\n",
            "shape_to   = (1,)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:211: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:184: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[-0.78740907]]])\n",
            "        shape      = (1,)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.78740907]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7f5c9d578220>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-0.78740907]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7f5c9d578220>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d578250>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d578250>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Reshape object at 0x7f5c9d578220>\n",
            "a = NDArray([[[-0.78740907]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-0.78740907]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Reshape object at 0x7f5c9d578220>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:174: NotImplementedError\n",
            "\u001b[31m\u001b[1m________________________________ test_reshape[cpu-shape1-shape_to1] ________________________________\u001b[0m\n",
            "\n",
            "shape = (4, 1, 6), shape_to = (6, 4, 1), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape,shape_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, RESHAPE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_reshape\u001b[39;49;00m(shape, shape_to, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.reshape(_A, shape_to), ndl.reshape(A, shape_to).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-0.75153327 -1.2383314   0.0258553  -0.02689702 -0.19400224\n",
            "    2.0804594 ]]\n",
            "\n",
            " [[-2.0084236  -1.65670...6939662 -0.19728902\n",
            "    0.29099283]]\n",
            "\n",
            " [[ 0.30973268  2.0619261   0.12308069  0.7655687  -0.46014473\n",
            "    2.055719  ]]])\n",
            "_A         = array([[[-0.75153327, -1.2383314 ,  0.0258553 , -0.02689702,\n",
            "         -0.19400224,  2.0804594 ]],\n",
            "\n",
            "       [[-2.0084236...3]],\n",
            "\n",
            "       [[ 0.30973268,  2.0619261 ,  0.12308069,  0.7655687 ,\n",
            "         -0.46014473,  2.055719  ]]], dtype=float32)\n",
            "device     = cpu()\n",
            "shape      = (4, 1, 6)\n",
            "shape_to   = (6, 4, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:211: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:184: in reshape\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Reshape(shape)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[-0.75153327 -1.2383314   0.0258553  -0.02689702 -0.19400224\n",
            "    2.0804594 ]]\n",
            "\n",
            " [[-2.0084236  -1.65670...6939662 -0.19728902\n",
            "    0.29099283]]\n",
            "\n",
            " [[ 0.30973268  2.0619261   0.12308069  0.7655687  -0.46014473\n",
            "    2.055719  ]]])\n",
            "        shape      = (6, 4, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.75153327 -1.2383314   0.0258553  -0.02689702 -0.19400224\n",
            "    2.0804594 ]]\n",
            "\n",
            " [[-2.0084236  -1.6567...39662 -0.19728902\n",
            "    0.29099283]]\n",
            "\n",
            " [[ 0.30973268  2.0619261   0.12308069  0.7655687  -0.46014473\n",
            "    2.055719  ]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Reshape object at 0x7f5c9d30a9b0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-0.75153327 -1.2383314   0.0258553  -0.02689702 -0.19400224\n",
            "    2.0804594 ]]\n",
            "\n",
            " [[-2.0084236  -1.6567...39662 -0.19728902\n",
            "    0.29099283]]\n",
            "\n",
            " [[ 0.30973268  2.0619261   0.12308069  0.7655687  -0.46014473\n",
            "    2.055719  ]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Reshape object at 0x7f5c9d30a9b0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d308ac0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d308ac0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Reshape object at 0x7f5c9d30a9b0>\n",
            "a = NDArray([[[-0.75153327 -1.2383314   0.0258553  -0.02689702 -0.19400224\n",
            "    2.0804594 ]]\n",
            "\n",
            " [[-2.0084236  -1.6567076   0...28902\n",
            "    0.29099283]]\n",
            "\n",
            " [[ 0.30973268  2.0619261   0.12308069  0.7655687  -0.46014473\n",
            "    2.055719  ]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-0.75153327 -1.2383314   0.0258553  -0.02689702 -0.19400224\n",
            "    2.0804594 ]]\n",
            "\n",
            " [[-2.0084236  -1.6567076   0...28902\n",
            "    0.29099283]]\n",
            "\n",
            " [[ 0.30973268  2.0619261   0.12308069  0.7655687  -0.46014473\n",
            "    2.055719  ]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Reshape object at 0x7f5c9d30a9b0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:174: NotImplementedError\n",
            "\u001b[31m\u001b[1m_________________________________ test_transpose[cpu-axes0-shape0] _________________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), axes = (0, 1), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-1.8413925]]])\n",
            "_A         = array([[[-1.8413925]]], dtype=float32)\n",
            "axes       = (0, 1)\n",
            "device     = cpu()\n",
            "np_axes    = (0, 1)\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:165: in transpose\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[-1.8413925]]])\n",
            "        axes       = (0, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-1.8413925]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d432020>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-1.8413925]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d432020>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d432fb0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d432fb0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d432020>\n",
            "a = NDArray([[[-1.8413925]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-1.8413925]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d432020>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:155: NotImplementedError\n",
            "\u001b[31m\u001b[1m_________________________________ test_transpose[cpu-axes0-shape1] _________________________________\u001b[0m\n",
            "\n",
            "shape = (4, 5, 6), axes = (0, 1), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-5.1026684e-01  1.0826996e-01  4.1043192e-01  1.6332511e+00\n",
            "    3.0424109e-01 -1.1153238e-01]\n",
            "  [ 1.7...-01  1.2805634e+00]\n",
            "  [ 2.7512822e-01 -2.1211612e+00 -4.0476760e-01  3.6475179e-01\n",
            "    8.7538981e-01  1.5410359e+00]]])\n",
            "_A         = array([[[-5.1026684e-01,  1.0826996e-01,  4.1043192e-01,  1.6332511e+00,\n",
            "          3.0424109e-01, -1.1153238e-01],\n",
            "   ...7512822e-01, -2.1211612e+00, -4.0476760e-01,  3.6475179e-01,\n",
            "          8.7538981e-01,  1.5410359e+00]]], dtype=float32)\n",
            "axes       = (0, 1)\n",
            "device     = cpu()\n",
            "np_axes    = (0, 1)\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:165: in transpose\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[-5.1026684e-01  1.0826996e-01  4.1043192e-01  1.6332511e+00\n",
            "    3.0424109e-01 -1.1153238e-01]\n",
            "  [ 1.7...-01  1.2805634e+00]\n",
            "  [ 2.7512822e-01 -2.1211612e+00 -4.0476760e-01  3.6475179e-01\n",
            "    8.7538981e-01  1.5410359e+00]]])\n",
            "        axes       = (0, 1)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-5.1026684e-01  1.0826996e-01  4.1043192e-01  1.6332511e+00\n",
            "    3.0424109e-01 -1.1153238e-01]\n",
            "  [ 1....1  1.2805634e+00]\n",
            "  [ 2.7512822e-01 -2.1211612e+00 -4.0476760e-01  3.6475179e-01\n",
            "    8.7538981e-01  1.5410359e+00]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d9dff10>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-5.1026684e-01  1.0826996e-01  4.1043192e-01  1.6332511e+00\n",
            "    3.0424109e-01 -1.1153238e-01]\n",
            "  [ 1....1  1.2805634e+00]\n",
            "  [ 2.7512822e-01 -2.1211612e+00 -4.0476760e-01  3.6475179e-01\n",
            "    8.7538981e-01  1.5410359e+00]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d9dff10>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d9df520>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d9df520>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d9dff10>\n",
            "a = NDArray([[[-5.1026684e-01  1.0826996e-01  4.1043192e-01  1.6332511e+00\n",
            "    3.0424109e-01 -1.1153238e-01]\n",
            "  [ 1.7358176...e+00]\n",
            "  [ 2.7512822e-01 -2.1211612e+00 -4.0476760e-01  3.6475179e-01\n",
            "    8.7538981e-01  1.5410359e+00]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-5.1026684e-01  1.0826996e-01  4.1043192e-01  1.6332511e+00\n",
            "    3.0424109e-01 -1.1153238e-01]\n",
            "  [ 1.7358176...e+00]\n",
            "  [ 2.7512822e-01 -2.1211612e+00 -4.0476760e-01  3.6475179e-01\n",
            "    8.7538981e-01  1.5410359e+00]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d9dff10>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:155: NotImplementedError\n",
            "\u001b[31m\u001b[1m_________________________________ test_transpose[cpu-axes1-shape0] _________________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), axes = (0, 2), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[0.0741415]]])\n",
            "_A         = array([[[0.0741415]]], dtype=float32)\n",
            "axes       = (0, 2)\n",
            "device     = cpu()\n",
            "np_axes    = (0, 2)\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:165: in transpose\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[0.0741415]]])\n",
            "        axes       = (0, 2)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[0.0741415]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9b184820>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[0.0741415]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9b184820>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9b186aa0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9b186aa0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9b184820>\n",
            "a = NDArray([[[0.0741415]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[0.0741415]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9b184820>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:155: NotImplementedError\n",
            "\u001b[31m\u001b[1m_________________________________ test_transpose[cpu-axes1-shape1] _________________________________\u001b[0m\n",
            "\n",
            "shape = (4, 5, 6), axes = (0, 2), device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-0.61900485  0.5339638  -0.7161466  -0.8014063  -0.3596195\n",
            "    0.92178184]\n",
            "  [ 1.9062705   0.8322557 ...0.39741617 -1.5956964\n",
            "   -0.53968245]\n",
            "  [ 0.52046865  0.80522346 -0.05476308 -0.2654866   0.02600498\n",
            "   -2.0909956 ]]])\n",
            "_A         = array([[[-0.61900485,  0.5339638 , -0.7161466 , -0.8014063 ,\n",
            "         -0.3596195 ,  0.92178184],\n",
            "        [ 1.9062705 ,...245],\n",
            "        [ 0.52046865,  0.80522346, -0.05476308, -0.2654866 ,\n",
            "          0.02600498, -2.0909956 ]]], dtype=float32)\n",
            "axes       = (0, 2)\n",
            "device     = cpu()\n",
            "np_axes    = (0, 2)\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:165: in transpose\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[-0.61900485  0.5339638  -0.7161466  -0.8014063  -0.3596195\n",
            "    0.92178184]\n",
            "  [ 1.9062705   0.8322557 ...0.39741617 -1.5956964\n",
            "   -0.53968245]\n",
            "  [ 0.52046865  0.80522346 -0.05476308 -0.2654866   0.02600498\n",
            "   -2.0909956 ]]])\n",
            "        axes       = (0, 2)\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-0.61900485  0.5339638  -0.7161466  -0.8014063  -0.3596195\n",
            "    0.92178184]\n",
            "  [ 1.9062705   0.8322557...39741617 -1.5956964\n",
            "   -0.53968245]\n",
            "  [ 0.52046865  0.80522346 -0.05476308 -0.2654866   0.02600498\n",
            "   -2.0909956 ]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d3e1b10>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-0.61900485  0.5339638  -0.7161466  -0.8014063  -0.3596195\n",
            "    0.92178184]\n",
            "  [ 1.9062705   0.8322557...39741617 -1.5956964\n",
            "   -0.53968245]\n",
            "  [ 0.52046865  0.80522346 -0.05476308 -0.2654866   0.02600498\n",
            "   -2.0909956 ]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d3e1b10>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3e0130>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3e0130>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d3e1b10>\n",
            "a = NDArray([[[-0.61900485  0.5339638  -0.7161466  -0.8014063  -0.3596195\n",
            "    0.92178184]\n",
            "  [ 1.9062705   0.8322557  -0.89...5956964\n",
            "   -0.53968245]\n",
            "  [ 0.52046865  0.80522346 -0.05476308 -0.2654866   0.02600498\n",
            "   -2.0909956 ]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-0.61900485  0.5339638  -0.7161466  -0.8014063  -0.3596195\n",
            "    0.92178184]\n",
            "  [ 1.9062705   0.8322557  -0.89...5956964\n",
            "   -0.53968245]\n",
            "  [ 0.52046865  0.80522346 -0.05476308 -0.2654866   0.02600498\n",
            "   -2.0909956 ]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d3e1b10>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:155: NotImplementedError\n",
            "\u001b[31m\u001b[1m_________________________________ test_transpose[cpu-None-shape0] __________________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), axes = None, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-1.0029142]]])\n",
            "_A         = array([[[-1.0029142]]], dtype=float32)\n",
            "axes       = None\n",
            "device     = cpu()\n",
            "np_axes    = (1, 2)\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:165: in transpose\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[-1.0029142]]])\n",
            "        axes       = None\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-1.0029142]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d52dba0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-1.0029142]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d52dba0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d52f280>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d52f280>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d52dba0>\n",
            "a = NDArray([[[-1.0029142]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-1.0029142]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d52dba0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:155: NotImplementedError\n",
            "\u001b[31m\u001b[1m_________________________________ test_transpose[cpu-None-shape1] __________________________________\u001b[0m\n",
            "\n",
            "shape = (4, 5, 6), axes = None, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[-2.58185983e-01  3.89220491e-02 -1.21470582e+00  3.07294279e-01\n",
            "   -2.18023267e-02 -7.92611111e-03]\n",
            " ...02182734e+00]\n",
            "  [ 1.41525996e+00 -5.51249869e-02 -2.43822598e+00 -5.65633655e-01\n",
            "    2.81555742e-01 -1.25411725e+00]]])\n",
            "_A         = array([[[-2.58185983e-01,  3.89220491e-02, -1.21470582e+00,\n",
            "          3.07294279e-01, -2.18023267e-02, -7.92611111e-03..., -5.51249869e-02, -2.43822598e+00,\n",
            "         -5.65633655e-01,  2.81555742e-01, -1.25411725e+00]]],\n",
            "      dtype=float32)\n",
            "axes       = None\n",
            "device     = cpu()\n",
            "np_axes    = (1, 2)\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:165: in transpose\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[-2.58185983e-01  3.89220491e-02 -1.21470582e+00  3.07294279e-01\n",
            "   -2.18023267e-02 -7.92611111e-03]\n",
            " ...02182734e+00]\n",
            "  [ 1.41525996e+00 -5.51249869e-02 -2.43822598e+00 -5.65633655e-01\n",
            "    2.81555742e-01 -1.25411725e+00]]])\n",
            "        axes       = None\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[-2.58185983e-01  3.89220491e-02 -1.21470582e+00  3.07294279e-01\n",
            "   -2.18023267e-02 -7.92611111e-03]\n",
            "...182734e+00]\n",
            "  [ 1.41525996e+00 -5.51249869e-02 -2.43822598e+00 -5.65633655e-01\n",
            "    2.81555742e-01 -1.25411725e+00]]]),)\n",
            "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d4043d0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[-2.58185983e-01  3.89220491e-02 -1.21470582e+00  3.07294279e-01\n",
            "   -2.18023267e-02 -7.92611111e-03]\n",
            "...182734e+00]\n",
            "  [ 1.41525996e+00 -5.51249869e-02 -2.43822598e+00 -5.65633655e-01\n",
            "    2.81555742e-01 -1.25411725e+00]]]),)\n",
            "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d4043d0>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d405030>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d405030>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d4043d0>\n",
            "a = NDArray([[[-2.58185983e-01  3.89220491e-02 -1.21470582e+00  3.07294279e-01\n",
            "   -2.18023267e-02 -7.92611111e-03]\n",
            "  [-6.2...  [ 1.41525996e+00 -5.51249869e-02 -2.43822598e+00 -5.65633655e-01\n",
            "    2.81555742e-01 -1.25411725e+00]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "a          = NDArray([[[-2.58185983e-01  3.89220491e-02 -1.21470582e+00  3.07294279e-01\n",
            "   -2.18023267e-02 -7.92611111e-03]\n",
            "  [-6.2...  [ 1.41525996e+00 -5.51249869e-02 -2.43822598e+00 -5.65633655e-01\n",
            "    2.81555742e-01 -1.25411725e+00]]], device=cpu())\n",
            "self       = <needle.ops.ops_mathematic.Transpose object at 0x7f5c9d4043d0>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:155: NotImplementedError\n",
            "\u001b[31m\u001b[1m_________________________________ test_logsumexp[cpu-shape0-None] __________________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), axes = None, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[1.3203336]]])\n",
            "A_t        = tensor([[[1.3203]]])\n",
            "_A         = array([[[1.3203336]]], dtype=float32)\n",
            "axes       = None\n",
            "device     = cpu()\n",
            "shape      = (1, 1, 1)\n",
            "t_axes     = (0, 1, 2)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:42: in logsumexp\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[1.3203336]]])\n",
            "        axes       = None\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[1.3203336]]]),)\n",
            "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f5c9d30a710>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[1.3203336]]]),)\n",
            "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f5c9d30a710>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d308fd0>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d308fd0>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f5c9d30a710>\n",
            "Z = NDArray([[[1.3203336]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "Z          = NDArray([[[1.3203336]]], device=cpu())\n",
            "self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f5c9d30a710>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:32: NotImplementedError\n",
            "\u001b[31m\u001b[1m___________________________________ test_logsumexp[cpu-shape1-0] ___________________________________\u001b[0m\n",
            "\n",
            "shape = (5, 3), axes = 0, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[ 0.02367617  1.2035823  -0.87347966]\n",
            " [-0.6478791   0.00766587  0.7891993 ]\n",
            " [-1.0347457  -0.08096286  0.1717708 ]\n",
            " [ 0.11561456 -0.9138545  -0.16316167]\n",
            " [-1.473657   -0.6495076   0.06253669]])\n",
            "A_t        = tensor([[ 0.0237,  1.2036, -0.8735],\n",
            "        [-0.6479,  0.0077,  0.7892],\n",
            "        [-1.0347, -0.0810,  0.1718],\n",
            "        [ 0.1156, -0.9139, -0.1632],\n",
            "        [-1.4737, -0.6495,  0.0625]])\n",
            "_A         = array([[ 0.02367617,  1.2035823 , -0.87347966],\n",
            "       [-0.6478791 ,  0.00766587,  0.7891993 ],\n",
            "       [-1.0347457 , -...7708 ],\n",
            "       [ 0.11561456, -0.9138545 , -0.16316167],\n",
            "       [-1.473657  , -0.6495076 ,  0.06253669]], dtype=float32)\n",
            "axes       = 0\n",
            "device     = cpu()\n",
            "shape      = (5, 3)\n",
            "t_axes     = 0\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:42: in logsumexp\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[ 0.02367617  1.2035823  -0.87347966]\n",
            " [-0.6478791   0.00766587  0.7891993 ]\n",
            " [-1.0347457  -0.08096286  0.1717708 ]\n",
            " [ 0.11561456 -0.9138545  -0.16316167]\n",
            " [-1.473657   -0.6495076   0.06253669]])\n",
            "        axes       = 0\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[ 0.02367617  1.2035823  -0.87347966]\n",
            " [-0.6478791   0.00766587  0.7891993 ]\n",
            " [-1.0347457  -0.08096286  0.1717708 ]\n",
            " [ 0.11561456 -0.9138545  -0.16316167]\n",
            " [-1.473657   -0.6495076   0.06253669]]),)\n",
            "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f5c9d9de260>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[ 0.02367617  1.2035823  -0.87347966]\n",
            " [-0.6478791   0.00766587  0.7891993 ]\n",
            " [-1.0347457  -0.08096286  0.1717708 ]\n",
            " [ 0.11561456 -0.9138545  -0.16316167]\n",
            " [-1.473657   -0.6495076   0.06253669]]),)\n",
            "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f5c9d9de260>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d9df550>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d9df550>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f5c9d9de260>\n",
            "Z = NDArray([[ 0.02367617  1.2035823  -0.87347966]\n",
            " [-0.6478791   0.00766587  0.7891993 ]\n",
            " [-1.0347457  -0.08096286  0.1717708 ]\n",
            " [ 0.11561456 -0.9138545  -0.16316167]\n",
            " [-1.473657   -0.6495076   0.06253669]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "Z          = NDArray([[ 0.02367617  1.2035823  -0.87347966]\n",
            " [-0.6478791   0.00766587  0.7891993 ]\n",
            " [-1.0347457  -0.08096286  0.1717708 ]\n",
            " [ 0.11561456 -0.9138545  -0.16316167]\n",
            " [-1.473657   -0.6495076   0.06253669]], device=cpu())\n",
            "self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f5c9d9de260>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:32: NotImplementedError\n",
            "\u001b[31m\u001b[1m___________________________________ test_logsumexp[cpu-shape2-1] ___________________________________\u001b[0m\n",
            "\n",
            "shape = (8, 3, 2), axes = 1, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[ 0.36176297  0.22754508]\n",
            "  [-1.3318138   0.70053685]\n",
            "  [ 1.1489531  -0.28914917]]\n",
            "\n",
            " [[ 0.28059793 -0....7]\n",
            "  [-0.1143936   0.7638089 ]]\n",
            "\n",
            " [[ 0.20855924 -1.0026654 ]\n",
            "  [-1.3880243   0.79729724]\n",
            "  [ 0.7556002  -1.3793341 ]]])\n",
            "A_t        = tensor([[[ 0.3618,  0.2275],\n",
            "         [-1.3318,  0.7005],\n",
            "         [ 1.1490, -0.2891]],\n",
            "\n",
            "        [[ 0.2806, -0.2148],\n",
            "...         [-0.1144,  0.7638]],\n",
            "\n",
            "        [[ 0.2086, -1.0027],\n",
            "         [-1.3880,  0.7973],\n",
            "         [ 0.7556, -1.3793]]])\n",
            "_A         = array([[[ 0.36176297,  0.22754508],\n",
            "        [-1.3318138 ,  0.70053685],\n",
            "        [ 1.1489531 , -0.28914917]],\n",
            "\n",
            "       [...  [[ 0.20855924, -1.0026654 ],\n",
            "        [-1.3880243 ,  0.79729724],\n",
            "        [ 0.7556002 , -1.3793341 ]]], dtype=float32)\n",
            "axes       = 1\n",
            "device     = cpu()\n",
            "shape      = (8, 3, 2)\n",
            "t_axes     = 1\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:42: in logsumexp\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[ 0.36176297  0.22754508]\n",
            "  [-1.3318138   0.70053685]\n",
            "  [ 1.1489531  -0.28914917]]\n",
            "\n",
            " [[ 0.28059793 -0....7]\n",
            "  [-0.1143936   0.7638089 ]]\n",
            "\n",
            " [[ 0.20855924 -1.0026654 ]\n",
            "  [-1.3880243   0.79729724]\n",
            "  [ 0.7556002  -1.3793341 ]]])\n",
            "        axes       = 1\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[ 0.36176297  0.22754508]\n",
            "  [-1.3318138   0.70053685]\n",
            "  [ 1.1489531  -0.28914917]]\n",
            "\n",
            " [[ 0.28059793 -0...\n",
            "  [-0.1143936   0.7638089 ]]\n",
            "\n",
            " [[ 0.20855924 -1.0026654 ]\n",
            "  [-1.3880243   0.79729724]\n",
            "  [ 0.7556002  -1.3793341 ]]]),)\n",
            "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f5c9d3f4f40>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[ 0.36176297  0.22754508]\n",
            "  [-1.3318138   0.70053685]\n",
            "  [ 1.1489531  -0.28914917]]\n",
            "\n",
            " [[ 0.28059793 -0...\n",
            "  [-0.1143936   0.7638089 ]]\n",
            "\n",
            " [[ 0.20855924 -1.0026654 ]\n",
            "  [-1.3880243   0.79729724]\n",
            "  [ 0.7556002  -1.3793341 ]]]),)\n",
            "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f5c9d3f4f40>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3f5060>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d3f5060>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f5c9d3f4f40>\n",
            "Z = NDArray([[[ 0.36176297  0.22754508]\n",
            "  [-1.3318138   0.70053685]\n",
            "  [ 1.1489531  -0.28914917]]\n",
            "\n",
            " [[ 0.28059793 -0.214813...36   0.7638089 ]]\n",
            "\n",
            " [[ 0.20855924 -1.0026654 ]\n",
            "  [-1.3880243   0.79729724]\n",
            "  [ 0.7556002  -1.3793341 ]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "Z          = NDArray([[[ 0.36176297  0.22754508]\n",
            "  [-1.3318138   0.70053685]\n",
            "  [ 1.1489531  -0.28914917]]\n",
            "\n",
            " [[ 0.28059793 -0.214813...36   0.7638089 ]]\n",
            "\n",
            " [[ 0.20855924 -1.0026654 ]\n",
            "  [-1.3880243   0.79729724]\n",
            "  [ 0.7556002  -1.3793341 ]]], device=cpu())\n",
            "self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f5c9d3f4f40>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:32: NotImplementedError\n",
            "\u001b[31m\u001b[1m___________________________________ test_logsumexp[cpu-shape3-2] ___________________________________\u001b[0m\n",
            "\n",
            "shape = (8, 3, 2), axes = 2, device = cpu()\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
            "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
            "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "A          = needle.Tensor([[[ 0.63066757  0.633094  ]\n",
            "  [ 0.09944986  0.5089685 ]\n",
            "  [ 0.08942481  1.0787337 ]]\n",
            "\n",
            " [[-0.03613293  0.... ]\n",
            "  [ 0.31761032 -0.43654212]]\n",
            "\n",
            " [[ 1.2888578  -0.13862996]\n",
            "  [ 0.79906607  2.1231344 ]\n",
            "  [-0.01205583 -1.9159924 ]]])\n",
            "A_t        = tensor([[[ 0.6307,  0.6331],\n",
            "         [ 0.0994,  0.5090],\n",
            "         [ 0.0894,  1.0787]],\n",
            "\n",
            "        [[-0.0361,  0.9820],\n",
            "...         [ 0.3176, -0.4365]],\n",
            "\n",
            "        [[ 1.2889, -0.1386],\n",
            "         [ 0.7991,  2.1231],\n",
            "         [-0.0121, -1.9160]]])\n",
            "_A         = array([[[ 0.63066757,  0.633094  ],\n",
            "        [ 0.09944986,  0.5089685 ],\n",
            "        [ 0.08942481,  1.0787337 ]],\n",
            "\n",
            "       [...  [[ 1.2888578 , -0.13862996],\n",
            "        [ 0.79906607,  2.1231344 ],\n",
            "        [-0.01205583, -1.9159924 ]]], dtype=float32)\n",
            "axes       = 2\n",
            "device     = cpu()\n",
            "shape      = (8, 3, 2)\n",
            "t_axes     = 2\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:42: in logsumexp\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
            "        a          = needle.Tensor([[[ 0.63066757  0.633094  ]\n",
            "  [ 0.09944986  0.5089685 ]\n",
            "  [ 0.08942481  1.0787337 ]]\n",
            "\n",
            " [[-0.03613293  0.... ]\n",
            "  [ 0.31761032 -0.43654212]]\n",
            "\n",
            " [[ 1.2888578  -0.13862996]\n",
            "  [ 0.79906607  2.1231344 ]\n",
            "  [-0.01205583 -1.9159924 ]]])\n",
            "        axes       = 2\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
            "        args       = (needle.Tensor([[[ 0.63066757  0.633094  ]\n",
            "  [ 0.09944986  0.5089685 ]\n",
            "  [ 0.08942481  1.0787337 ]]\n",
            "\n",
            " [[-0.03613293  0...\n",
            "  [ 0.31761032 -0.43654212]]\n",
            "\n",
            " [[ 1.2888578  -0.13862996]\n",
            "  [ 0.79906607  2.1231344 ]\n",
            "  [-0.01205583 -1.9159924 ]]]),)\n",
            "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f5c9d308940>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
            "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
            "        inputs     = (needle.Tensor([[[ 0.63066757  0.633094  ]\n",
            "  [ 0.09944986  0.5089685 ]\n",
            "  [ 0.08942481  1.0787337 ]]\n",
            "\n",
            " [[-0.03613293  0...\n",
            "  [ 0.31761032 -0.43654212]]\n",
            "\n",
            " [[ 1.2888578  -0.13862996]\n",
            "  [ 0.79906607  2.1231344 ]\n",
            "  [-0.01205583 -1.9159924 ]]]),)\n",
            "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f5c9d308940>\n",
            "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d309d80>\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
            "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f5c9d309d80>\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f5c9d308940>\n",
            "Z = NDArray([[[ 0.63066757  0.633094  ]\n",
            "  [ 0.09944986  0.5089685 ]\n",
            "  [ 0.08942481  1.0787337 ]]\n",
            "\n",
            " [[-0.03613293  0.982036...032 -0.43654212]]\n",
            "\n",
            " [[ 1.2888578  -0.13862996]\n",
            "  [ 0.79906607  2.1231344 ]\n",
            "  [-0.01205583 -1.9159924 ]]], device=cpu())\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
            "\n",
            "Z          = NDArray([[[ 0.63066757  0.633094  ]\n",
            "  [ 0.09944986  0.5089685 ]\n",
            "  [ 0.08942481  1.0787337 ]]\n",
            "\n",
            " [[-0.03613293  0.982036...032 -0.43654212]]\n",
            "\n",
            " [[ 1.2888578  -0.13862996]\n",
            "  [ 0.79906607  2.1231344 ]\n",
            "  [-0.01205583 -1.9159924 ]]], device=cpu())\n",
            "self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f5c9d308940>\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:32: NotImplementedError\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_ewise_fn[cpu-shape0-divide]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_ewise_fn[cpu-shape0-subtract]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_ewise_fn[cpu-shape1-divide]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_ewise_fn[cpu-shape1-subtract]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_scalar_fn[cpu-shape0-divide]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_scalar_fn[cpu-shape1-divide]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cpu-16-16-16]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cpu-8-8-8]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cpu-1-2-3]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cpu-3-4-5]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cpu-5-4-3]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cpu-16-16-32]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cpu-64-64-64]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cpu-72-72-72]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cpu-72-73-74]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cpu-74-73-72]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cpu-128-128-128]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_power[cpu-shape0]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_power[cpu-shape1]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_log[cpu-shape0]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_log[cpu-shape1]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_exp[cpu-shape0]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_exp[cpu-shape1]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_relu[cpu-shape0]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_relu[cpu-shape1]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh[cpu-shape0]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh[cpu-shape1]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh_backward[cpu-shape0]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh_backward[cpu-shape1]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack[cpu-shape0-0-1]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack[cpu-shape1-0-2]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack[cpu-shape2-2-5]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cpu-shape0-0-1]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cpu-shape1-0-2]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cpu-shape2-2-5]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation[cpu-shape0-None]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation[cpu-shape1-0]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation[cpu-shape2-1]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation[cpu-shape3-2]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape0-None]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape1-0]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape2-1]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape3-2]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_broadcast_to[cpu-shape0-shape_to0]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_broadcast_to[cpu-shape1-shape_to1]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_reshape[cpu-shape0-shape_to0]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_reshape[cpu-shape1-shape_to1]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cpu-axes0-shape0]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cpu-axes0-shape1]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cpu-axes1-shape0]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cpu-axes1-shape1]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cpu-None-shape0]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cpu-None-shape1]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape0-None]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape1-0]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape2-1]\u001b[0m - NotImplementedError\n",
            "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape3-2]\u001b[0m - NotImplementedError\n",
            "\u001b[31m==================== \u001b[31m\u001b[1m57 failed\u001b[0m, \u001b[32m2 passed\u001b[0m, \u001b[33m59 skipped\u001b[0m, \u001b[33m1685 deselected\u001b[0m\u001b[31m in 10.91s\u001b[0m\u001b[31m ====================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -l -v -k \"nd_backend\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUf-d6OOHwpc"
      },
      "outputs": [],
      "source": [
        "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"new_nd_backend\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-kGXVA2Hwpc"
      },
      "source": [
        "## Part 2: CIFAR-10 dataset [10 points]\n",
        "\n",
        "Next, you will write support for the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) image classification dataset, which consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50k training images and 10k test images.\n",
        "\n",
        "Start by implementing the `__init__` function in the `CIFAR10Dataset` class in `python/needle/data/datasets/cifar10_dataset.py`. You can read in the link above how to properly read the CIFAR-10 dataset files you downloaded at the beginning of the homework. Also fill in `__getitem__` and `__len__`. Note that the return shape of the data from `__getitem__` should be in order (3, 32, 32).\n",
        "\n",
        "Copy `python/needle/data/data_transforms.py` and `python/needle/data/data_basic.py` from previous homeworks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpuntwdWHwpc"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -l -v -k \"test_cifar10\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23Cqz4DiHwpc"
      },
      "outputs": [],
      "source": [
        "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"cifar10\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "9BtgpGiZHwpc"
      },
      "source": [
        "## Part 3: Convolutional neural network [40 points]\n",
        "\n",
        "Here's an outline of what you will do in this task.\n",
        "\n",
        "In `python/needle/backend_ndarray/ndarray.py`, implement:\n",
        "- `flip`\n",
        "- `pad`\n",
        "\n",
        "In `python/needle/ops_mathematic.py`, implement (forward and backward):\n",
        "- `Flip`\n",
        "- `Dilate`\n",
        "- `UnDilate`\n",
        "- `Conv`\n",
        "\n",
        "In `python/needle/nn/nn_conv.py`, implement:\n",
        "- `Conv`\n",
        "\n",
        "In `apps/models.py`, fill in the `ResNet9` class.  \n",
        "\n",
        "In `apps/simple_ml.py`, fill in:\n",
        "- `epoch_general_cifar10`,\n",
        "- `train_cifar10`\n",
        "- `evaluate_cifar10`\n",
        "\n",
        "We have provided a `BatchNorm2d` implementation in `python/needle/nn/nn_basic.py` for you as a wrapper around your previous `BatchNorm1d` implementation.\n",
        "\n",
        "**Note**: Remember to copy the solution of `nn_basic.py` from previous homework, make sure to not overwrite the `BatchNorm2d` module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsNz7kunHwpc"
      },
      "source": [
        "### Padding ndarrays\n",
        "\n",
        "Convolution as typically implemented in deep learning libraries cuts down the size of inputs;\n",
        "e.g., a (1, 32, 32, 3) image convolved with a 3x3 filter would give a (1, 30, 30, c) output.\n",
        "A way around this is to pad the input ndarray before performing convolution, e.g., pad with zeros to get a (1, 34, 34, 3) ndarray so that the result is (1, 32, 32, 3).\n",
        "\n",
        "Padding is also required for the backward pass of convolution.\n",
        "\n",
        "You should implement `pad` in `ndarray.py` to closely reflect the behavior of `np.pad`.\n",
        "That is, `pad` should take a tuple of 2-tuples with length equal to the number of dimensions of the array,\n",
        "where each element in the 2-tuple corresponds to \"left padding\" and \"right padding\", respectively.\n",
        "\n",
        "For example, if `A` is a (10, 32, 32, 8) ndarray (think NHWC), then `A.pad( (0, 0), (2, 2), (2, 2), (0, 0) )` would be a (10, 36, 36, 8) ndarray where the \"spatial\" dimension has been padded by two zeros on all sides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yopMxwegHwpc"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -l -v -k \"pad_forward\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9gQk6HHHwpc"
      },
      "source": [
        "-------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rNKULMUHwpc"
      },
      "source": [
        "### Flipping ndarrays & FlipOp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JlHRhzaHwpc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import ctypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vy829KaHwpc"
      },
      "source": [
        "Some utility code for a demonstration below which you can probably ignore. It might be instructive to check out the `offset` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_2q3KQJHwpc"
      },
      "outputs": [],
      "source": [
        "# reads off the underlying data array in order (i.e., offset 0, offset 1, ..., offset n)\n",
        "# i.e., ignoring strides\n",
        "def raw_data(X):\n",
        "    X = np.array(X) # copy, thus compact X\n",
        "    return np.frombuffer(ctypes.string_at(X.ctypes.data, X.nbytes), dtype=X.dtype, count=X.size)\n",
        "\n",
        "# Xold and Xnew should reference the same underlying data\n",
        "def offset(Xold, Xnew):\n",
        "    assert Xold.itemsize == Xnew.itemsize\n",
        "    # compare addresses to the beginning of the arrays\n",
        "    return (Xnew.ctypes.data - Xold.ctypes.data)//Xnew.itemsize\n",
        "\n",
        "def strides(X):\n",
        "    return ', '.join([str(x//X.itemsize) for x in X.strides])\n",
        "\n",
        "def format_array(X, shape):\n",
        "    assert len(shape) == 3, \"I only made this formatting work for ndims = 3\"\n",
        "    def chunks(l, n):\n",
        "        n = max(1, n)\n",
        "        return (l[i:i+n] for i in range(0, len(l), n))\n",
        "    a = [str(x) if x >= 10 else ' ' + str(x) for x in X]\n",
        "    a = ['(' + ' '.join(y) + ')' for y in [x for x in chunks(a, shape[-1])]]\n",
        "    a = ['|' + ' '.join(y) + '|' for y in [x for x in chunks(a, shape[-2])]]\n",
        "    return '  '.join(a)\n",
        "\n",
        "def inspect_array(X, *, is_a_copy_of):\n",
        "    # compacts X, then reads it off in order\n",
        "    print('Data: %s' % format_array(raw_data(X), X.shape))\n",
        "    # compares address of X to copy_of, thus finding X's offset\n",
        "    print('Offset: %s' % offset(is_a_copy_of, X))\n",
        "    print('Strides: %s' % strides(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "VYUJmY0NHwpc"
      },
      "source": [
        "In order to implement the backwards pass of 2D convolution, we will (probably) need a function which _flips_\n",
        "axes of ndarrays. We say \"probably\" because you could probably cleverly implement your convolution forward\n",
        "function to avoid this. However, we think it is easiest to think about this if you have the ability to \"flip\" the kernel along its vertical and horizontal dimensions.\n",
        "\n",
        "We will try to build up your intuition for the \"flip\" operation below in order to help you figure out how to implement it in `ndarray.py`. To do that, we explore numpy's `np.flip` function below. One thing to note is that\n",
        "`flip` is typically implemented by using negative strides and changing the _offset_ of the underlying array.\n",
        "\n",
        "For example, flipping an array on _all_ of its axes is equivalent to reversing the array. In this case, you can imagine that we would want all the strides to be negative, and the offset to be the length of the array (to start at the end of the array and \"stride\" backwards).\n",
        "\n",
        "Since we did not explicitly support negative strides in our implementation for the last homework, we will merely call `NDArray.make` with them to make our \"flipped\" array and then immediately call `.compact()`. Other than changing unsigned ints to signed ints in a few places, we suspect your existing `compact` function should not have to change at all to accomodate negative strides. In the .cc and .cu files we distributed, we have already changed the function signatures to reflect this.\n",
        "\n",
        "Alternatively, you could simply implement `flip` in the CPU backend by copying memory, which you _may_ find more intuitive. We suggest following our mini tutorial below to keep your implementation Python-focused, since we believe it is involves approximately the same amount of effort to implement it slightly more naively in C."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_lgw3YUHwpc"
      },
      "source": [
        "Use this array as reference for the other examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoxMwWDlHwpd"
      },
      "outputs": [],
      "source": [
        "A = np.arange(1, 25).reshape(3, 2, 4)\n",
        "inspect_array(A, is_a_copy_of=A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7eF5sGWHwpd"
      },
      "source": [
        "We have put brackets around each axis of the array. Notice that for this array, the offset is 0 and the strides are all positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke15CO7vHwpd"
      },
      "source": [
        "----------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtUaZ632Hwpd"
      },
      "source": [
        "See what happens when you flip the array along the last axis below.\n",
        "Note that the `inspect_array` function compacts the array after flipping it so you can see the\n",
        "\"logical\" order of the data, and the offset is calculated by comparing the address of the **non**-compacted\n",
        "flipped array with that of `is_copy_of`, i.e., the array `A` we looked at above.\n",
        "\n",
        "That is, we are looking at how numpy calculates the strides and offset for flipped arrays in order\n",
        "to copy this behavior in our own implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWp_IUDXHwpd"
      },
      "outputs": [],
      "source": [
        "inspect_array(np.flip(A, (2,)), is_a_copy_of=A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_6nNn1IHwpd"
      },
      "source": [
        "So flipping the last axis reverses the order of the elements within each 4-dimensional \"cell\", as you can see above. The stride corresponding to the axis we flipped has been negated. And the offset is 3 -- this makes sense, e.g., because we want the new \"first\" element of the array to be 4, which was at index 3 in `A`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_XlgmV_Hwpd"
      },
      "outputs": [],
      "source": [
        "inspect_array(np.flip(A, (1,)), is_a_copy_of=A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYE9kbGLHwpd"
      },
      "source": [
        "Again for the middle axis: we negate the middle stride, and the offset is 4, which seems reasonable since we now want the first element to be 5, which was at index 4 in the original array `A`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTdiZxexHwpd"
      },
      "outputs": [],
      "source": [
        "inspect_array(np.flip(A, (0,)), is_a_copy_of=A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkBHCzETHwpd"
      },
      "source": [
        "Try to infer the more general algorithm for computing the offset given the axis to flip."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oasmWSInHwpd"
      },
      "source": [
        "----------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t8jbFmbHwpd"
      },
      "source": [
        "Observe what happens when we flip _all_ axes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsHjWkIrHwpd"
      },
      "outputs": [],
      "source": [
        "inspect_array(np.flip(A, (0, 1, 2)), is_a_copy_of=A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw6vLy7jHwpd"
      },
      "source": [
        "As mentioned earlier, the offset is then sufficient to point to the last element of the array, and this is just the \"reverse order\" version of `A`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8g38DN_Hwpd"
      },
      "source": [
        "When we flip just axes 1 and 0..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsxZ9_pJHwpd"
      },
      "outputs": [],
      "source": [
        "inspect_array(np.flip(A, (0, 1)), is_a_copy_of=A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e4d4abKHwpn"
      },
      "source": [
        "The offset is 20. Looking back on our previous offset computations, do you notice something?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKg_ztzfHwpn"
      },
      "source": [
        "-------------------\n",
        "\n",
        "With this exploration of numpy's ndarray flipping functionality, which uses negative strides and a custom offset,\n",
        "try to implement `flip` in `ndarray.py`. You also must implement \"flip\" forward and backward functions in `ops_mathematic.py`; note that these should be extremely short.\n",
        "\n",
        "**Important:** You should call NDArray.make with the new strides and offset, and then immediately `.compact()` this array. The resulting array is then copied and has positive strides. We want this (less-than-optimal) behavior because we did not account for negative strides in our previous implementation. _Aside:_ If you want, consider where/if negative strides break your implementation. `__getitem__` definitely doesn't work due to how we processed slices; is there anything else? (_Note_: this isn't graded.)\n",
        "\n",
        "Also, if you want to add a `flip` operator implementation on the CPU/CUDA backends instead, that's also okay.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_JXcTPCHwpn"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -l -v -k \"flip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV09yCNcHwpn"
      },
      "source": [
        "-------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4mZiPz9Hwpn"
      },
      "source": [
        "### Dilation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LA-mQt8Hwpn"
      },
      "source": [
        "The dilation operator puts zeros between elements of an ndarray. We will need it for computing the backward pass of convolution when the stride of the convolution is greater than 1. As an example, dilation should do the following to a 2x2 matrix when dilated by 1 on both axes:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4\n",
        "\\end{bmatrix}\n",
        "\\Longrightarrow\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 2 & 0 \\\\\n",
        "0 & 0 & 0 & 0 \\\\\n",
        "3 & 0 & 4 & 0 \\\\\n",
        "0 & 0 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "To get some intuition for why we need dilation for the backward pass of strided convolution, consider a  `stride=2`, `padding=\"same\"`, `input_channels=output_channels=8` convolution applied to an input of size (10, 32, 32, 8). The resulting output will be of size (10, 16, 16, 8) due to the stride, and thus `out_grad` will have shape (10, 16, 16, 8). Yet, the gradient of the input needs to, of course, have shape (10, 32, 32, 8) -- so we must need to increase the size of `out_grad` in some way. Consider also that you could implement strided convolution as `Conv(x)[:, ::2, ::2, :]`, i.e., only keeping every other pixel in the spatial dimension.\n",
        "\n",
        "\n",
        "Implement `Dilate` and `UnDilate` in `ops_mathematic.py`. Each operator takes two additional parameters (in attrs): the `dilation` amount and the `axes` to dilate. You must also implement the corresponding op `UnDilate`, whose forward pass will be used to implement the gradient of `Dilate`. (This is so we do not have to implement `GetItem` and `SetItem` ops, which can be highly inefficient to backprop through without additional optimizations.)\n",
        "\n",
        "**Note**: The dilation amount is additive, not multiplicative. In the example above, a dilation of `1` implies adding one row/column of zeros between each element along each dilated axis (and one removed row/column for each undilated axis). A dilation of `0` means no change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aaIBbQ7Hwpn"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -l -v -k \"dilate\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TsYwEpWHwpn"
      },
      "source": [
        "---------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skrn6-j2Hwpn"
      },
      "source": [
        "### Submit new ops (flip/dilation) to mugrade [10 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i15Ix-OuHwpn"
      },
      "outputs": [],
      "source": [
        "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"new_ops\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yw0X76_Hwpn"
      },
      "source": [
        "-----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycK4GA4yHwpn"
      },
      "source": [
        "### Convolution forward\n",
        "\n",
        "Implement the forward pass of 2D multi-channel convolution in `ops_mathematic.py`. You should probably refer to [this notebook](https://github.com/dlsyscourse/public_notebooks/blob/main/convolution_implementation.ipynb) from lecture, which implements 2D multi-channel convolution using im2col in numpy.\n",
        "\n",
        "**Note:** Your convolution op should accept tensors in the NHWC format, as in the example above, and weights in the format (kernel_size, kernel_size, input_channels, output_channels).\n",
        "\n",
        "However, you will need to add two additional features. Your convolution function should accept arguments for `padding` (default 0) and `stride` (default 1). For `padding`, you should simply apply your padding function to the spatial dimensions (i.e., axes 1 and 2).\n",
        "\n",
        "Implementing strided convolution should consist of a relatively small set of changes to your plain convolution implementation.\n",
        "\n",
        "We recommend working your way up through the full feature set: Implement convolution without stride first, ensuring you pass some of the tests below, and then add in support for stride."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jlYnQKDHwpo"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -l -v -k \"op_conv and forward\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMbp2L0zHwpo"
      },
      "source": [
        "-----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkUs74MNHwpo"
      },
      "source": [
        "### Convolution backward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D4PbJG4Hwpo"
      },
      "source": [
        "Finding the gradients of 2D multi-channel convolution can be technically quite challenging (especially \"rigorously\"). We will try to provide some useful hints here. Basically, we encourage you to make use of the surprising fact that _whatever makes the dimensions work out is typically right_.\n",
        "\n",
        "Ultimately, the backward pass of convolution can be done in terms of the convolution operator itself, with some clever manipulations using `flip`, `dilate`, and multiple applications of `transpose` to both the arguments and the results.\n",
        "\n",
        "In the last section, we essentially implemented convolution as a matrix product: ignoring the various restride and reshape operations, we basically have something like `X @ W`, where `X` is the input and `W` is the weight. We also have `out_grad`, which is the same shape as `X @ W`. Now, you have already implemented the backward pass of matrix multiplication in a previous assignment, and we can use this knowledge to get some insight into the backward pass of convolution. In particular, referencing your matmul backward implementation, you may notice (heuristically speaking here):\n",
        "\n",
        "`X.grad = out_grad @ W.transpose` \\\n",
        "`W.grad = X.transpose @ out_grad`\n",
        "\n",
        "Surprisingly enough, things work out if we just assume that these are also convolutions (and now assuming that `out_grad`, `W`, and `X` are tensors amenable to 2D multi-channel convolution instead of matrices):\n",
        "\n",
        "`X.grad = ≈conv(≈out_grad, ≈W)` \\\n",
        "`W.grad = ≈conv(≈X, ≈out_grad)`\n",
        "\n",
        "In which the \"≈\" indicates that you need to apply some additional operators to these terms in order to get the dimensions to work out, such as permuting/transposing axes, dilating, changing the `padding=` argument to the convolution function, or permuting/transposing axes of the resulting convolution.\n",
        "\n",
        "As we saw on the [last few slides here](https://dlsyscourse.org/slides/conv_nets.pdf) in class, the transpose of a convolution can be found by simply flipping the kernel. Since we're working in 2D instead of 1D, this means flipping the kernel both vertically and horizontally (thus why we implemented `flip`).\n",
        "\n",
        "Summarizing some hints for both `X.grad` and `W.grad`:\n",
        "\n",
        "`X.grad`\n",
        "- The convolution of `out_grad` and `W`, with some operations applied to those\n",
        "- `W` should be flipped over both the kernel dimensions\n",
        "- If the convolution is strided, increase the size of `out_grad` with a corresponding dilation\n",
        "- Do an example to analyze dimensions: note the shape you want for `X.grad`, and think about how you must permute/transpose the arguments and add padding to the convolution to achieve this shape\n",
        "    - This padding depends on both the kernel size and the `padding` argument to the convolution\n",
        "\n",
        "`W.grad`\n",
        "- The convolution of `X` and `out_grad`, with some operations applied to those\n",
        "- The gradients of `W` must be accumulated over the batches; how can you make the conv operator itself do this accumulation?\n",
        "    - Consider turning batches into channels via transpose/permute\n",
        "- Analyze dimensions: how can you modify `X` and `out_grad` so that the shape of their convolution matches the shape of `W`? You may need to transpose/permute the result.\n",
        "    - Remember to account for the `padding` argument passed to convolution\n",
        "\n",
        "General tips\n",
        "- Deal with strided convolutions last (you should be able to just drop in `dilate` when you've passed most of the tests)\n",
        "- Start with the case where `padding=0`, then consider changing `padding` arguments\n",
        "- You can \"permute\" axes with multiple calls to `transpose`\n",
        "\n",
        "It might also be useful to skip ahead to nn.Conv, pass the forward tests, and then use both the tests below and the nn.Conv backward tests to debug your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7I7UHni8Hwpo"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -l -v -k \"op_conv and backward\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NfIyPX3Hwpo"
      },
      "source": [
        "-----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skelPJaDHwpo"
      },
      "source": [
        "### nn.Conv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "Zv0Abx2oHwpo"
      },
      "source": [
        "#### Fixing init._calculate_fans for convolution\n",
        "Previously, we have implemented Kaiming uniform/normal initializations, where we essentially assigned `fan_in = input_size` and `fan_out = output_size`.\n",
        "For convolution, this becomes somewhat more detailed, in that you should multiply both of these by the \"receptive field size\", which is in this case just the product of the kernel sizes -- which in our case are always going to be the same, i.e., $k\\times k$ kernels.\n",
        "\n",
        "**You will need to edit your `kaiming_uniform` in `python/needle/init/init_initializers.py`, etc. init functions to support multidimensional arrays.** In particular, it should support a new `shape` argument which is then passed to, e.g., the underlying `rand` function. Specifically, if the argument `shape` is not `None`, then ignore `fan_in` and `fan_out`, and use the value of `shape` for initializations instead.\n",
        "\n",
        "You can test this below; though it is not _directly_ graded, it must match ours to pass the nn.Conv mugrade tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "249uXGuAHwpo"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -l -v -k \"kaiming_uniform\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQK_mDvyHwpo"
      },
      "source": [
        "#### Implementing nn.Conv\n",
        "\n",
        "Essentially, nn.Conv is just a wrapper of the convolution operator we previously implemented\n",
        "which adds a bias term, initializes the weight and bias, and ensures that the padding is set so that the input and output dimensions are the same (in the `stride=1` case, anyways).\n",
        "\n",
        "Importantly, nn.Conv should support NCHW format instead of NHWC format. In particular, we think this makes more sense given our current BatchNorm implementation. You can implement this by applying `transpose` twice to both the input and output.  \n",
        "\n",
        "- Ensure nn.Conv works for `(N, C, H, W)` tensors even though we implemented the conv op for `(N, H, W, C)` tensors\n",
        "- Initialize the `(k, k, i, o)` weight tensor using Kaiming uniform initialization with default settings\n",
        "- Initialize the `(o,)` bias tensor using uniform initialization on the interval $\\displaystyle\\pm\\frac{1}{\\sqrt{\\verb|in_channels| \\times \\verb|kernel_size|^2}}$\n",
        "- Calculate the appropriate padding to ensure input and output dimensions are the same\n",
        "- Calculate the convolution, then add the properly-broadcasted bias term if present\n",
        "\n",
        "You can now test your nn.Conv against PyTorch's nn.Conv2d with the two PyTest calls below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ItK5vKrHwpo"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -l -v -k \"nn_conv_forward\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPu0Z-JUHwpo"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -l -v -k \"nn_conv_backward\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3YUCg0wHwpo"
      },
      "source": [
        "-----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5ZdpgRbHwpo"
      },
      "source": [
        "### Submit nn.Conv to mugrade [20 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDGT6MXtHwpo"
      },
      "outputs": [],
      "source": [
        "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"conv_forward\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzPhmZLLHwpo"
      },
      "outputs": [],
      "source": [
        "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"conv_backward\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcok0LqcHwpo"
      },
      "source": [
        "------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "YuZlAZrqHwpo"
      },
      "source": [
        "### Implementing \"ResNet9\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmF4KCM2Hwpo"
      },
      "source": [
        "You will now use your convolutional layer to implement a model similar to _ResNet9_, which is known to be a reasonable model for getting good accuracy on CIFAR-10 quickly (see [here](https://github.com/davidcpage/cifar10-fast)). Our main change is that we used striding instead of pooling and divided all of the channels by 4 for the sake of performance (as our framework is not as well-optimized as industry-grade frameworks).\n",
        "\n",
        "In the figure below, before the first linear layer, you should \"flatten\" the tensor. You can use the module `Flatten` in `nn_basic.py`, or you can simply use `.reshape` in the `forward()` method of your ResNet9.\n",
        "\n",
        "Make sure that you pass the device to all modules in your model; otherwise, you will get errors about mismatched devices when trying to run with CUDA.\n",
        "\n",
        "<center><img src=\"https://github.com/dlsyscourse/hw4/blob/main/ResNet9.png?raw=true\" alt=\"ResNet9\" style=\"width: 400px;\" /></center>\n",
        "\n",
        "We have tried to make it easier to pass the tests here than for previous assignments where you have implemented models. In particular, we are just going to make sure it has the right number of parameters and similar accuracy and loss after 1 or 2 batches of CIFAR-10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "OV_WfYdiHwpp"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -l -v -k \"resnet9\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tnka6WVHwpp"
      },
      "source": [
        "Now we can train a ResNet on CIFAR10: (remember to copy the solutions in `python/needle/optim.py` from previous homeworks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FVRwwS8Hwpp"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -l -v -k \"train_cifar10\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE3l6wv4Hwpp"
      },
      "source": [
        "### Submit ResNet9 to mugrade [10 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZcXZzudHwpp"
      },
      "outputs": [],
      "source": [
        "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"resnet9\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5ma2t62Hwpp"
      },
      "source": [
        "-----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlr4n2IPHwpp"
      },
      "source": [
        "Now, you can train your model on CIFAR-10 using the following code. Note that this is likely going to be quite slow, and also  not all that accurate due to the lack of data augmentation. You should expect it to take around 500s per epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIt3CX5aHwpp"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('./python')\n",
        "sys.path.append('./apps')\n",
        "import needle as ndl\n",
        "from models import ResNet9\n",
        "from simple_ml import train_cifar10, evaluate_cifar10\n",
        "\n",
        "device = ndl.cpu()\n",
        "dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
        "dataloader = ndl.data.DataLoader(\\\n",
        "         dataset=dataset,\n",
        "         batch_size=128,\n",
        "         shuffle=True,)\n",
        "model = ResNet9(device=device, dtype=\"float32\")\n",
        "train_cifar10(model, dataloader, n_epochs=10, optimizer=ndl.optim.Adam,\n",
        "      lr=0.001, weight_decay=0.001)\n",
        "evaluate_cifar10(model, dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36tCWcm-Hwpp"
      },
      "source": [
        "## Part 4: Recurrent neural network [10 points]\n",
        "\n",
        "**Note:** In the following sections, you may find yourself wanting to index into tensors, i.e., to use getitem or setitem. However, we have not implemented these for tensors in our library; instead, you should use `stack` and `split` operations.\n",
        "\n",
        "In `python/needle/nn_sequence.py`, implement `RNNCell`.\n",
        "\n",
        "$h^\\prime = \\text{tanh}(xW_{ih} + b_{ih} + hW_{hh} + b_{hh})$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
        "\n",
        "All weights and biases should be uniformly initialized on the interval $\\displaystyle\\pm\\frac{1}{\\sqrt{\\verb|hidden_size|}}$.\n",
        "\n",
        "In `python/needle/nn_sequence.py`, implement `RNN`.\n",
        "\n",
        "For each element in the input sequence, each layer computes the following function:\n",
        "\n",
        "$h_t = \\text{tanh}(x_tW_{ih} + b_{ih} + h_{(t-1)}W_{hh} + b_{hh})$\n",
        "\n",
        "where $h_t$ is the hidden state at time $t$, $x_t$ is the input at time $t$, and $h_{(t-1)}$ is the hidden state of the previous layer at time $t-1$ or the initial hidden state at time $0$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
        "\n",
        "In a multi-layer RNN, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4IPlDTXHwpp"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -l -v -k \"test_rnn\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viCjoEaEHwpp"
      },
      "outputs": [],
      "source": [
        "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"rnn\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEeqPuaRHwpp"
      },
      "source": [
        "## Part 5: Long short-term memory network [10 points]\n",
        "In `python/needle/nn/nn_sequence.py`, implement `Sigmoid`.\n",
        "\n",
        "$$\\sigma(x) = \\frac{1}{1 + \\text{exp}(-x)}$$\n",
        "\n",
        "In `python/needle/nn/nn_sequence.py`, implement `LSTMCell`.\n",
        "\n",
        "\\begin{align*}\n",
        "i &= \\sigma(xW_{ii} + b_{ii} + hW_{hi} + b_{hi}) \\\\\n",
        "f &= \\sigma(xW_{if} + b_{if} + hW_{hf} + b_{hf}) \\\\\n",
        "g &= \\text{tanh}(xW_{ig} + b_{ig} + hW_{hg} + b_{hg}) \\\\\n",
        "o &= \\sigma(xW_{io} + b_{io} + hW_{ho} + b_{ho}) \\\\\n",
        "c^\\prime &= f * c + i * g \\\\\n",
        "h^\\prime &= o * \\text{tanh}(c^\\prime)\n",
        "\\end{align*}\n",
        "\n",
        "where $\\sigma$ is the sigmoid function, and $i$, $f$, $g$, $o$ are the input, forget, cell, and output gates, respectively.\n",
        "\n",
        "All weights and biases should be uniformly initialized on the interval $\\displaystyle\\pm\\frac{1}{\\sqrt{\\verb|hidden_size|}}$.\n",
        "\n",
        "Now implement `LSTM` in `python/needle/nn/nn_sequence.py`, which applies a multi-layer LSTM RNN to an input sequence. For each element in the input sequence, each layer computes the following function:\n",
        "\n",
        "\\begin{align*}\n",
        "i_t &= \\sigma(x_tW_{ii} + b_{ii} + h_{(t-1)}W_{hi} + b_{hi}) \\\\\n",
        "f_t &= \\sigma(x_tW_{if} + b_{if} + h_{(t-1)}W_{hf} + b_{hf}) \\\\\n",
        "g_t &= \\text{tanh}(x_tW_{ig} + b_{ig} + h_{(t-1)}W_{hg} + b_{hg}) \\\\\n",
        "o_t &= \\sigma(x_tW_{io} + b_{io} + h_{(t-1)}W_{ho} + b_{ho}) \\\\\n",
        "c_t &= f * c_{(t-1)} + i * g \\\\\n",
        "h_t &= o * \\text{tanh}(c_t)\n",
        "\\end{align*}\n",
        "\n",
        "where $h_t$ is the hidden state at time $t$, $c_t$ is the cell state at time $t$, $x_t$ is the input at time $t$, $h_{(t-1)}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $0$, and $i_t$, $f_t$, $g_t$, $o_t$ are the input, forget, cell, and output gates at time $t$ respectively.\n",
        "\n",
        "In a multi-layer LSTM, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtuSTiqLHwpp"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -l -v -k \"test_lstm\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESvJ2vjzHwpq"
      },
      "outputs": [],
      "source": [
        "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"lstm\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNWK_s97Hwpq"
      },
      "source": [
        "## Part 6: Penn Treebank dataset [10 points]\n",
        "\n",
        "In word-level language modeling tasks, the model predicts the probability of the next word in the sequence, based on the words already observed in the sequence. You will write support for the Penn Treebank dataset, which consists of stories from the Wall Street Journal, to train and evaluate a language model on word-level prediction.\n",
        "\n",
        "In `python/needle/data/datasets/ptb_dataset.py`, start by implementing the `Dictionary` class, which creates a dictionary from a list of words, mapping each word to a unique integer.\n",
        "\n",
        "Next, we will use this `Dictionary` class to create a corpus from the train and test txt files in the Penn Treebank dataset that you downloaded at the beginning of the notebook. Implement the `tokenize` function in the `Corpus` class to do this.\n",
        "\n",
        "In order to prepare the data for training and evaluation, you will next implement the `batchify` function. Starting from sequential data, batchify arranges the dataset into columns. For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "\n",
        "```\n",
        "┌ a g m s ┐\n",
        "│ b h n t │\n",
        "│ c i o u │\n",
        "│ d j p v │\n",
        "│ e k q w │\n",
        "└ f l r x ┘\n",
        "```\n",
        "\n",
        "These columns are treated as independent by the model, which means that the dependence of e. g. 'g' on 'f' cannot be learned, but allows more efficient batch processing.\n",
        "\n",
        "Next, implement the `get_batch` function. `get_batch` subdivides the source data into chunks of length `bptt`. If source is equal to the example output of the batchify function, with a bptt-limit of 2, we'd get the following two `Tensor`s for i = 0:\n",
        "```\n",
        "┌ a g m s ┐ ┌ b h n t ┐\n",
        "└ b h n t ┘ └ c i o u ┘\n",
        "```\n",
        "Note that despite the name of the function, the subdivison of data is not done along the batch dimension (i.e. dimension 1), since that was handled by the batchify function. The chunks are along dimension 0, corresponding to the seq_len dimension in the LSTM or RNN. Also, as per the function docs, the second returned `Tensor` (the targets) should be reshaped to be 1-dimensional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaxpT1w4Hwpq"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -l -v -k \"ptb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXiYIuqOHwpq"
      },
      "outputs": [],
      "source": [
        "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"ptb\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfrKWpxaHwpq"
      },
      "source": [
        "## Part 7: Training a word-level language model [10 points]\n",
        "\n",
        "Finally, you will use the `RNN` and `LSTM` components you have written to construct a language model that we will train on the Penn Treebank dataset.\n",
        "\n",
        "First, in `python/needle/nn/nn_sequence.py` implement `Embedding`. Consider we have a dictionary with 1000 words. Then for a word which indexes into this dictionary, we can represent this word as a one-hot vector of size 1000, and then use a linear layer to project this to a vector of some embedding size.\n",
        "\n",
        "In `apps/models.py`, you can now implement `LanguageModel`. Your language model should consist of\n",
        "\n",
        "- An embedding layer (which maps word IDs to embeddings)\n",
        "- A sequence model (either RNN or LSTM)\n",
        "- A linear layer (which outputs probabilities of the next word)\n",
        "\n",
        "In `apps/simple_ml.py` implement `epoch_general_ptb`, `train_ptb`, and `evaluate_ptb`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5URZCvdHwpq"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -l -v -k \"language_model_implementation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XpNy5KLHwpq"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -l -v -k \"language_model_training\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btu8cSIgHwpq"
      },
      "outputs": [],
      "source": [
        "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"language_model\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akQSFpB1Hwpq"
      },
      "source": [
        "Now, you can train your language model on the Penn Treebank dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxYyfy3rHwpq"
      },
      "outputs": [],
      "source": [
        "import needle as ndl\n",
        "sys.path.append('./apps')\n",
        "from models import LanguageModel\n",
        "from simple_ml import train_ptb, evaluate_ptb\n",
        "\n",
        "device = ndl.cpu()\n",
        "corpus = ndl.data.Corpus(\"data/ptb\")\n",
        "train_data = ndl.data.batchify(corpus.train, batch_size=16, device=ndl.cpu(), dtype=\"float32\")\n",
        "model = LanguageModel(30, len(corpus.dictionary), hidden_size=10, num_layers=2, seq_model='rnn', device=ndl.cpu())\n",
        "train_ptb(model, train_data, seq_len=1, n_epochs=1, device=device)\n",
        "evaluate_ptb(model, train_data, seq_len=40, device=device)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}